{
    "patch": [
        {
            "func_name": "hvm_load_segment_selector",
            "patch": "--- func_before\n+++ func_after\n@@ -2735,15 +2735,14 @@\n static int hvm_load_segment_selector(\n-    enum x86_segment seg, uint16_t sel)\n+    enum x86_segment seg, uint16_t sel, unsigned int eflags)\n {\n     struct segment_register desctab, cs, segr;\n     struct desc_struct *pdesc, desc;\n     u8 dpl, rpl, cpl;\n     bool_t writable;\n     int fault_type = TRAP_invalid_tss;\n-    struct cpu_user_regs *regs = guest_cpu_user_regs();\n     struct vcpu *v = current;\n \n-    if ( regs->eflags & X86_EFLAGS_VM )\n+    if ( eflags & X86_EFLAGS_VM )\n     {\n         segr.sel = sel;\n         segr.base = (uint32_t)sel << 4;",
            "file_path": "xen/arch/x86/hvm/hvm.c"
        },
        {
            "func_name": "hvm_task_switch",
            "patch": "--- func_before\n+++ func_after\n@@ -2995,6 +2995,8 @@\n     if ( rc != HVMCOPY_okay )\n         goto out;\n \n+    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt, 0) )\n+        goto out;\n \n     if ( hvm_set_cr3(tss.cr3, 1) )\n         goto out;\n@@ -3017,13 +3019,12 @@\n     }\n \n     exn_raised = 0;\n-    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt) ||\n-         hvm_load_segment_selector(x86_seg_es, tss.es) ||\n-         hvm_load_segment_selector(x86_seg_cs, tss.cs) ||\n-         hvm_load_segment_selector(x86_seg_ss, tss.ss) ||\n-         hvm_load_segment_selector(x86_seg_ds, tss.ds) ||\n-         hvm_load_segment_selector(x86_seg_fs, tss.fs) ||\n-         hvm_load_segment_selector(x86_seg_gs, tss.gs) )\n+    if ( hvm_load_segment_selector(x86_seg_es, tss.es, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_cs, tss.cs, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_ss, tss.ss, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_ds, tss.ds, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_fs, tss.fs, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_gs, tss.gs, tss.eflags) )\n         exn_raised = 1;\n \n     rc = hvm_copy_to_guest_virt(",
            "file_path": "xen/arch/x86/hvm/hvm.c"
        }
    ],
    "description": "Xen 4.0.x through 4.7.x mishandle x86 task switches to VM86 mode, which allows local 32-bit x86 HVM guest OS users to gain privileges or cause a denial of service (guest OS crash) by leveraging a guest operating system that uses hardware task switching and allows a new task to start in VM86 mode.",
    "msg": "x86/HVM: don't load LDTR with VM86 mode attrs during task switch\n\nJust like TR, LDTR is purely a protected mode facility and hence needs\nto be loaded accordingly. Also move its loading to where it\narchitecurally belongs.\n\nThis is CVE-2016-9382 / XSA-192.\n\nSigned-off-by: Jan Beulich <jbeulich@suse.com>\nReviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>\nTested-by: Andrew Cooper <andrew.cooper3@citrix.com>",
    "root_cause": "The vulnerability arises from hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags for LDTR loading during task switches. The patch fixes this by loading LDTR earlier with eflags=0 (xen/arch/x86/hvm/hvm.c:2995). However, tss.eflags (used for other segments) is populated by unknown logic in SVM/VMX code, creating a gap in the trigger chain.",
    "confidence_score": 0.75,
    "enriched_data": [
        {
            "tool": "func_info.fetch_func_by_name({'func_name': 'tss_structure_population', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": "no valid result"
        },
        {
            "tool": "caller_info.find_caller_for_func({'func_name': 'hvm_task_switch', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": [
                {
                    "type": "caller_info",
                    "call_line": 2536,
                    "call_code": "hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)",
                    "caller_code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                    "caller_start": 2283,
                    "file_path": "xen/arch/x86/hvm/svm/svm.c"
                },
                {
                    "type": "caller_info",
                    "call_line": 3630,
                    "call_code": "hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)",
                    "caller_code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                    "caller_start": 3290,
                    "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                }
            ]
        },
        {
            "tool": "caller_info.find_caller_for_func({'func_name': 'hvm_task_switch', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": [
                {
                    "type": "caller_info",
                    "call_line": 2536,
                    "call_code": "hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)",
                    "caller_code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                    "caller_start": 2283,
                    "file_path": "xen/arch/x86/hvm/svm/svm.c"
                },
                {
                    "type": "caller_info",
                    "call_line": 3630,
                    "call_code": "hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)",
                    "caller_code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                    "caller_start": 3290,
                    "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                }
            ]
        },
        {
            "tool": "func_info.fetch_func_by_name({'func_name': 'tss_eflags', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": "no valid result"
        },
        {
            "tool": "caller_info.find_caller_for_func({'func_name': 'tss_eflags', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": "no valid result, it may be a leaf function"
        },
        {
            "tool": "func_info.fetch_func_by_name({'func_name': 'svm_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": [
                {
                    "type": "function_def",
                    "code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                    "start_line": 2283,
                    "end_line": 2712,
                    "full_name": "svm_vmexit_handler",
                    "file_path": "xen/arch/x86/hvm/svm/svm.c"
                }
            ]
        },
        {
            "tool": "func_info.fetch_func_by_name({'func_name': 'vmx_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": [
                {
                    "type": "function_def",
                    "code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                    "start_line": 3290,
                    "end_line": 3957,
                    "full_name": "vmx_vmexit_handler",
                    "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                }
            ]
        },
        {
            "tool": "func_info.fetch_func_by_name({'func_name': 'svm_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": [
                {
                    "type": "function_def",
                    "code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                    "start_line": 2283,
                    "end_line": 2712,
                    "full_name": "svm_vmexit_handler",
                    "file_path": "xen/arch/x86/hvm/svm/svm.c"
                }
            ]
        },
        {
            "tool": "func_info.fetch_func_by_name({'func_name': 'vmx_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
            "result": [
                {
                    "type": "function_def",
                    "code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                    "start_line": 3290,
                    "end_line": 3957,
                    "full_name": "vmx_vmexit_handler",
                    "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                }
            ]
        }
    ],
    "analysis": {
        "patch": [
            {
                "func_name": "hvm_load_segment_selector",
                "patch": "--- func_before\n+++ func_after\n@@ -2735,15 +2735,14 @@\n static int hvm_load_segment_selector(\n-    enum x86_segment seg, uint16_t sel)\n+    enum x86_segment seg, uint16_t sel, unsigned int eflags)\n {\n     struct segment_register desctab, cs, segr;\n     struct desc_struct *pdesc, desc;\n     u8 dpl, rpl, cpl;\n     bool_t writable;\n     int fault_type = TRAP_invalid_tss;\n-    struct cpu_user_regs *regs = guest_cpu_user_regs();\n     struct vcpu *v = current;\n \n-    if ( regs->eflags & X86_EFLAGS_VM )\n+    if ( eflags & X86_EFLAGS_VM )\n     {\n         segr.sel = sel;\n         segr.base = (uint32_t)sel << 4;",
                "file_path": "xen/arch/x86/hvm/hvm.c"
            },
            {
                "func_name": "hvm_task_switch",
                "patch": "--- func_before\n+++ func_after\n@@ -2995,6 +2995,8 @@\n     if ( rc != HVMCOPY_okay )\n         goto out;\n \n+    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt, 0) )\n+        goto out;\n \n     if ( hvm_set_cr3(tss.cr3, 1) )\n         goto out;\n@@ -3017,13 +3019,12 @@\n     }\n \n     exn_raised = 0;\n-    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt) ||\n-         hvm_load_segment_selector(x86_seg_es, tss.es) ||\n-         hvm_load_segment_selector(x86_seg_cs, tss.cs) ||\n-         hvm_load_segment_selector(x86_seg_ss, tss.ss) ||\n-         hvm_load_segment_selector(x86_seg_ds, tss.ds) ||\n-         hvm_load_segment_selector(x86_seg_fs, tss.fs) ||\n-         hvm_load_segment_selector(x86_seg_gs, tss.gs) )\n+    if ( hvm_load_segment_selector(x86_seg_es, tss.es, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_cs, tss.cs, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_ss, tss.ss, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_ds, tss.ds, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_fs, tss.fs, tss.eflags) ||\n+         hvm_load_segment_selector(x86_seg_gs, tss.gs, tss.eflags) )\n         exn_raised = 1;\n \n     rc = hvm_copy_to_guest_virt(",
                "file_path": "xen/arch/x86/hvm/hvm.c"
            }
        ],
        "msg": "x86/HVM: don't load LDTR with VM86 mode attrs during task switch\n\nJust like TR, LDTR is purely a protected mode facility and hence needs\nto be loaded accordingly. Also move its loading to where it\narchitecurally belongs.\n\nThis is CVE-2016-9382 / XSA-192.\n\nSigned-off-by: Jan Beulich <jbeulich@suse.com>\nReviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>\nTested-by: Andrew Cooper <andrew.cooper3@citrix.com>",
        "description": "Xen 4.0.x through 4.7.x mishandle x86 task switches to VM86 mode, which allows local 32-bit x86 HVM guest OS users to gain privileges or cause a denial of service (guest OS crash) by leveraging a guest operating system that uses hardware task switching and allows a new task to start in VM86 mode.",
        "language": "c",
        "vulnerability_type": "logic",
        "repair_strategy": "The patch modifies hvm_load_segment_selector to use the provided eflags parameter instead of relying on guest_cpu_user_regs()->eflags (file: xen/arch/x86/hvm/hvm.c, line 2735-2736). It also ensures LDTR is loaded earlier in hvm_task_switch with eflags=0, preventing VM86 mode attributes from being applied (file: xen/arch/x86/hvm/hvm.c, line 2995-2997).",
        "need_context": true,
        "required_context": [
            "function: TSS structure population in Xen SVM (how eflags is set in svm_vmexit_handler's TSS)",
            "function: TSS structure population in Xen VMX (how eflags is set in vmx_vmexit_handler's TSS)"
        ],
        "confidence_score": 0.75,
        "root_cause": "The vulnerability arises from hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags for LDTR loading during task switches. The patch fixes this by loading LDTR earlier with eflags=0 (xen/arch/x86/hvm/hvm.c:2995). However, tss.eflags (used for other segments) is populated by unknown logic in SVM/VMX code, creating a gap in the trigger chain.",
        "analysis": "The root cause is confirmed for LDTR loading, but the source of tss.eflags remains untraced. The patch ensures LDTR is loaded with eflags=0 (preventing VM86 contamination) in hvm_task_switch (xen/arch/x86/hvm/hvm.c:2995), while other segments use tss.eflags. However, the TSS population logic for eflags in SVM/VMX code is missing from the provided context. Callers to hvm_task_switch (svm_vmexit_handler and vmx_vmexit_handler) derive parameters from VMCB/VMX exit data but do not show how tss.eflags is initialized. Without this, the full trigger chain (e.g., how VM86 flags propagate into tss.eflags) cannot be completed."
    },
    "iteration": 5,
    "history": [
        {
            "stage": "initial",
            "prompt": "You are a senior security engineer. Using **only the provided patch and description**, and **restricted to context retrievable by the 'caller' and 'function' tools**, perform the initial vulnerability analysis.\n\n## INPUTS\n- **Commit Message**: x86/HVM: don't load LDTR with VM86 mode attrs during task switch\n\nJust like TR, LDTR is purely a protected mode facility and hence needs\nto be loaded accordingly. Also move its loading to where it\narchitecurally belongs.\n\nThis is CVE-2016-9382 / XSA-192.\n\nSigned-off-by: Jan Beulich <jbeulich@suse.com>\nReviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>\nTested-by: Andrew Cooper <andrew.cooper3@citrix.com>\n- **Vulnerability Description**: Xen 4.0.x through 4.7.x mishandle x86 task switches to VM86 mode, which allows local 32-bit x86 HVM guest OS users to gain privileges or cause a denial of service (guest OS crash) by leveraging a guest operating system that uses hardware task switching and allows a new task to start in VM86 mode.\n- **Patch Code**: [{'func_name': 'hvm_load_segment_selector', 'patch': '--- func_before\\n+++ func_after\\n@@ -2735,15 +2735,14 @@\\n static int hvm_load_segment_selector(\\n-    enum x86_segment seg, uint16_t sel)\\n+    enum x86_segment seg, uint16_t sel, unsigned int eflags)\\n {\\n     struct segment_register desctab, cs, segr;\\n     struct desc_struct *pdesc, desc;\\n     u8 dpl, rpl, cpl;\\n     bool_t writable;\\n     int fault_type = TRAP_invalid_tss;\\n-    struct cpu_user_regs *regs = guest_cpu_user_regs();\\n     struct vcpu *v = current;\\n \\n-    if ( regs->eflags & X86_EFLAGS_VM )\\n+    if ( eflags & X86_EFLAGS_VM )\\n     {\\n         segr.sel = sel;\\n         segr.base = (uint32_t)sel << 4;', 'file_path': 'xen/arch/x86/hvm/hvm.c'}, {'func_name': 'hvm_task_switch', 'patch': '--- func_before\\n+++ func_after\\n@@ -2995,6 +2995,8 @@\\n     if ( rc != HVMCOPY_okay )\\n         goto out;\\n \\n+    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt, 0) )\\n+        goto out;\\n \\n     if ( hvm_set_cr3(tss.cr3, 1) )\\n         goto out;\\n@@ -3017,13 +3019,12 @@\\n     }\\n \\n     exn_raised = 0;\\n-    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt) ||\\n-         hvm_load_segment_selector(x86_seg_es, tss.es) ||\\n-         hvm_load_segment_selector(x86_seg_cs, tss.cs) ||\\n-         hvm_load_segment_selector(x86_seg_ss, tss.ss) ||\\n-         hvm_load_segment_selector(x86_seg_ds, tss.ds) ||\\n-         hvm_load_segment_selector(x86_seg_fs, tss.fs) ||\\n-         hvm_load_segment_selector(x86_seg_gs, tss.gs) )\\n+    if ( hvm_load_segment_selector(x86_seg_es, tss.es, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_cs, tss.cs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ss, tss.ss, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ds, tss.ds, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_fs, tss.fs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_gs, tss.gs, tss.eflags) )\\n         exn_raised = 1;\\n \\n     rc = hvm_copy_to_guest_virt(', 'file_path': 'xen/arch/x86/hvm/hvm.c'}]\n\n## INSTRUCTIONS\n1. **Patch Review**  \n   • Detect primary language (`c`, `cpp`, `java`).  \n   • Classify vulnerability (`memory`, `logic`, `config`).  \n   • Explain how each patch hunk mitigates risk, citing `file:line`.  \n   → Populate `language`, `vulnerability_type`, `repair_rationale`.\n\n2. **Trace Root Cause within Patch**  \n   Follow calls / data flow in the patch, citing `file:line`.  \n   Where evidence ends, insert: `GAP: <why evidence stops>` → build `root_cause`.\n\n3. **Determine Missing Context (tools limited)**  \n   If gaps remain:  \n     • set `\"need_context\": true`  \n     • add precise, non-duplicate requests to `\"required_context\"` using **only**:  \n       - `function: …`  \n       - `caller: …`  \n   If the chain is fully evidenced from the patch, set `\"need_context\": false`.\n\n4. **Confidence Score**  \n   Set `confidence_score ≥ 0.90` only if the full chain is evidenced; otherwise `≤ 0.75`.\n\n## OUTPUT (MUST valid JSON — double quotes, no trailing commas)\n```json\n  {\n      \"language\": \"c | cpp | java\",\n      \"vulnerability_type\": \"memory | logic | config\",\n      \"repair_strategy\": \"<citing file:line from the patch>\",\n      \"analysis\": \"<Synthesis from Process Step 2. If full chain evident, explain that. If context needed, explain why requested context is essential to bridge gaps.>\",\n      \"need_context\": true | false, //false ONLY IF the full pattern was unambiguously evident from patch alone.\n      \"required_context\": \"<list of precise context requests if any context needed>\" ,\n      \"root_cause\": \"\"<Partial/complete trace with file:line evidence from the patch, noting gaps. If full chain evident, provide it.>\",\n      \"confidence_score\": 0.0-1.0 // Confidence in understanding the *full* path based on *this initial analysis phase*\n  }\n  ```\n\n## RULES\n* **Output must be valid JSON (no trailing commas).**\n* **Source-of-Truth** – Cite only code visible in the patch (`file:line`). Commit message / description are auxiliary.\n* **Zero-Assumption** – Never infer. Missing evidence → declare GAP, set `need_context=true`, and request context with allowed tools.\n* **Tool Restriction** – `required_context` items **must** be of type `function` or `caller` only.\n* **Stop Condition** – Terminate only when the trigger chain is fully evidenced from the patch; else continue requesting context.\n",
            "result": {
                "language": "c",
                "vulnerability_type": "logic",
                "repair_strategy": "The patch modifies hvm_load_segment_selector to use the provided eflags parameter instead of relying on guest_cpu_user_regs()->eflags (file: xen/arch/x86/hvm/hvm.c, line 2735-2736). It also ensures LDTR is loaded earlier in hvm_task_switch with eflags=0, preventing VM86 mode attributes from being applied (file: xen/arch/x86/hvm/hvm.c, line 2995-2997).",
                "analysis": "The vulnerability arises from incorrect handling of VM86 mode flags during task switches. The original code checked the guest's eflags directly, which could include VM86 mode attributes. The patch ensures LDTR is only loaded with appropriate flags by passing tss.eflags explicitly. However, the exact source of tss.eflags in the TSS structure is not visible in the patch, creating a gap in understanding how VM86 mode is validated.",
                "need_context": true,
                "required_context": [
                    "function: tss structure population in Xen",
                    "caller: hvm_task_switch"
                ],
                "root_cause": "The root cause lies in using VM86 mode attributes when loading the LDTR during task switches. The patch fixes this by: 1) Adding eflags parameter to hvm_load_segment_selector (file: xen/arch/x86/hvm/hvm.c, line 2735) to validate VM86 mode context, 2) Loading LDTR earlier in hvm_task_switch with eflags=0 to avoid VM86 mode contamination (file: xen/arch/x86/hvm/hvm.c, line 2995). GAP: The source of tss.eflags in the TSS structure is not visible in the provided patch, making it unclear how VM86 mode is enforced in the TSS.",
                "confidence_score": 0.75
            },
            "timestamp": "2025-05-29T19:57:18.913063"
        },
        {
            "stage": "collection",
            "required_context": [
                "function: tss structure population in Xen",
                "caller: hvm_task_switch"
            ],
            "results": [
                {
                    "tool": "func_info.fetch_func_by_name({'func_name': 'tss_structure_population', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": "no valid result"
                },
                {
                    "tool": "caller_info.find_caller_for_func({'func_name': 'hvm_task_switch', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": [
                        {
                            "type": "caller_info",
                            "call_line": 2536,
                            "call_code": "hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)",
                            "caller_code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                            "caller_start": 2283,
                            "file_path": "xen/arch/x86/hvm/svm/svm.c"
                        },
                        {
                            "type": "caller_info",
                            "call_line": 3630,
                            "call_code": "hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)",
                            "caller_code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                            "caller_start": 3290,
                            "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                        }
                    ]
                }
            ],
            "timestamp": "2025-05-29T19:57:30.084031"
        },
        {
            "stage": "analysis",
            "iteration": 1,
            "prompt": "You are a senior security engineer. Using **all available code context**, build the **complete, evidence-backed root-cause and trigger chain** for the vulnerability.\n\n## INPUTS\n- **Commit Message**: x86/HVM: don't load LDTR with VM86 mode attrs during task switch\n\nJust like TR, LDTR is purely a protected mode facility and hence needs\nto be loaded accordingly. Also move its loading to where it\narchitecurally belongs.\n\nThis is CVE-2016-9382 / XSA-192.\n\nSigned-off-by: Jan Beulich <jbeulich@suse.com>\nReviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>\nTested-by: Andrew Cooper <andrew.cooper3@citrix.com>\n- **Vulnerability Description**: Xen 4.0.x through 4.7.x mishandle x86 task switches to VM86 mode, which allows local 32-bit x86 HVM guest OS users to gain privileges or cause a denial of service (guest OS crash) by leveraging a guest operating system that uses hardware task switching and allows a new task to start in VM86 mode.\n- **Patch Code**: [{'func_name': 'hvm_load_segment_selector', 'patch': '--- func_before\\n+++ func_after\\n@@ -2735,15 +2735,14 @@\\n static int hvm_load_segment_selector(\\n-    enum x86_segment seg, uint16_t sel)\\n+    enum x86_segment seg, uint16_t sel, unsigned int eflags)\\n {\\n     struct segment_register desctab, cs, segr;\\n     struct desc_struct *pdesc, desc;\\n     u8 dpl, rpl, cpl;\\n     bool_t writable;\\n     int fault_type = TRAP_invalid_tss;\\n-    struct cpu_user_regs *regs = guest_cpu_user_regs();\\n     struct vcpu *v = current;\\n \\n-    if ( regs->eflags & X86_EFLAGS_VM )\\n+    if ( eflags & X86_EFLAGS_VM )\\n     {\\n         segr.sel = sel;\\n         segr.base = (uint32_t)sel << 4;', 'file_path': 'xen/arch/x86/hvm/hvm.c'}, {'func_name': 'hvm_task_switch', 'patch': '--- func_before\\n+++ func_after\\n@@ -2995,6 +2995,8 @@\\n     if ( rc != HVMCOPY_okay )\\n         goto out;\\n \\n+    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt, 0) )\\n+        goto out;\\n \\n     if ( hvm_set_cr3(tss.cr3, 1) )\\n         goto out;\\n@@ -3017,13 +3019,12 @@\\n     }\\n \\n     exn_raised = 0;\\n-    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt) ||\\n-         hvm_load_segment_selector(x86_seg_es, tss.es) ||\\n-         hvm_load_segment_selector(x86_seg_cs, tss.cs) ||\\n-         hvm_load_segment_selector(x86_seg_ss, tss.ss) ||\\n-         hvm_load_segment_selector(x86_seg_ds, tss.ds) ||\\n-         hvm_load_segment_selector(x86_seg_fs, tss.fs) ||\\n-         hvm_load_segment_selector(x86_seg_gs, tss.gs) )\\n+    if ( hvm_load_segment_selector(x86_seg_es, tss.es, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_cs, tss.cs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ss, tss.ss, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ds, tss.ds, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_fs, tss.fs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_gs, tss.gs, tss.eflags) )\\n         exn_raised = 1;\\n \\n     rc = hvm_copy_to_guest_virt(', 'file_path': 'xen/arch/x86/hvm/hvm.c'}]\n- **History Summary (all prior context & analysis)**: [\n  {\n    \"stage\": \"initial\",\n    \"result\": {\n      \"language\": \"c\",\n      \"vulnerability_type\": \"logic\",\n      \"repair_strategy\": \"The patch modifies hvm_load_segment_selector to use the provided eflags parameter instead of relying on guest_cpu_user_regs()->eflags (file: xen/arch/x86/hvm/hvm.c, line 2735-2736). It also ensures LDTR is loaded earlier in hvm_task_switch with eflags=0, preventing VM86 mode attributes from being applied (file: xen/arch/x86/hvm/hvm.c, line 2995-2997).\",\n      \"required_context\": [\n        \"function: tss structure population in Xen\",\n        \"caller: hvm_task_switch\"\n      ],\n      \"root_cause\": \"The root cause lies in using VM86 mode attributes when loading the LDTR during task switches. The patch fixes this by: 1) Adding eflags parameter to hvm_load_segment_selector (file: xen/arch/x86/hvm/hvm.c, line 2735) to validate VM86 mode context, 2) Loading LDTR earlier in hvm_task_switch with eflags=0 to avoid VM86 mode contamination (file: xen/arch/x86/hvm/hvm.c, line 2995). GAP: The source of tss.eflags in the TSS structure is not visible in the provided patch, making it unclear how VM86 mode is enforced in the TSS.\"\n    }\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:hvm_task_switch)\",\n        \"result\": [\n          {\n            \"call_line\": 2536,\n            \"call_code\": \"hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)\",\n            \"caller_code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"caller_start\": 2283,\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          },\n          {\n            \"call_line\": 3630,\n            \"call_code\": \"hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)\",\n            \"caller_code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"caller_start\": 3290,\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  }\n]\n\n## INSTRUCTIONS\n1. **Review & Integrate Context** \n  Review all `history_summary` (*incl. past analysis and request results*) \n\n2. **Extend Chain from Code**  \n   Trace calls / data flow step-by-step, citing `file:line` for each fact.  \n   Where evidence ends, insert  \n   `GAP: <why evidence stops>`.\n\n3. **Request Missing Context**  \n   If any critical gap remains:  \n     * Always request context and set `\"need_context\": true`. \n     * Add precise, non-duplicate requests to `\"required_context\"` using the following types:  \n      `function`, `caller`, `e.g. caller: I need the caller of function X in file Y`.\n      **DO NOT** request the new patch code.\n     * Try alternative parameters or request types for gaps previously unfulfilled.  \n   If the chain is fully evidenced, set `\"need_context\": false`.\n\n4. **Score Confidence**  \n   *Full chain with direct evidence* ≥ 0.90.  Incomplete chain ≤ 0.75.\n\n## OUTPUT  (JSON only, no trailing commas)\n  ```json\n  {\n      \"analysis\": \"Summary of iteration progress, remaining gaps, explanation for new context requests (if any). If complete, summarize the full chain.\",\n      \"need_context\": true/false, \n      \"required_context\": \"<list of precise context requests>\" ,\n      \"root_cause\": \"<Partial/complete trace with file:line evidence from ALL available context>\",\n      \"confidence_score\": 0.0-1.0\n      // attack_vectors added when confidence >= 0.90 AND need_context is False\n  }\n  ```\n  ## RULES\n  * Output must be valid JSON.\n  * **Source-of-Truth** – Only accept facts visible in code (file:line). Commit message & description are auxiliary.\n  * **Reject every hypothesis or inference** - When evidence is missing, always declare a GAP and request the needed context.\n  * **Tool Restriction** – `required_context` items **must** be of type `function` or `caller` only.\n  * **Stop Condition** – Terminate only when the trigger chain is fully evidenced from the patch; else continue requesting context.\n",
            "result": {
                "analysis": "The root cause is traced to incorrect handling of VM86 mode attributes during task switches, specifically when loading the LDTR. The patch fixes this by ensuring LDTR is loaded with eflags=0 (preventing VM86 mode contamination) and using tss.eflags for other segments. A critical gap remains in understanding how tss.eflags is populated, as this determines whether VM86 mode attributes are improperly applied during task switches. Without this context, the full trigger chain (e.g., how tss.eflags is set) cannot be confirmed.",
                "need_context": true,
                "required_context": [
                    "function: TSS structure population in Xen (specifically tss.eflags initialization)",
                    "caller: hvm_task_switch's source of tss.eflags in Xen's SVM/VMX code"
                ],
                "root_cause": "The vulnerability arises from using VM86 mode attributes (via guest_cpu_user_regs()->eflags) when loading the LDTR during task switches. The patch: 1) Adds eflags parameter to hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) to validate VM86 mode context, 2) Loads LDTR earlier in hvm_task_switch with eflags=0 (xen/arch/x86/hvm/hvm.c:2995), preventing VM86 mode contamination. The trigger involves task switches where tss.eflags (source unknown in current context) improperly includes VM86 mode flags.",
                "confidence_score": 0.75
            },
            "timestamp": "2025-05-29T19:57:31.136069"
        },
        {
            "stage": "collection",
            "required_context": [
                "function: TSS structure population in Xen (specifically tss.eflags initialization)",
                "caller: hvm_task_switch's source of tss.eflags in Xen's SVM/VMX code"
            ],
            "results": [
                {
                    "tool": "caller_info.find_caller_for_func({'func_name': 'hvm_task_switch', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": [
                        {
                            "type": "caller_info",
                            "call_line": 2536,
                            "call_code": "hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)",
                            "caller_code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                            "caller_start": 2283,
                            "file_path": "xen/arch/x86/hvm/svm/svm.c"
                        },
                        {
                            "type": "caller_info",
                            "call_line": 3630,
                            "call_code": "hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)",
                            "caller_code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                            "caller_start": 3290,
                            "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                        }
                    ]
                }
            ],
            "timestamp": "2025-05-29T19:57:32.508407"
        },
        {
            "stage": "analysis",
            "iteration": 2,
            "prompt": "You are a senior security engineer. Using **all available code context**, build the **complete, evidence-backed root-cause and trigger chain** for the vulnerability.\n\n## INPUTS\n- **Commit Message**: x86/HVM: don't load LDTR with VM86 mode attrs during task switch\n\nJust like TR, LDTR is purely a protected mode facility and hence needs\nto be loaded accordingly. Also move its loading to where it\narchitecurally belongs.\n\nThis is CVE-2016-9382 / XSA-192.\n\nSigned-off-by: Jan Beulich <jbeulich@suse.com>\nReviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>\nTested-by: Andrew Cooper <andrew.cooper3@citrix.com>\n- **Vulnerability Description**: Xen 4.0.x through 4.7.x mishandle x86 task switches to VM86 mode, which allows local 32-bit x86 HVM guest OS users to gain privileges or cause a denial of service (guest OS crash) by leveraging a guest operating system that uses hardware task switching and allows a new task to start in VM86 mode.\n- **Patch Code**: [{'func_name': 'hvm_load_segment_selector', 'patch': '--- func_before\\n+++ func_after\\n@@ -2735,15 +2735,14 @@\\n static int hvm_load_segment_selector(\\n-    enum x86_segment seg, uint16_t sel)\\n+    enum x86_segment seg, uint16_t sel, unsigned int eflags)\\n {\\n     struct segment_register desctab, cs, segr;\\n     struct desc_struct *pdesc, desc;\\n     u8 dpl, rpl, cpl;\\n     bool_t writable;\\n     int fault_type = TRAP_invalid_tss;\\n-    struct cpu_user_regs *regs = guest_cpu_user_regs();\\n     struct vcpu *v = current;\\n \\n-    if ( regs->eflags & X86_EFLAGS_VM )\\n+    if ( eflags & X86_EFLAGS_VM )\\n     {\\n         segr.sel = sel;\\n         segr.base = (uint32_t)sel << 4;', 'file_path': 'xen/arch/x86/hvm/hvm.c'}, {'func_name': 'hvm_task_switch', 'patch': '--- func_before\\n+++ func_after\\n@@ -2995,6 +2995,8 @@\\n     if ( rc != HVMCOPY_okay )\\n         goto out;\\n \\n+    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt, 0) )\\n+        goto out;\\n \\n     if ( hvm_set_cr3(tss.cr3, 1) )\\n         goto out;\\n@@ -3017,13 +3019,12 @@\\n     }\\n \\n     exn_raised = 0;\\n-    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt) ||\\n-         hvm_load_segment_selector(x86_seg_es, tss.es) ||\\n-         hvm_load_segment_selector(x86_seg_cs, tss.cs) ||\\n-         hvm_load_segment_selector(x86_seg_ss, tss.ss) ||\\n-         hvm_load_segment_selector(x86_seg_ds, tss.ds) ||\\n-         hvm_load_segment_selector(x86_seg_fs, tss.fs) ||\\n-         hvm_load_segment_selector(x86_seg_gs, tss.gs) )\\n+    if ( hvm_load_segment_selector(x86_seg_es, tss.es, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_cs, tss.cs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ss, tss.ss, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ds, tss.ds, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_fs, tss.fs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_gs, tss.gs, tss.eflags) )\\n         exn_raised = 1;\\n \\n     rc = hvm_copy_to_guest_virt(', 'file_path': 'xen/arch/x86/hvm/hvm.c'}]\n- **History Summary (all prior context & analysis)**: [\n  {\n    \"stage\": \"initial\",\n    \"result\": {\n      \"language\": \"c\",\n      \"vulnerability_type\": \"logic\",\n      \"repair_strategy\": \"The patch modifies hvm_load_segment_selector to use the provided eflags parameter instead of relying on guest_cpu_user_regs()->eflags (file: xen/arch/x86/hvm/hvm.c, line 2735-2736). It also ensures LDTR is loaded earlier in hvm_task_switch with eflags=0, preventing VM86 mode attributes from being applied (file: xen/arch/x86/hvm/hvm.c, line 2995-2997).\",\n      \"required_context\": [\n        \"function: tss structure population in Xen\",\n        \"caller: hvm_task_switch\"\n      ],\n      \"root_cause\": \"The root cause lies in using VM86 mode attributes when loading the LDTR during task switches. The patch fixes this by: 1) Adding eflags parameter to hvm_load_segment_selector (file: xen/arch/x86/hvm/hvm.c, line 2735) to validate VM86 mode context, 2) Loading LDTR earlier in hvm_task_switch with eflags=0 to avoid VM86 mode contamination (file: xen/arch/x86/hvm/hvm.c, line 2995). GAP: The source of tss.eflags in the TSS structure is not visible in the provided patch, making it unclear how VM86 mode is enforced in the TSS.\"\n    }\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:hvm_task_switch)\",\n        \"result\": [\n          {\n            \"call_line\": 2536,\n            \"call_code\": \"hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)\",\n            \"caller_code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"caller_start\": 2283,\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          },\n          {\n            \"call_line\": 3630,\n            \"call_code\": \"hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)\",\n            \"caller_code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"caller_start\": 3290,\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"stage\": \"analysis\",\n    \"need_context\": true,\n    \"root_cause_partial\": \"The vulnerability arises from using VM86 mode attributes (via guest_cpu_user_regs()->eflags) when loading the LDTR during task switches. The patch: 1) Adds eflags parameter to hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) to validate VM86 mode context, 2) Loads LDTR earlier in hvm_task_switch with eflags=0 (xen/arch/x86/hvm/hvm.c:2995), preventing VM86 mode contamination. The trigger involves task switches where tss.eflags (source unknown in current context) improperly includes VM86 mode flags.\",\n    \"required_context_next\": [\n      \"function: TSS structure population in Xen (specifically tss.eflags initialization)\",\n      \"caller: hvm_task_switch's source of tss.eflags in Xen's SVM/VMX code\"\n    ],\n    \"analysis_summary_text\": \"The root cause is traced to incorrect handling of VM86 mode attributes during task switches, specifically when loading the LDTR. The patch fixes this by ensuring LDTR is loaded with eflags=0 (preventing VM86 mode contamination) and using tss.eflags for other segments. A critical gap remains in understanding how tss.eflags is populated, as this determines whether VM86 mode attributes are improperly applied during task switches. Without this context, the full trigger chain (e.g., how tss.eflags is set) cannot be confirmed.\"\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:hvm_task_switch)\",\n        \"result\": [\n          {\n            \"call_line\": 2536,\n            \"call_code\": \"hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)\",\n            \"caller_code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"caller_start\": 2283,\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          },\n          {\n            \"call_line\": 3630,\n            \"call_code\": \"hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)\",\n            \"caller_code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"caller_start\": 3290,\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  }\n]\n\n## INSTRUCTIONS\n1. **Review & Integrate Context** \n  Review all `history_summary` (*incl. past analysis and request results*) \n\n2. **Extend Chain from Code**  \n   Trace calls / data flow step-by-step, citing `file:line` for each fact.  \n   Where evidence ends, insert  \n   `GAP: <why evidence stops>`.\n\n3. **Request Missing Context**  \n   If any critical gap remains:  \n     * Always request context and set `\"need_context\": true`. \n     * Add precise, non-duplicate requests to `\"required_context\"` using the following types:  \n      `function`, `caller`, `e.g. caller: I need the caller of function X in file Y`.\n      **DO NOT** request the new patch code.\n     * Try alternative parameters or request types for gaps previously unfulfilled.  \n   If the chain is fully evidenced, set `\"need_context\": false`.\n\n4. **Score Confidence**  \n   *Full chain with direct evidence* ≥ 0.90.  Incomplete chain ≤ 0.75.\n\n## OUTPUT  (JSON only, no trailing commas)\n  ```json\n  {\n      \"analysis\": \"Summary of iteration progress, remaining gaps, explanation for new context requests (if any). If complete, summarize the full chain.\",\n      \"need_context\": true/false, \n      \"required_context\": \"<list of precise context requests>\" ,\n      \"root_cause\": \"<Partial/complete trace with file:line evidence from ALL available context>\",\n      \"confidence_score\": 0.0-1.0\n      // attack_vectors added when confidence >= 0.90 AND need_context is False\n  }\n  ```\n  ## RULES\n  * Output must be valid JSON.\n  * **Source-of-Truth** – Only accept facts visible in code (file:line). Commit message & description are auxiliary.\n  * **Reject every hypothesis or inference** - When evidence is missing, always declare a GAP and request the needed context.\n  * **Tool Restriction** – `required_context` items **must** be of type `function` or `caller` only.\n  * **Stop Condition** – Terminate only when the trigger chain is fully evidenced from the patch; else continue requesting context.\n",
            "result": {
                "analysis": "The vulnerability stems from incorrect handling of VM86 mode attributes during task switches. The patch fixes LDTR loading by using eflags=0 and relying on tss.eflags for other segments. The root cause is traced to improper VM86 mode attribute usage during LDTR loading in hvm_task_switch. However, the source of tss.eflags remains unclear. Callers to hvm_task_switch (svm_vmexit_handler and vmx_vmexit_handler) pass parameters derived from VMCB/VMX exit data, but the TSS population logic (specifically tss.eflags initialization) is not visible in the provided code. This gap prevents confirming how VM86 mode flags propagate into tss.eflags.",
                "need_context": true,
                "required_context": [
                    "function: TSS structure population in Xen (specifically tss.eflags initialization in SVM/VMX code)",
                    "caller: How tss.eflags is set in Xen's SVM/VMX task switch handling"
                ],
                "root_cause": "The root cause is traced to hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags (instead of provided eflags) for LDTR loading during task switches. The patch (xen/arch/x86/hvm/hvm.c:2995) loads LDTR earlier with eflags=0, preventing VM86 mode contamination. The remaining gap is understanding how tss.eflags is populated, as this determines whether VM86 mode attributes are improperly applied during task switches.",
                "confidence_score": 0.75
            },
            "timestamp": "2025-05-29T19:57:32.716491"
        },
        {
            "stage": "collection",
            "required_context": [
                "function: TSS structure population in Xen (specifically tss.eflags initialization in SVM/VMX code)",
                "caller: How tss.eflags is set in Xen's SVM/VMX task switch handling"
            ],
            "results": [
                {
                    "tool": "func_info.fetch_func_by_name({'func_name': 'tss_eflags', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": "no valid result"
                },
                {
                    "tool": "caller_info.find_caller_for_func({'func_name': 'tss_eflags', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": "no valid result, it may be a leaf function"
                }
            ],
            "timestamp": "2025-05-29T19:57:34.010537"
        },
        {
            "stage": "analysis",
            "iteration": 3,
            "prompt": "You are a senior security engineer. Using **all available code context**, build the **complete, evidence-backed root-cause and trigger chain** for the vulnerability.\n\n## INPUTS\n- **Commit Message**: x86/HVM: don't load LDTR with VM86 mode attrs during task switch\n\nJust like TR, LDTR is purely a protected mode facility and hence needs\nto be loaded accordingly. Also move its loading to where it\narchitecurally belongs.\n\nThis is CVE-2016-9382 / XSA-192.\n\nSigned-off-by: Jan Beulich <jbeulich@suse.com>\nReviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>\nTested-by: Andrew Cooper <andrew.cooper3@citrix.com>\n- **Vulnerability Description**: Xen 4.0.x through 4.7.x mishandle x86 task switches to VM86 mode, which allows local 32-bit x86 HVM guest OS users to gain privileges or cause a denial of service (guest OS crash) by leveraging a guest operating system that uses hardware task switching and allows a new task to start in VM86 mode.\n- **Patch Code**: [{'func_name': 'hvm_load_segment_selector', 'patch': '--- func_before\\n+++ func_after\\n@@ -2735,15 +2735,14 @@\\n static int hvm_load_segment_selector(\\n-    enum x86_segment seg, uint16_t sel)\\n+    enum x86_segment seg, uint16_t sel, unsigned int eflags)\\n {\\n     struct segment_register desctab, cs, segr;\\n     struct desc_struct *pdesc, desc;\\n     u8 dpl, rpl, cpl;\\n     bool_t writable;\\n     int fault_type = TRAP_invalid_tss;\\n-    struct cpu_user_regs *regs = guest_cpu_user_regs();\\n     struct vcpu *v = current;\\n \\n-    if ( regs->eflags & X86_EFLAGS_VM )\\n+    if ( eflags & X86_EFLAGS_VM )\\n     {\\n         segr.sel = sel;\\n         segr.base = (uint32_t)sel << 4;', 'file_path': 'xen/arch/x86/hvm/hvm.c'}, {'func_name': 'hvm_task_switch', 'patch': '--- func_before\\n+++ func_after\\n@@ -2995,6 +2995,8 @@\\n     if ( rc != HVMCOPY_okay )\\n         goto out;\\n \\n+    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt, 0) )\\n+        goto out;\\n \\n     if ( hvm_set_cr3(tss.cr3, 1) )\\n         goto out;\\n@@ -3017,13 +3019,12 @@\\n     }\\n \\n     exn_raised = 0;\\n-    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt) ||\\n-         hvm_load_segment_selector(x86_seg_es, tss.es) ||\\n-         hvm_load_segment_selector(x86_seg_cs, tss.cs) ||\\n-         hvm_load_segment_selector(x86_seg_ss, tss.ss) ||\\n-         hvm_load_segment_selector(x86_seg_ds, tss.ds) ||\\n-         hvm_load_segment_selector(x86_seg_fs, tss.fs) ||\\n-         hvm_load_segment_selector(x86_seg_gs, tss.gs) )\\n+    if ( hvm_load_segment_selector(x86_seg_es, tss.es, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_cs, tss.cs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ss, tss.ss, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ds, tss.ds, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_fs, tss.fs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_gs, tss.gs, tss.eflags) )\\n         exn_raised = 1;\\n \\n     rc = hvm_copy_to_guest_virt(', 'file_path': 'xen/arch/x86/hvm/hvm.c'}]\n- **History Summary (all prior context & analysis)**: [\n  {\n    \"stage\": \"initial\",\n    \"result\": {\n      \"language\": \"c\",\n      \"vulnerability_type\": \"logic\",\n      \"repair_strategy\": \"The patch modifies hvm_load_segment_selector to use the provided eflags parameter instead of relying on guest_cpu_user_regs()->eflags (file: xen/arch/x86/hvm/hvm.c, line 2735-2736). It also ensures LDTR is loaded earlier in hvm_task_switch with eflags=0, preventing VM86 mode attributes from being applied (file: xen/arch/x86/hvm/hvm.c, line 2995-2997).\",\n      \"required_context\": [\n        \"function: tss structure population in Xen\",\n        \"caller: hvm_task_switch\"\n      ],\n      \"root_cause\": \"The root cause lies in using VM86 mode attributes when loading the LDTR during task switches. The patch fixes this by: 1) Adding eflags parameter to hvm_load_segment_selector (file: xen/arch/x86/hvm/hvm.c, line 2735) to validate VM86 mode context, 2) Loading LDTR earlier in hvm_task_switch with eflags=0 to avoid VM86 mode contamination (file: xen/arch/x86/hvm/hvm.c, line 2995). GAP: The source of tss.eflags in the TSS structure is not visible in the provided patch, making it unclear how VM86 mode is enforced in the TSS.\"\n    }\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:hvm_task_switch)\",\n        \"result\": [\n          {\n            \"call_line\": 2536,\n            \"call_code\": \"hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)\",\n            \"caller_code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"caller_start\": 2283,\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          },\n          {\n            \"call_line\": 3630,\n            \"call_code\": \"hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)\",\n            \"caller_code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"caller_start\": 3290,\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"stage\": \"analysis\",\n    \"need_context\": true,\n    \"root_cause_partial\": \"The vulnerability arises from using VM86 mode attributes (via guest_cpu_user_regs()->eflags) when loading the LDTR during task switches. The patch: 1) Adds eflags parameter to hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) to validate VM86 mode context, 2) Loads LDTR earlier in hvm_task_switch with eflags=0 (xen/arch/x86/hvm/hvm.c:2995), preventing VM86 mode contamination. The trigger involves task switches where tss.eflags (source unknown in current context) improperly includes VM86 mode flags.\",\n    \"required_context_next\": [\n      \"function: TSS structure population in Xen (specifically tss.eflags initialization)\",\n      \"caller: hvm_task_switch's source of tss.eflags in Xen's SVM/VMX code\"\n    ],\n    \"analysis_summary_text\": \"The root cause is traced to incorrect handling of VM86 mode attributes during task switches, specifically when loading the LDTR. The patch fixes this by ensuring LDTR is loaded with eflags=0 (preventing VM86 mode contamination) and using tss.eflags for other segments. A critical gap remains in understanding how tss.eflags is populated, as this determines whether VM86 mode attributes are improperly applied during task switches. Without this context, the full trigger chain (e.g., how tss.eflags is set) cannot be confirmed.\"\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:hvm_task_switch)\",\n        \"result\": [\n          {\n            \"call_line\": 2536,\n            \"call_code\": \"hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)\",\n            \"caller_code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"caller_start\": 2283,\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          },\n          {\n            \"call_line\": 3630,\n            \"call_code\": \"hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)\",\n            \"caller_code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"caller_start\": 3290,\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"stage\": \"analysis\",\n    \"need_context\": true,\n    \"root_cause_partial\": \"The root cause is traced to hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags (instead of provided eflags) for LDTR loading during task switches. The patch (xen/arch/x86/hvm/hvm.c:2995) loads LDTR earlier with eflags=0, preventing VM86 mode contamination. The remaining gap is understanding how tss.eflags is populated, as this determines whether VM86 mode attributes are improperly applied during task switches.\",\n    \"required_context_next\": [\n      \"function: TSS structure population in Xen (specifically tss.eflags initialization in SVM/VMX code)\",\n      \"caller: How tss.eflags is set in Xen's SVM/VMX task switch handling\"\n    ],\n    \"analysis_summary_text\": \"The vulnerability stems from incorrect handling of VM86 mode attributes during task switches. The patch fixes LDTR loading by using eflags=0 and relying on tss.eflags for other segments. The root cause is traced to improper VM86 mode attribute usage during LDTR loading in hvm_task_switch. However, the source of tss.eflags remains unclear. Callers to hvm_task_switch (svm_vmexit_handler and vmx_vmexit_handler) pass parameters derived from VMCB/VMX exit data, but the TSS population logic (specifically tss.eflags initialization) is not visible in the provided code. This gap prevents confirming how VM86 mode flags propagate into tss.eflags.\"\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:tss_eflags)\",\n        \"result\": \"no valid result, it may be a leaf function\"\n      }\n    ]\n  }\n]\n\n## INSTRUCTIONS\n1. **Review & Integrate Context** \n  Review all `history_summary` (*incl. past analysis and request results*) \n\n2. **Extend Chain from Code**  \n   Trace calls / data flow step-by-step, citing `file:line` for each fact.  \n   Where evidence ends, insert  \n   `GAP: <why evidence stops>`.\n\n3. **Request Missing Context**  \n   If any critical gap remains:  \n     * Always request context and set `\"need_context\": true`. \n     * Add precise, non-duplicate requests to `\"required_context\"` using the following types:  \n      `function`, `caller`, `e.g. caller: I need the caller of function X in file Y`.\n      **DO NOT** request the new patch code.\n     * Try alternative parameters or request types for gaps previously unfulfilled.  \n   If the chain is fully evidenced, set `\"need_context\": false`.\n\n4. **Score Confidence**  \n   *Full chain with direct evidence* ≥ 0.90.  Incomplete chain ≤ 0.75.\n\n## OUTPUT  (JSON only, no trailing commas)\n  ```json\n  {\n      \"analysis\": \"Summary of iteration progress, remaining gaps, explanation for new context requests (if any). If complete, summarize the full chain.\",\n      \"need_context\": true/false, \n      \"required_context\": \"<list of precise context requests>\" ,\n      \"root_cause\": \"<Partial/complete trace with file:line evidence from ALL available context>\",\n      \"confidence_score\": 0.0-1.0\n      // attack_vectors added when confidence >= 0.90 AND need_context is False\n  }\n  ```\n  ## RULES\n  * Output must be valid JSON.\n  * **Source-of-Truth** – Only accept facts visible in code (file:line). Commit message & description are auxiliary.\n  * **Reject every hypothesis or inference** - When evidence is missing, always declare a GAP and request the needed context.\n  * **Tool Restriction** – `required_context` items **must** be of type `function` or `caller` only.\n  * **Stop Condition** – Terminate only when the trigger chain is fully evidenced from the patch; else continue requesting context.\n",
            "result": {
                "analysis": "The vulnerability stems from improper handling of VM86 mode attributes during task switches. The patch fixes LDTR loading by using eflags=0 in hvm_task_switch (xen/arch/x86/hvm/hvm.c:2995) and passing tss.eflags to hvm_load_segment_selector for other segments. The root cause is confirmed for LDTR, but the source of tss.eflags remains unknown. Callers to hvm_task_switch (svm_vmexit_handler and vmx_vmexit_handler) derive parameters from VMCB/VMX exit data, but TSS population logic (specifically tss.eflags initialization) is missing from the provided code. Without this, the trigger chain for tss.eflags propagation cannot be completed.",
                "need_context": true,
                "required_context": [
                    "function: TSS structure population in Xen SVM (how eflags is set in svm_vmexit_handler's TSS)",
                    "function: TSS structure population in Xen VMX (how eflags is set in vmx_vmexit_handler's TSS)"
                ],
                "root_cause": "The root cause lies in hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags instead of the provided eflags parameter for LDTR loading during task switches. The patch loads LDTR earlier with eflags=0 (xen/arch/x86/hvm/hvm.c:2995), preventing VM86 mode contamination. However, tss.eflags (used for other segments) is populated by unknown logic in SVM/VMX code, creating a gap in the trigger chain.",
                "confidence_score": 0.75
            },
            "timestamp": "2025-05-29T19:57:34.359324"
        },
        {
            "stage": "collection",
            "required_context": [
                "function: TSS structure population in Xen SVM (how eflags is set in svm_vmexit_handler's TSS)",
                "function: TSS structure population in Xen VMX (how eflags is set in vmx_vmexit_handler's TSS)"
            ],
            "results": [
                {
                    "tool": "func_info.fetch_func_by_name({'func_name': 'svm_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": [
                        {
                            "type": "function_def",
                            "code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                            "start_line": 2283,
                            "end_line": 2712,
                            "full_name": "svm_vmexit_handler",
                            "file_path": "xen/arch/x86/hvm/svm/svm.c"
                        }
                    ]
                },
                {
                    "tool": "func_info.fetch_func_by_name({'func_name': 'vmx_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": [
                        {
                            "type": "function_def",
                            "code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                            "start_line": 3290,
                            "end_line": 3957,
                            "full_name": "vmx_vmexit_handler",
                            "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                        }
                    ]
                }
            ],
            "timestamp": "2025-05-29T19:57:35.147501"
        },
        {
            "stage": "analysis",
            "iteration": 4,
            "prompt": "You are a senior security engineer. Using **all available code context**, build the **complete, evidence-backed root-cause and trigger chain** for the vulnerability.\n\n## INPUTS\n- **Commit Message**: x86/HVM: don't load LDTR with VM86 mode attrs during task switch\n\nJust like TR, LDTR is purely a protected mode facility and hence needs\nto be loaded accordingly. Also move its loading to where it\narchitecurally belongs.\n\nThis is CVE-2016-9382 / XSA-192.\n\nSigned-off-by: Jan Beulich <jbeulich@suse.com>\nReviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>\nTested-by: Andrew Cooper <andrew.cooper3@citrix.com>\n- **Vulnerability Description**: Xen 4.0.x through 4.7.x mishandle x86 task switches to VM86 mode, which allows local 32-bit x86 HVM guest OS users to gain privileges or cause a denial of service (guest OS crash) by leveraging a guest operating system that uses hardware task switching and allows a new task to start in VM86 mode.\n- **Patch Code**: [{'func_name': 'hvm_load_segment_selector', 'patch': '--- func_before\\n+++ func_after\\n@@ -2735,15 +2735,14 @@\\n static int hvm_load_segment_selector(\\n-    enum x86_segment seg, uint16_t sel)\\n+    enum x86_segment seg, uint16_t sel, unsigned int eflags)\\n {\\n     struct segment_register desctab, cs, segr;\\n     struct desc_struct *pdesc, desc;\\n     u8 dpl, rpl, cpl;\\n     bool_t writable;\\n     int fault_type = TRAP_invalid_tss;\\n-    struct cpu_user_regs *regs = guest_cpu_user_regs();\\n     struct vcpu *v = current;\\n \\n-    if ( regs->eflags & X86_EFLAGS_VM )\\n+    if ( eflags & X86_EFLAGS_VM )\\n     {\\n         segr.sel = sel;\\n         segr.base = (uint32_t)sel << 4;', 'file_path': 'xen/arch/x86/hvm/hvm.c'}, {'func_name': 'hvm_task_switch', 'patch': '--- func_before\\n+++ func_after\\n@@ -2995,6 +2995,8 @@\\n     if ( rc != HVMCOPY_okay )\\n         goto out;\\n \\n+    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt, 0) )\\n+        goto out;\\n \\n     if ( hvm_set_cr3(tss.cr3, 1) )\\n         goto out;\\n@@ -3017,13 +3019,12 @@\\n     }\\n \\n     exn_raised = 0;\\n-    if ( hvm_load_segment_selector(x86_seg_ldtr, tss.ldt) ||\\n-         hvm_load_segment_selector(x86_seg_es, tss.es) ||\\n-         hvm_load_segment_selector(x86_seg_cs, tss.cs) ||\\n-         hvm_load_segment_selector(x86_seg_ss, tss.ss) ||\\n-         hvm_load_segment_selector(x86_seg_ds, tss.ds) ||\\n-         hvm_load_segment_selector(x86_seg_fs, tss.fs) ||\\n-         hvm_load_segment_selector(x86_seg_gs, tss.gs) )\\n+    if ( hvm_load_segment_selector(x86_seg_es, tss.es, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_cs, tss.cs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ss, tss.ss, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_ds, tss.ds, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_fs, tss.fs, tss.eflags) ||\\n+         hvm_load_segment_selector(x86_seg_gs, tss.gs, tss.eflags) )\\n         exn_raised = 1;\\n \\n     rc = hvm_copy_to_guest_virt(', 'file_path': 'xen/arch/x86/hvm/hvm.c'}]\n- **History Summary (all prior context & analysis)**: [\n  {\n    \"stage\": \"initial\",\n    \"result\": {\n      \"language\": \"c\",\n      \"vulnerability_type\": \"logic\",\n      \"repair_strategy\": \"The patch modifies hvm_load_segment_selector to use the provided eflags parameter instead of relying on guest_cpu_user_regs()->eflags (file: xen/arch/x86/hvm/hvm.c, line 2735-2736). It also ensures LDTR is loaded earlier in hvm_task_switch with eflags=0, preventing VM86 mode attributes from being applied (file: xen/arch/x86/hvm/hvm.c, line 2995-2997).\",\n      \"required_context\": [\n        \"function: tss structure population in Xen\",\n        \"caller: hvm_task_switch\"\n      ],\n      \"root_cause\": \"The root cause lies in using VM86 mode attributes when loading the LDTR during task switches. The patch fixes this by: 1) Adding eflags parameter to hvm_load_segment_selector (file: xen/arch/x86/hvm/hvm.c, line 2735) to validate VM86 mode context, 2) Loading LDTR earlier in hvm_task_switch with eflags=0 to avoid VM86 mode contamination (file: xen/arch/x86/hvm/hvm.c, line 2995). GAP: The source of tss.eflags in the TSS structure is not visible in the provided patch, making it unclear how VM86 mode is enforced in the TSS.\"\n    }\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:hvm_task_switch)\",\n        \"result\": [\n          {\n            \"call_line\": 2536,\n            \"call_code\": \"hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)\",\n            \"caller_code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"caller_start\": 2283,\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          },\n          {\n            \"call_line\": 3630,\n            \"call_code\": \"hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)\",\n            \"caller_code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"caller_start\": 3290,\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"stage\": \"analysis\",\n    \"need_context\": true,\n    \"root_cause_partial\": \"The vulnerability arises from using VM86 mode attributes (via guest_cpu_user_regs()->eflags) when loading the LDTR during task switches. The patch: 1) Adds eflags parameter to hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) to validate VM86 mode context, 2) Loads LDTR earlier in hvm_task_switch with eflags=0 (xen/arch/x86/hvm/hvm.c:2995), preventing VM86 mode contamination. The trigger involves task switches where tss.eflags (source unknown in current context) improperly includes VM86 mode flags.\",\n    \"required_context_next\": [\n      \"function: TSS structure population in Xen (specifically tss.eflags initialization)\",\n      \"caller: hvm_task_switch's source of tss.eflags in Xen's SVM/VMX code\"\n    ],\n    \"analysis_summary_text\": \"The root cause is traced to incorrect handling of VM86 mode attributes during task switches, specifically when loading the LDTR. The patch fixes this by ensuring LDTR is loaded with eflags=0 (preventing VM86 mode contamination) and using tss.eflags for other segments. A critical gap remains in understanding how tss.eflags is populated, as this determines whether VM86 mode attributes are improperly applied during task switches. Without this context, the full trigger chain (e.g., how tss.eflags is set) cannot be confirmed.\"\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:hvm_task_switch)\",\n        \"result\": [\n          {\n            \"call_line\": 2536,\n            \"call_code\": \"hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode)\",\n            \"caller_code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"caller_start\": 2283,\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          },\n          {\n            \"call_line\": 3630,\n            \"call_code\": \"hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode)\",\n            \"caller_code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"caller_start\": 3290,\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"stage\": \"analysis\",\n    \"need_context\": true,\n    \"root_cause_partial\": \"The root cause is traced to hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags (instead of provided eflags) for LDTR loading during task switches. The patch (xen/arch/x86/hvm/hvm.c:2995) loads LDTR earlier with eflags=0, preventing VM86 mode contamination. The remaining gap is understanding how tss.eflags is populated, as this determines whether VM86 mode attributes are improperly applied during task switches.\",\n    \"required_context_next\": [\n      \"function: TSS structure population in Xen (specifically tss.eflags initialization in SVM/VMX code)\",\n      \"caller: How tss.eflags is set in Xen's SVM/VMX task switch handling\"\n    ],\n    \"analysis_summary_text\": \"The vulnerability stems from incorrect handling of VM86 mode attributes during task switches. The patch fixes LDTR loading by using eflags=0 and relying on tss.eflags for other segments. The root cause is traced to improper VM86 mode attribute usage during LDTR loading in hvm_task_switch. However, the source of tss.eflags remains unclear. Callers to hvm_task_switch (svm_vmexit_handler and vmx_vmexit_handler) pass parameters derived from VMCB/VMX exit data, but the TSS population logic (specifically tss.eflags initialization) is not visible in the provided code. This gap prevents confirming how VM86 mode flags propagate into tss.eflags.\"\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"caller_info(func:tss_eflags)\",\n        \"result\": \"no valid result, it may be a leaf function\"\n      }\n    ]\n  },\n  {\n    \"stage\": \"analysis\",\n    \"need_context\": true,\n    \"root_cause_partial\": \"The root cause lies in hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags instead of the provided eflags parameter for LDTR loading during task switches. The patch loads LDTR earlier with eflags=0 (xen/arch/x86/hvm/hvm.c:2995), preventing VM86 mode contamination. However, tss.eflags (used for other segments) is populated by unknown logic in SVM/VMX code, creating a gap in the trigger chain.\",\n    \"required_context_next\": [\n      \"function: TSS structure population in Xen SVM (how eflags is set in svm_vmexit_handler's TSS)\",\n      \"function: TSS structure population in Xen VMX (how eflags is set in vmx_vmexit_handler's TSS)\"\n    ],\n    \"analysis_summary_text\": \"The vulnerability stems from improper handling of VM86 mode attributes during task switches. The patch fixes LDTR loading by using eflags=0 in hvm_task_switch (xen/arch/x86/hvm/hvm.c:2995) and passing tss.eflags to hvm_load_segment_selector for other segments. The root cause is confirmed for LDTR, but the source of tss.eflags remains unknown. Callers to hvm_task_switch (svm_vmexit_handler and vmx_vmexit_handler) derive parameters from VMCB/VMX exit data, but TSS population logic (specifically tss.eflags initialization) is missing from the provided code. Without this, the trigger chain for tss.eflags propagation cannot be completed.\"\n  },\n  {\n    \"stage\": \"collection\",\n    \"result\": [\n      {\n        \"source\": \"func_info(func:svm_vmexit_handler)\",\n        \"result\": [\n          {\n            \"code\": \"void svm_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    uint64_t exit_reason;\\n    struct vcpu *v = current;\\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\\n    eventinj_t eventinj;\\n    int inst_len, rc;\\n    vintr_t intr;\\n    bool_t vcpu_guestmode = 0;\\n    struct vlapic *vlapic = vcpu_vlapic(v);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\\n            vmcb_get_cr3(vmcb);\\n\\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\\n        vcpu_guestmode = 1;\\n\\n    /*\\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\\n     * because we update the vTPR on MMIO writes to the TPR.\\n     * NB. We need to preserve the low bits of the TPR to make checked builds\\n     * of Windows work, even though they don't actually do anything.\\n     */\\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\\n    {\\n        intr = vmcb_get_vintr(vmcb);\\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\\n                   ((intr.fields.tpr & 0x0F) << 4) |\\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\\n    }\\n\\n    exit_reason = vmcb->exitcode;\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\\n                    1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip,\\n                    0, 0, 0, 0);\\n\\n    if ( vcpu_guestmode ) {\\n        enum nestedhvm_vmexits nsret;\\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\\n        uint64_t exitinfo1, exitinfo2;\\n\\n        paging_update_nestedmode(v);\\n\\n        /* Write real exitinfo1 back into virtual vmcb.\\n         * nestedsvm_check_intercepts() expects to have the correct\\n         * exitinfo1 value there.\\n         */\\n        exitinfo1 = ns_vmcb->exitinfo1;\\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\\n        switch (nsret) {\\n        case NESTEDHVM_VMEXIT_CONTINUE:\\n            BUG();\\n            break;\\n        case NESTEDHVM_VMEXIT_HOST:\\n            break;\\n        case NESTEDHVM_VMEXIT_INJECT:\\n            /* Switch vcpu from l2 to l1 guest. We must perform\\n             * the switch here to have svm_do_resume() working\\n             * as intended.\\n             */\\n            exitinfo1 = vmcb->exitinfo1;\\n            exitinfo2 = vmcb->exitinfo2;\\n            nv->nv_vmswitch_in_progress = 1;\\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\\n            nv->nv_vmswitch_in_progress = 0;\\n            switch (nsret) {\\n            case NESTEDHVM_VMEXIT_DONE:\\n                /* defer VMEXIT injection */\\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\\n                goto out;\\n            case NESTEDHVM_VMEXIT_FATALERROR:\\n                gdprintk(XENLOG_ERR, \\\"unexpected nestedsvm_vmexit() error\\\\n\\\");\\n                domain_crash(v->domain);\\n                goto out;\\n            default:\\n                BUG();\\n            case NESTEDHVM_VMEXIT_ERROR:\\n                break;\\n            }\\n            /* fallthrough */\\n        case NESTEDHVM_VMEXIT_ERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\\\n\\\");\\n            goto out;\\n        case NESTEDHVM_VMEXIT_FATALERROR:\\n            gdprintk(XENLOG_ERR,\\n                \\\"unexpected nestedsvm_check_intercepts() error\\\\n\\\");\\n            domain_crash(v->domain);\\n            goto out;\\n        default:\\n            gdprintk(XENLOG_INFO, \\\"nestedsvm_check_intercepts() returned %i\\\\n\\\",\\n                nsret);\\n            domain_crash(v->domain);\\n            goto out;\\n        }\\n    }\\n\\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\\n    {\\n        gdprintk(XENLOG_ERR, \\\"invalid VMCB state:\\\\n\\\");\\n        svm_vmcb_dump(__func__, vmcb);\\n        domain_crash(v->domain);\\n        goto out;\\n    }\\n\\n    perfc_incra(svmexits, exit_reason);\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\\n\\n    /* Event delivery caused this intercept? Queue for redelivery. */\\n    eventinj = vmcb->exitintinfo;\\n    if ( unlikely(eventinj.fields.v) &&\\n         hvm_event_needs_reinjection(eventinj.fields.type,\\n                                     eventinj.fields.vector) )\\n        vmcb->eventinj = eventinj;\\n\\n    switch ( exit_reason )\\n    {\\n    case VMEXIT_INTR:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(INTR);\\n        break;\\n\\n    case VMEXIT_NMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(NMI);\\n        break;\\n\\n    case VMEXIT_SMI:\\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n        HVMTRACE_0D(SMI);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_DB:\\n        if ( !v->domain->debugger_attached )\\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_BP:\\n        if ( !v->domain->debugger_attached )\\n            goto unexpected_exit_type;\\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\\n            break;\\n        __update_guest_eip(regs, inst_len);\\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\\n        domain_pause_for_debugger();\\n        break;\\n\\n    case VMEXIT_EXCEPTION_NM:\\n        svm_fpu_dirty_intercept();\\n        break;  \\n\\n    case VMEXIT_EXCEPTION_PF: {\\n        unsigned long va;\\n        va = vmcb->exitinfo2;\\n        regs->error_code = vmcb->exitinfo1;\\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                    \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = paging_fault(va, regs);\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n\\n        if ( rc )\\n        {\\n            if ( trace_will_trace_event(TRC_SHADOW) )\\n                break;\\n            if ( hvm_long_mode_enabled(v) )\\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\\n            else\\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\\n            break;\\n        }\\n\\n        hvm_inject_page_fault(regs->error_code, va);\\n        break;\\n    }\\n\\n    case VMEXIT_EXCEPTION_AC:\\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\\n        break;\\n\\n    case VMEXIT_EXCEPTION_UD:\\n        hvm_ud_intercept(regs);\\n        break;\\n\\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\\n    case VMEXIT_EXCEPTION_MC:\\n        HVMTRACE_0D(MCE);\\n        svm_vmexit_mce_intercept(v, regs);\\n        break;\\n\\n    case VMEXIT_VINTR: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n        intr = vmcb_get_vintr(vmcb);\\n\\n        intr.fields.irq = 0;\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\\n\\n        vmcb_set_vintr(vmcb, intr);\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_INVD:\\n    case VMEXIT_WBINVD:\\n        svm_vmexit_do_invalidate_cache(regs);\\n        break;\\n\\n    case VMEXIT_TASK_SWITCH: {\\n        enum hvm_task_switch_reason reason;\\n        int32_t errcode = -1;\\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\\n            reason = TSW_iret;\\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\\n            reason = TSW_jmp;\\n        else\\n            reason = TSW_call_or_int;\\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\\n            errcode = (uint32_t)vmcb->exitinfo2;\\n\\n        /*\\n         * Some processors set the EXITINTINFO field when the task switch\\n         * is caused by a task gate in the IDT. In this case we will be\\n         * emulating the event injection, so we do not want the processor\\n         * to re-inject the original event!\\n         */\\n        vmcb->eventinj.bytes = 0;\\n\\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\\n        break;\\n    }\\n\\n    case VMEXIT_CPUID:\\n        svm_vmexit_do_cpuid(regs);\\n        break;\\n\\n    case VMEXIT_HLT:\\n        svm_vmexit_do_hlt(vmcb, regs);\\n        break;\\n\\n    case VMEXIT_IOIO:\\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\\n        {\\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\\n            svm_vmexit_do_cr_access(vmcb, regs);\\n        else if ( !handle_mmio() ) \\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPG:\\n        if ( cpu_has_svm_decode )\\n        {\\n            svm_invlpg_intercept(vmcb->exitinfo1);\\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\\n        }\\n        else if ( !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case VMEXIT_INVLPGA:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\\n            break;\\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\\n        __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_VMMCALL:\\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\\n            break;\\n        BUG_ON(vcpu_guestmode);\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            __update_guest_eip(regs, inst_len);\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n\\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\\n        svm_dr_access(v, regs);\\n        break;\\n\\n    case VMEXIT_MSR:\\n        svm_do_msr_access(regs);\\n        break;\\n\\n    case VMEXIT_SHUTDOWN:\\n        hvm_triple_fault();\\n        break;\\n\\n    case VMEXIT_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case VMEXIT_RDTSC:\\n        svm_vmexit_do_rdtsc(regs);\\n        break;\\n\\n    case VMEXIT_MONITOR:\\n    case VMEXIT_MWAIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_VMRUN:\\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMLOAD:\\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_VMSAVE:\\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\\n        break;\\n    case VMEXIT_STGI:\\n        svm_vmexit_do_stgi(regs, v);\\n        break;\\n    case VMEXIT_CLGI:\\n        svm_vmexit_do_clgi(regs, v);\\n        break;\\n    case VMEXIT_SKINIT:\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case VMEXIT_XSETBV:\\n        if ( vmcb_get_cpl(vmcb) )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\\n                  hvm_handle_xsetbv(regs->ecx,\\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\\n            __update_guest_eip(regs, inst_len);\\n        break;\\n\\n    case VMEXIT_NPF:\\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\\n        if ( cpu_has_svm_decode )\\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\\n        rc = vmcb->exitinfo1 & PFEC_page_present\\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\\n        if ( rc >= 0 )\\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\\n        else\\n        {\\n            printk(XENLOG_G_ERR\\n                   \\\"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\\\n\\\",\\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\\n            domain_crash(v->domain);\\n        }\\n        v->arch.hvm_svm.cached_insn_len = 0;\\n        break;\\n\\n    case VMEXIT_IRET: {\\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\\n\\n        /*\\n         * IRET clears the NMI mask. However because we clear the mask\\n         * /before/ executing IRET, we set the interrupt shadow to prevent\\n         * a pending NMI from being injected immediately. This will work\\n         * perfectly unless the IRET instruction faults: in that case we\\n         * may inject an NMI before the NMI handler's IRET instruction is\\n         * retired.\\n         */\\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\\n        vmcb->interrupt_shadow = 1;\\n\\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\\n        break;\\n    }\\n\\n    case VMEXIT_PAUSE:\\n        svm_vmexit_do_pause(regs);\\n        break;\\n\\n    default:\\n    unexpected_exit_type:\\n        gdprintk(XENLOG_ERR, \\\"unexpected VMEXIT: exit reason = %#\\\"PRIx64\\\", \\\"\\n                 \\\"exitinfo1 = %#\\\"PRIx64\\\", exitinfo2 = %#\\\"PRIx64\\\"\\\\n\\\",\\n                 exit_reason, \\n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\\n        svm_crash_or_fault(v);\\n        break;\\n    }\\n\\n  out:\\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\\n        return;\\n\\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\\n    intr = vmcb_get_vintr(vmcb);\\n    intr.fields.tpr =\\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\\n    vmcb_set_vintr(vmcb, intr);\\n}\\n\",\n            \"start_line\": 2283,\n            \"end_line\": 2712,\n            \"full_name\": \"svm_vmexit_handler\",\n            \"file_path\": \"xen/arch/x86/hvm/svm/svm.c\"\n          }\n        ]\n      },\n      {\n        \"source\": \"func_info(func:vmx_vmexit_handler)\",\n        \"result\": [\n          {\n            \"code\": \"void vmx_vmexit_handler(struct cpu_user_regs *regs)\\n{\\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\\n    unsigned int vector = 0, mode;\\n    struct vcpu *v = current;\\n\\n    __vmread(GUEST_RIP,    &regs->rip);\\n    __vmread(GUEST_RSP,    &regs->rsp);\\n    __vmread(GUEST_RFLAGS, &regs->rflags);\\n\\n    hvm_invalidate_regs_fields(regs);\\n\\n    if ( paging_mode_hap(v->domain) )\\n    {\\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\\n    }\\n\\n    __vmread(VM_EXIT_REASON, &exit_reason);\\n\\n    if ( hvm_long_mode_enabled(v) )\\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\\n                    0, 0, 0);\\n    else\\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\\n                    (uint32_t)regs->eip, \\n                    0, 0, 0, 0);\\n\\n    perfc_incra(vmexits, exit_reason);\\n\\n    /* Handle the interrupt we missed before allowing any more in. */\\n    switch ( (uint16_t)exit_reason )\\n    {\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        vmx_do_extint(regs);\\n        break;\\n    case EXIT_REASON_EXCEPTION_NMI:\\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\\n        if ( vector == TRAP_machine_check )\\n            do_machine_check(regs);\\n        if ( (vector == TRAP_nmi) &&\\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\\n        {\\n            exception_table[TRAP_nmi](regs);\\n            enable_nmis();\\n        }\\n        break;\\n    case EXIT_REASON_MCE_DURING_VMENTRY:\\n        do_machine_check(regs);\\n        break;\\n    }\\n\\n    /* Now enable interrupts so it's safe to take locks. */\\n    local_irq_enable();\\n\\n    /*\\n     * If the guest has the ability to switch EPTP without an exit,\\n     * figure out whether it has done so and update the altp2m data.\\n     */\\n    if ( altp2m_active(v->domain) &&\\n        (v->arch.hvm_vmx.secondary_exec_control &\\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\\n    {\\n        unsigned long idx;\\n\\n        if ( v->arch.hvm_vmx.secondary_exec_control &\\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\\n            __vmread(EPTP_INDEX, &idx);\\n        else\\n        {\\n            unsigned long eptp;\\n\\n            __vmread(EPT_POINTER, &eptp);\\n\\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\\n                 INVALID_ALTP2M )\\n            {\\n                gdprintk(XENLOG_ERR, \\\"EPTP not found in alternate p2m list\\\\n\\\");\\n                domain_crash(v->domain);\\n            }\\n        }\\n\\n        if ( idx != vcpu_altp2m(v).p2midx )\\n        {\\n            BUG_ON(idx >= MAX_ALTP2M);\\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\\n            vcpu_altp2m(v).p2midx = idx;\\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\\n        }\\n    }\\n\\n    /* XXX: This looks ugly, but we need a mechanism to ensure\\n     * any pending vmresume has really happened\\n     */\\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n    {\\n        paging_update_nestedmode(v);\\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\\n            goto out;\\n    }\\n\\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\\n        return vmx_failed_vmentry(exit_reason, regs);\\n\\n    if ( v->arch.hvm_vmx.vmx_realmode )\\n    {\\n        /* Put RFLAGS back the way the guest wants it */\\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\\n\\n        /* Unless this exit was for an interrupt, we've hit something\\n         * vm86 can't handle.  Try again, using the emulator. */\\n        switch ( exit_reason )\\n        {\\n        case EXIT_REASON_EXCEPTION_NMI:\\n            if ( vector != TRAP_page_fault\\n                 && vector != TRAP_nmi \\n                 && vector != TRAP_machine_check ) \\n            {\\n        default:\\n                perfc_incr(realmode_exits);\\n                v->arch.hvm_vmx.vmx_emulate = 1;\\n                HVMTRACE_0D(REALMODE_EMULATE);\\n                return;\\n            }\\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        case EXIT_REASON_INIT:\\n        case EXIT_REASON_SIPI:\\n        case EXIT_REASON_PENDING_VIRT_INTR:\\n        case EXIT_REASON_PENDING_VIRT_NMI:\\n        case EXIT_REASON_MCE_DURING_VMENTRY:\\n        case EXIT_REASON_GETSEC:\\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n        case EXIT_REASON_INVEPT:\\n        case EXIT_REASON_INVVPID:\\n            break;\\n        }\\n    }\\n\\n    hvm_maybe_deassert_evtchn_irq();\\n\\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\\n        vmx_idtv_reinject(idtv_info);\\n\\n    switch ( exit_reason )\\n    {\\n        unsigned long ecode;\\n\\n    case EXIT_REASON_EXCEPTION_NMI:\\n    {\\n        /*\\n         * We don't set the software-interrupt exiting (INT n).\\n         * (1) We can get an exception (e.g. #PG) in the guest, or\\n         * (2) NMI\\n         */\\n\\n        /*\\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\\n         * 25.7.1.2, \\\"Resuming Guest Software after Handling an Exception\\\").\\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\\n         */\\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\\n             (vector != TRAP_double_fault) )\\n        {\\n            unsigned long guest_info;\\n\\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\\n                      guest_info | VMX_INTR_SHADOW_NMI);\\n        }\\n\\n        perfc_incra(cause_vector, vector);\\n\\n        switch ( vector )\\n        {\\n        case TRAP_debug:\\n            /*\\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\\n             * Table 23-1, \\\"Exit Qualification for Debug Exceptions\\\").\\n             */\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len = 0;\\n                int rc;\\n                unsigned long trap_type = MASK_EXTR(intr_info,\\n                                                    INTR_INFO_INTR_TYPE_MASK);\\n\\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\\n                                       trap_type, insn_len);\\n\\n                /*\\n                 * rc < 0 error in monitor/vm_event, crash\\n                 * !rc    continue normally\\n                 * rc > 0 paused waiting for response, work here is done\\n                 */\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n                domain_pause_for_debugger();\\n            break;\\n        case TRAP_int3:\\n            HVMTRACE_1D(TRAP, vector);\\n            if ( !v->domain->debugger_attached )\\n            {\\n                unsigned long insn_len;\\n                int rc;\\n\\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\\n                rc = hvm_monitor_debug(regs->eip,\\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\\n                                       X86_EVENTTYPE_SW_EXCEPTION,\\n                                       insn_len);\\n\\n                if ( rc < 0 )\\n                    goto exit_and_crash;\\n                if ( !rc )\\n                    vmx_propagate_intr(intr_info);\\n            }\\n            else\\n            {\\n                update_guest_eip(); /* Safe: INT3 */\\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\\n                domain_pause_for_debugger();\\n            }\\n            break;\\n        case TRAP_no_device:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_fpu_dirty_intercept();\\n            break;\\n        case TRAP_page_fault:\\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\\n            regs->error_code = ecode;\\n\\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\\n                        \\\"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\\\",\\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\\n\\n            if ( paging_fault(exit_qualification, regs) )\\n            {\\n                if ( trace_will_trace_event(TRC_SHADOW) )\\n                    break;\\n                if ( hvm_long_mode_enabled(v) )\\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\\n                                     TRC_PAR_LONG(exit_qualification) );\\n                else\\n                    HVMTRACE_2D(PF_XEN,\\n                                regs->error_code, exit_qualification );\\n                break;\\n            }\\n\\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\\n            break;\\n        case TRAP_alignment_check:\\n            HVMTRACE_1D(TRAP, vector);\\n            vmx_propagate_intr(intr_info);\\n            break;\\n        case TRAP_nmi:\\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\\n                 X86_EVENTTYPE_NMI )\\n                goto exit_and_crash;\\n            HVMTRACE_0D(NMI);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_machine_check:\\n            HVMTRACE_0D(MCE);\\n            /* Already handled above. */\\n            break;\\n        case TRAP_invalid_op:\\n            HVMTRACE_1D(TRAP, vector);\\n            hvm_ud_intercept(regs);\\n            break;\\n        default:\\n            HVMTRACE_1D(TRAP, vector);\\n            goto exit_and_crash;\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\\n        /* Already handled above. */\\n        break;\\n    case EXIT_REASON_TRIPLE_FAULT:\\n        hvm_triple_fault();\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_INTR:\\n        /* Disable the interrupt window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_PENDING_VIRT_NMI:\\n        /* Disable the NMI window. */\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\\n        vmx_update_cpu_exec_control(v);\\n        break;\\n    case EXIT_REASON_TASK_SWITCH: {\\n        static const enum hvm_task_switch_reason reasons[] = {\\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\\n        };\\n        unsigned int inst_len, source;\\n\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        source = (exit_qualification >> 30) & 3;\\n        /* Vectored event should fill in interrupt information. */\\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\\n        /*\\n         * In the following cases there is an instruction to skip over:\\n         *  - TSW is due to a CALL, IRET or JMP instruction.\\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\\n         */\\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\\n                     > 3)) /* IntrType > 3? */\\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\\n        else\\n             ecode = -1;\\n        regs->eip += inst_len;\\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\\n        break;\\n    }\\n    case EXIT_REASON_CPUID:\\n    {\\n        int rc;\\n\\n        if ( is_pvh_vcpu(v) )\\n        {\\n            pv_cpuid(regs);\\n            rc = 0;\\n        }\\n        else\\n            rc = vmx_do_cpuid(regs);\\n\\n        /*\\n         * rc < 0 error in monitor/vm_event, crash\\n         * !rc    continue normally\\n         * rc > 0 paused waiting for response, work here is done\\n         */\\n        if ( rc < 0 )\\n            goto exit_and_crash;\\n        if ( !rc )\\n            update_guest_eip(); /* Safe: CPUID */\\n        break;\\n    }\\n    case EXIT_REASON_HLT:\\n        update_guest_eip(); /* Safe: HLT */\\n        hvm_hlt(regs->eflags);\\n        break;\\n    case EXIT_REASON_INVLPG:\\n        update_guest_eip(); /* Safe: INVLPG */\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_invlpg_intercept(exit_qualification);\\n        break;\\n    case EXIT_REASON_RDTSCP:\\n        regs->ecx = hvm_msr_tsc_aux(v);\\n        /* fall through */\\n    case EXIT_REASON_RDTSC:\\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\\n        hvm_rdtsc_intercept(regs);\\n        break;\\n    case EXIT_REASON_VMCALL:\\n    {\\n        int rc;\\n        HVMTRACE_1D(VMMCALL, regs->eax);\\n        rc = hvm_do_hypercall(regs);\\n        if ( rc != HVM_HCALL_preempted )\\n        {\\n            update_guest_eip(); /* Safe: VMCALL */\\n            if ( rc == HVM_HCALL_invalidate )\\n                send_invalidate_req();\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_CR_ACCESS:\\n    {\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\\n        break;\\n    }\\n    case EXIT_REASON_DR_ACCESS:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        vmx_dr_access(exit_qualification, regs);\\n        break;\\n    case EXIT_REASON_MSR_READ:\\n    {\\n        uint64_t msr_content;\\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\\n        {\\n            regs->eax = (uint32_t)msr_content;\\n            regs->edx = (uint32_t)(msr_content >> 32);\\n            update_guest_eip(); /* Safe: RDMSR */\\n        }\\n        break;\\n    }\\n    case EXIT_REASON_MSR_WRITE:\\n    {\\n        uint64_t msr_content;\\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\\n            update_guest_eip(); /* Safe: WRMSR */\\n        break;\\n    }\\n\\n    case EXIT_REASON_VMXOFF:\\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMXON:\\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMCLEAR:\\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMPTRLD:\\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMPTRST:\\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMREAD:\\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n \\n    case EXIT_REASON_VMWRITE:\\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMLAUNCH:\\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMRESUME:\\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVEPT:\\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_INVVPID:\\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_VMFUNC:\\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        else\\n            update_guest_eip();\\n        break;\\n\\n    case EXIT_REASON_MWAIT_INSTRUCTION:\\n    case EXIT_REASON_MONITOR_INSTRUCTION:\\n    case EXIT_REASON_GETSEC:\\n        /*\\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\\n         * running in guest context, and the CPU checks that before getting\\n         * as far as vmexit.\\n         */\\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\\n        break;\\n\\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\\n        break;\\n\\n    case EXIT_REASON_APIC_ACCESS:\\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        break;\\n\\n    case EXIT_REASON_EOI_INDUCED:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n\\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\\n\\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\\n        break;\\n\\n    case EXIT_REASON_IO_INSTRUCTION:\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        if ( exit_qualification & 0x10 )\\n        {\\n            /* INS, OUTS */\\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\\n                 !handle_mmio() )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n        }\\n        else\\n        {\\n            /* IN, OUT */\\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\\n            int bytes = (exit_qualification & 0x07) + 1;\\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\\n            if ( handle_pio(port, bytes, dir) )\\n                update_guest_eip(); /* Safe: IN, OUT */\\n        }\\n        break;\\n\\n    case EXIT_REASON_INVD:\\n    case EXIT_REASON_WBINVD:\\n    {\\n        update_guest_eip(); /* Safe: INVD, WBINVD */\\n        vmx_wbinvd_intercept();\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_VIOLATION:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\\n        ept_handle_violation(exit_qualification, gpa);\\n        break;\\n    }\\n\\n    case EXIT_REASON_EPT_MISCONFIG:\\n    {\\n        paddr_t gpa;\\n\\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\\n        if ( !ept_handle_misconfig(gpa) )\\n            goto exit_and_crash;\\n        break;\\n    }\\n\\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\\n        vmx_update_cpu_exec_control(v);\\n        if ( v->arch.hvm_vcpu.single_step )\\n        {\\n            hvm_monitor_debug(regs->eip,\\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\\n                              0, 0);\\n\\n            if ( v->domain->debugger_attached )\\n                domain_pause_for_debugger();\\n        }\\n\\n        break;\\n\\n    case EXIT_REASON_PAUSE_INSTRUCTION:\\n        perfc_incr(pauseloop_exits);\\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\\n        break;\\n\\n    case EXIT_REASON_XSETBV:\\n        if ( hvm_handle_xsetbv(regs->ecx,\\n                               (regs->rdx << 32) | regs->_eax) == 0 )\\n            update_guest_eip(); /* Safe: XSETBV */\\n        break;\\n\\n    case EXIT_REASON_APIC_WRITE:\\n        vmx_handle_apic_write();\\n        break;\\n\\n    case EXIT_REASON_PML_FULL:\\n        vmx_vcpu_flush_pml_buffer(v);\\n        break;\\n\\n    case EXIT_REASON_XSAVES:\\n        vmx_handle_xsaves();\\n        break;\\n\\n    case EXIT_REASON_XRSTORS:\\n        vmx_handle_xrstors();\\n        break;\\n\\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\\n    case EXIT_REASON_INVPCID:\\n    /* fall through */\\n    default:\\n    exit_and_crash:\\n        {\\n            struct segment_register ss;\\n\\n            gdprintk(XENLOG_WARNING, \\\"Bad vmexit (reason %#lx)\\\\n\\\",\\n                     exit_reason);\\n\\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\\n            if ( ss.attr.fields.dpl )\\n                hvm_inject_hw_exception(TRAP_invalid_op,\\n                                        HVM_DELIVER_NO_ERROR_CODE);\\n            else\\n                domain_crash(v->domain);\\n        }\\n        break;\\n    }\\n\\nout:\\n    if ( nestedhvm_vcpu_in_guestmode(v) )\\n        nvmx_idtv_handling();\\n\\n    /*\\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\\n     * criteria. As we must not allow less than fully privileged mode to have\\n     * such an effect on the domain, we correct rIP in that case (accepting\\n     * this not being architecturally correct behavior, as the injected #GP\\n     * fault will then not see the correct [invalid] return address).\\n     * And since we know the guest will crash, we crash it right away if it\\n     * already is in most privileged mode.\\n     */\\n    mode = vmx_guest_x86_mode(v);\\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\\n                   : regs->rip != regs->_eip )\\n    {\\n        struct segment_register ss;\\n\\n        gprintk(XENLOG_WARNING, \\\"Bad rIP %lx for mode %u\\\\n\\\", regs->rip, mode);\\n\\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\\n        if ( ss.attr.fields.dpl )\\n        {\\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\\n            /* Need to fix rIP nevertheless. */\\n            if ( mode == 8 )\\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\\n                            (64 - VADDR_BITS);\\n            else\\n                regs->rip = regs->_eip;\\n        }\\n        else\\n            domain_crash(v->domain);\\n    }\\n}\\n\",\n            \"start_line\": 3290,\n            \"end_line\": 3957,\n            \"full_name\": \"vmx_vmexit_handler\",\n            \"file_path\": \"xen/arch/x86/hvm/vmx/vmx.c\"\n          }\n        ]\n      }\n    ]\n  }\n]\n\n## INSTRUCTIONS\n1. **Review & Integrate Context** \n  Review all `history_summary` (*incl. past analysis and request results*) \n\n2. **Extend Chain from Code**  \n   Trace calls / data flow step-by-step, citing `file:line` for each fact.  \n   Where evidence ends, insert  \n   `GAP: <why evidence stops>`.\n\n3. **Request Missing Context**  \n   If any critical gap remains:  \n     * Always request context and set `\"need_context\": true`. \n     * Add precise, non-duplicate requests to `\"required_context\"` using the following types:  \n      `function`, `caller`, `e.g. caller: I need the caller of function X in file Y`.\n      **DO NOT** request the new patch code.\n     * Try alternative parameters or request types for gaps previously unfulfilled.  \n   If the chain is fully evidenced, set `\"need_context\": false`.\n\n4. **Score Confidence**  \n   *Full chain with direct evidence* ≥ 0.90.  Incomplete chain ≤ 0.75.\n\n## OUTPUT  (JSON only, no trailing commas)\n  ```json\n  {\n      \"analysis\": \"Summary of iteration progress, remaining gaps, explanation for new context requests (if any). If complete, summarize the full chain.\",\n      \"need_context\": true/false, \n      \"required_context\": \"<list of precise context requests>\" ,\n      \"root_cause\": \"<Partial/complete trace with file:line evidence from ALL available context>\",\n      \"confidence_score\": 0.0-1.0\n      // attack_vectors added when confidence >= 0.90 AND need_context is False\n  }\n  ```\n  ## RULES\n  * Output must be valid JSON.\n  * **Source-of-Truth** – Only accept facts visible in code (file:line). Commit message & description are auxiliary.\n  * **Reject every hypothesis or inference** - When evidence is missing, always declare a GAP and request the needed context.\n  * **Tool Restriction** – `required_context` items **must** be of type `function` or `caller` only.\n  * **Stop Condition** – Terminate only when the trigger chain is fully evidenced from the patch; else continue requesting context.\n",
            "result": {
                "analysis": "The root cause is confirmed for LDTR loading, but the source of tss.eflags remains untraced. The patch ensures LDTR is loaded with eflags=0 (preventing VM86 contamination) in hvm_task_switch (xen/arch/x86/hvm/hvm.c:2995), while other segments use tss.eflags. However, the TSS population logic for eflags in SVM/VMX code is missing from the provided context. Callers to hvm_task_switch (svm_vmexit_handler and vmx_vmexit_handler) derive parameters from VMCB/VMX exit data but do not show how tss.eflags is initialized. Without this, the full trigger chain (e.g., how VM86 flags propagate into tss.eflags) cannot be completed.",
                "need_context": true,
                "required_context": [
                    "function: TSS structure population in Xen SVM (how eflags is set in svm_vmexit_handler's TSS)",
                    "function: TSS structure population in Xen VMX (how eflags is set in vmx_vmexit_handler's TSS)"
                ],
                "root_cause": "The vulnerability arises from hvm_load_segment_selector (xen/arch/x86/hvm/hvm.c:2735) using guest_cpu_user_regs()->eflags for LDTR loading during task switches. The patch fixes this by loading LDTR earlier with eflags=0 (xen/arch/x86/hvm/hvm.c:2995). However, tss.eflags (used for other segments) is populated by unknown logic in SVM/VMX code, creating a gap in the trigger chain.",
                "confidence_score": 0.75
            },
            "timestamp": "2025-05-29T19:57:35.446561"
        },
        {
            "stage": "collection",
            "required_context": [
                "function: TSS structure population in Xen SVM (how eflags is set in svm_vmexit_handler's TSS)",
                "function: TSS structure population in Xen VMX (how eflags is set in vmx_vmexit_handler's TSS)"
            ],
            "results": [
                {
                    "tool": "func_info.fetch_func_by_name({'func_name': 'svm_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": [
                        {
                            "type": "function_def",
                            "code": "void svm_vmexit_handler(struct cpu_user_regs *regs)\n{\n    uint64_t exit_reason;\n    struct vcpu *v = current;\n    struct vmcb_struct *vmcb = v->arch.hvm_svm.vmcb;\n    eventinj_t eventinj;\n    int inst_len, rc;\n    vintr_t intr;\n    bool_t vcpu_guestmode = 0;\n    struct vlapic *vlapic = vcpu_vlapic(v);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n        v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3] =\n            vmcb_get_cr3(vmcb);\n\n    if ( nestedhvm_enabled(v->domain) && nestedhvm_vcpu_in_guestmode(v) )\n        vcpu_guestmode = 1;\n\n    /*\n     * Before doing anything else, we need to sync up the VLAPIC's TPR with\n     * SVM's vTPR. It's OK if the guest doesn't touch CR8 (e.g. 32-bit Windows)\n     * because we update the vTPR on MMIO writes to the TPR.\n     * NB. We need to preserve the low bits of the TPR to make checked builds\n     * of Windows work, even though they don't actually do anything.\n     */\n    if ( !vcpu_guestmode && !vlapic_hw_disabled(vlapic) )\n    {\n        intr = vmcb_get_vintr(vmcb);\n        vlapic_set_reg(vlapic, APIC_TASKPRI,\n                   ((intr.fields.tpr & 0x0F) << 4) |\n                   (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0x0F));\n    }\n\n    exit_reason = vmcb->exitcode;\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, vcpu_guestmode ? TRC_HVM_NESTEDFLAG : 0,\n                    1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip,\n                    0, 0, 0, 0);\n\n    if ( vcpu_guestmode ) {\n        enum nestedhvm_vmexits nsret;\n        struct nestedvcpu *nv = &vcpu_nestedhvm(v);\n        struct vmcb_struct *ns_vmcb = nv->nv_vvmcx;\n        uint64_t exitinfo1, exitinfo2;\n\n        paging_update_nestedmode(v);\n\n        /* Write real exitinfo1 back into virtual vmcb.\n         * nestedsvm_check_intercepts() expects to have the correct\n         * exitinfo1 value there.\n         */\n        exitinfo1 = ns_vmcb->exitinfo1;\n        ns_vmcb->exitinfo1 = vmcb->exitinfo1;\n        nsret = nestedsvm_check_intercepts(v, regs, exit_reason);\n        switch (nsret) {\n        case NESTEDHVM_VMEXIT_CONTINUE:\n            BUG();\n            break;\n        case NESTEDHVM_VMEXIT_HOST:\n            break;\n        case NESTEDHVM_VMEXIT_INJECT:\n            /* Switch vcpu from l2 to l1 guest. We must perform\n             * the switch here to have svm_do_resume() working\n             * as intended.\n             */\n            exitinfo1 = vmcb->exitinfo1;\n            exitinfo2 = vmcb->exitinfo2;\n            nv->nv_vmswitch_in_progress = 1;\n            nsret = nestedsvm_vmexit_n2n1(v, regs);\n            nv->nv_vmswitch_in_progress = 0;\n            switch (nsret) {\n            case NESTEDHVM_VMEXIT_DONE:\n                /* defer VMEXIT injection */\n                nestedsvm_vmexit_defer(v, exit_reason, exitinfo1, exitinfo2);\n                goto out;\n            case NESTEDHVM_VMEXIT_FATALERROR:\n                gdprintk(XENLOG_ERR, \"unexpected nestedsvm_vmexit() error\\n\");\n                domain_crash(v->domain);\n                goto out;\n            default:\n                BUG();\n            case NESTEDHVM_VMEXIT_ERROR:\n                break;\n            }\n            /* fallthrough */\n        case NESTEDHVM_VMEXIT_ERROR:\n            gdprintk(XENLOG_ERR,\n                \"nestedsvm_check_intercepts() returned NESTEDHVM_VMEXIT_ERROR\\n\");\n            goto out;\n        case NESTEDHVM_VMEXIT_FATALERROR:\n            gdprintk(XENLOG_ERR,\n                \"unexpected nestedsvm_check_intercepts() error\\n\");\n            domain_crash(v->domain);\n            goto out;\n        default:\n            gdprintk(XENLOG_INFO, \"nestedsvm_check_intercepts() returned %i\\n\",\n                nsret);\n            domain_crash(v->domain);\n            goto out;\n        }\n    }\n\n    if ( unlikely(exit_reason == VMEXIT_INVALID) )\n    {\n        gdprintk(XENLOG_ERR, \"invalid VMCB state:\\n\");\n        svm_vmcb_dump(__func__, vmcb);\n        domain_crash(v->domain);\n        goto out;\n    }\n\n    perfc_incra(svmexits, exit_reason);\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    vmcb->cleanbits.bytes = cpu_has_svm_cleanbits ? ~0u : 0u;\n\n    /* Event delivery caused this intercept? Queue for redelivery. */\n    eventinj = vmcb->exitintinfo;\n    if ( unlikely(eventinj.fields.v) &&\n         hvm_event_needs_reinjection(eventinj.fields.type,\n                                     eventinj.fields.vector) )\n        vmcb->eventinj = eventinj;\n\n    switch ( exit_reason )\n    {\n    case VMEXIT_INTR:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(INTR);\n        break;\n\n    case VMEXIT_NMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(NMI);\n        break;\n\n    case VMEXIT_SMI:\n        /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n        HVMTRACE_0D(SMI);\n        break;\n\n    case VMEXIT_EXCEPTION_DB:\n        if ( !v->domain->debugger_attached )\n            hvm_inject_hw_exception(TRAP_debug, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_BP:\n        if ( !v->domain->debugger_attached )\n            goto unexpected_exit_type;\n        /* AMD Vol2, 15.11: INT3, INTO, BOUND intercepts do not update RIP. */\n        if ( (inst_len = __get_instruction_length(v, INSTR_INT3)) == 0 )\n            break;\n        __update_guest_eip(regs, inst_len);\n        current->arch.gdbsx_vcpu_event = TRAP_int3;\n        domain_pause_for_debugger();\n        break;\n\n    case VMEXIT_EXCEPTION_NM:\n        svm_fpu_dirty_intercept();\n        break;  \n\n    case VMEXIT_EXCEPTION_PF: {\n        unsigned long va;\n        va = vmcb->exitinfo2;\n        regs->error_code = vmcb->exitinfo1;\n        HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                    \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                    (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                    (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                    (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = paging_fault(va, regs);\n        v->arch.hvm_svm.cached_insn_len = 0;\n\n        if ( rc )\n        {\n            if ( trace_will_trace_event(TRC_SHADOW) )\n                break;\n            if ( hvm_long_mode_enabled(v) )\n                HVMTRACE_LONG_2D(PF_XEN, regs->error_code, TRC_PAR_LONG(va));\n            else\n                HVMTRACE_2D(PF_XEN, regs->error_code, va);\n            break;\n        }\n\n        hvm_inject_page_fault(regs->error_code, va);\n        break;\n    }\n\n    case VMEXIT_EXCEPTION_AC:\n        HVMTRACE_1D(TRAP, TRAP_alignment_check);\n        hvm_inject_hw_exception(TRAP_alignment_check, vmcb->exitinfo1);\n        break;\n\n    case VMEXIT_EXCEPTION_UD:\n        hvm_ud_intercept(regs);\n        break;\n\n    /* Asynchronous event, handled when we STGI'd after the VMEXIT. */\n    case VMEXIT_EXCEPTION_MC:\n        HVMTRACE_0D(MCE);\n        svm_vmexit_mce_intercept(v, regs);\n        break;\n\n    case VMEXIT_VINTR: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n        intr = vmcb_get_vintr(vmcb);\n\n        intr.fields.irq = 0;\n        general1_intercepts &= ~GENERAL1_INTERCEPT_VINTR;\n\n        vmcb_set_vintr(vmcb, intr);\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_INVD:\n    case VMEXIT_WBINVD:\n        svm_vmexit_do_invalidate_cache(regs);\n        break;\n\n    case VMEXIT_TASK_SWITCH: {\n        enum hvm_task_switch_reason reason;\n        int32_t errcode = -1;\n        if ( (vmcb->exitinfo2 >> 36) & 1 )\n            reason = TSW_iret;\n        else if ( (vmcb->exitinfo2 >> 38) & 1 )\n            reason = TSW_jmp;\n        else\n            reason = TSW_call_or_int;\n        if ( (vmcb->exitinfo2 >> 44) & 1 )\n            errcode = (uint32_t)vmcb->exitinfo2;\n\n        /*\n         * Some processors set the EXITINTINFO field when the task switch\n         * is caused by a task gate in the IDT. In this case we will be\n         * emulating the event injection, so we do not want the processor\n         * to re-inject the original event!\n         */\n        vmcb->eventinj.bytes = 0;\n\n        hvm_task_switch((uint16_t)vmcb->exitinfo1, reason, errcode);\n        break;\n    }\n\n    case VMEXIT_CPUID:\n        svm_vmexit_do_cpuid(regs);\n        break;\n\n    case VMEXIT_HLT:\n        svm_vmexit_do_hlt(vmcb, regs);\n        break;\n\n    case VMEXIT_IOIO:\n        if ( (vmcb->exitinfo1 & (1u<<2)) == 0 )\n        {\n            uint16_t port = (vmcb->exitinfo1 >> 16) & 0xFFFF;\n            int bytes = ((vmcb->exitinfo1 >> 4) & 0x07);\n            int dir = (vmcb->exitinfo1 & 1) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                __update_guest_eip(regs, vmcb->exitinfo2 - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_CR0_READ ... VMEXIT_CR15_READ:\n    case VMEXIT_CR0_WRITE ... VMEXIT_CR15_WRITE:\n        if ( cpu_has_svm_decode && (vmcb->exitinfo1 & (1ULL << 63)) )\n            svm_vmexit_do_cr_access(vmcb, regs);\n        else if ( !handle_mmio() ) \n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPG:\n        if ( cpu_has_svm_decode )\n        {\n            svm_invlpg_intercept(vmcb->exitinfo1);\n            __update_guest_eip(regs, vmcb->nextrip - vmcb->rip);\n        }\n        else if ( !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case VMEXIT_INVLPGA:\n        if ( (inst_len = __get_instruction_length(v, INSTR_INVLPGA)) == 0 )\n            break;\n        svm_invlpga_intercept(v, regs->eax, regs->ecx);\n        __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_VMMCALL:\n        if ( (inst_len = __get_instruction_length(v, INSTR_VMCALL)) == 0 )\n            break;\n        BUG_ON(vcpu_guestmode);\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            __update_guest_eip(regs, inst_len);\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n\n    case VMEXIT_DR0_READ ... VMEXIT_DR7_READ:\n    case VMEXIT_DR0_WRITE ... VMEXIT_DR7_WRITE:\n        svm_dr_access(v, regs);\n        break;\n\n    case VMEXIT_MSR:\n        svm_do_msr_access(regs);\n        break;\n\n    case VMEXIT_SHUTDOWN:\n        hvm_triple_fault();\n        break;\n\n    case VMEXIT_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case VMEXIT_RDTSC:\n        svm_vmexit_do_rdtsc(regs);\n        break;\n\n    case VMEXIT_MONITOR:\n    case VMEXIT_MWAIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_VMRUN:\n        svm_vmexit_do_vmrun(regs, v, regs->eax);\n        break;\n    case VMEXIT_VMLOAD:\n        svm_vmexit_do_vmload(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_VMSAVE:\n        svm_vmexit_do_vmsave(vmcb, regs, v, regs->eax);\n        break;\n    case VMEXIT_STGI:\n        svm_vmexit_do_stgi(regs, v);\n        break;\n    case VMEXIT_CLGI:\n        svm_vmexit_do_clgi(regs, v);\n        break;\n    case VMEXIT_SKINIT:\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case VMEXIT_XSETBV:\n        if ( vmcb_get_cpl(vmcb) )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        else if ( (inst_len = __get_instruction_length(v, INSTR_XSETBV)) &&\n                  hvm_handle_xsetbv(regs->ecx,\n                                    (regs->rdx << 32) | regs->_eax) == 0 )\n            __update_guest_eip(regs, inst_len);\n        break;\n\n    case VMEXIT_NPF:\n        perfc_incra(svmexits, VMEXIT_NPF_PERFC);\n        if ( cpu_has_svm_decode )\n            v->arch.hvm_svm.cached_insn_len = vmcb->guest_ins_len & 0xf;\n        rc = vmcb->exitinfo1 & PFEC_page_present\n             ? p2m_pt_handle_deferred_changes(vmcb->exitinfo2) : 0;\n        if ( rc >= 0 )\n            svm_do_nested_pgfault(v, regs, vmcb->exitinfo1, vmcb->exitinfo2);\n        else\n        {\n            printk(XENLOG_G_ERR\n                   \"%pv: Error %d handling NPF (gpa=%08lx ec=%04lx)\\n\",\n                   v, rc, vmcb->exitinfo2, vmcb->exitinfo1);\n            domain_crash(v->domain);\n        }\n        v->arch.hvm_svm.cached_insn_len = 0;\n        break;\n\n    case VMEXIT_IRET: {\n        u32 general1_intercepts = vmcb_get_general1_intercepts(vmcb);\n\n        /*\n         * IRET clears the NMI mask. However because we clear the mask\n         * /before/ executing IRET, we set the interrupt shadow to prevent\n         * a pending NMI from being injected immediately. This will work\n         * perfectly unless the IRET instruction faults: in that case we\n         * may inject an NMI before the NMI handler's IRET instruction is\n         * retired.\n         */\n        general1_intercepts &= ~GENERAL1_INTERCEPT_IRET;\n        vmcb->interrupt_shadow = 1;\n\n        vmcb_set_general1_intercepts(vmcb, general1_intercepts);\n        break;\n    }\n\n    case VMEXIT_PAUSE:\n        svm_vmexit_do_pause(regs);\n        break;\n\n    default:\n    unexpected_exit_type:\n        gdprintk(XENLOG_ERR, \"unexpected VMEXIT: exit reason = %#\"PRIx64\", \"\n                 \"exitinfo1 = %#\"PRIx64\", exitinfo2 = %#\"PRIx64\"\\n\",\n                 exit_reason, \n                 (u64)vmcb->exitinfo1, (u64)vmcb->exitinfo2);\n        svm_crash_or_fault(v);\n        break;\n    }\n\n  out:\n    if ( vcpu_guestmode || vlapic_hw_disabled(vlapic) )\n        return;\n\n    /* The exit may have updated the TPR: reflect this in the hardware vtpr */\n    intr = vmcb_get_vintr(vmcb);\n    intr.fields.tpr =\n        (vlapic_get_reg(vlapic, APIC_TASKPRI) & 0xFF) >> 4;\n    vmcb_set_vintr(vmcb, intr);\n}\n",
                            "start_line": 2283,
                            "end_line": 2712,
                            "full_name": "svm_vmexit_handler",
                            "file_path": "xen/arch/x86/hvm/svm/svm.c"
                        }
                    ]
                },
                {
                    "tool": "func_info.fetch_func_by_name({'func_name': 'vmx_vmexit_handler', 'project_dir': './storage/result/Part2_result/github/CVE-2016-9382/old_repos/xen-project-xen_04beafa'})",
                    "result": [
                        {
                            "type": "function_def",
                            "code": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        __vmread(GUEST_CR3, &v->arch.hvm_vcpu.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm_vcpu.guest_cr[3] = v->arch.hvm_vcpu.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_enabled(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    (uint32_t)regs->eip, (uint32_t)((uint64_t)regs->eip >> 32),\n                    0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    (uint32_t)regs->eip, \n                    0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm_vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm_vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm_vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm_vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm_vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                /*\n                 * rc < 0 error in monitor/vm_event, crash\n                 * !rc    continue normally\n                 * rc > 0 paused waiting for response, work here is done\n                 */\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->eip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        (unsigned long)regs->eax, (unsigned long)regs->ebx,\n                        (unsigned long)regs->ecx, (unsigned long)regs->edx,\n                        (unsigned long)regs->esi, (unsigned long)regs->edi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_enabled(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n        regs->eip += inst_len;\n        hvm_task_switch((uint16_t)exit_qualification, reasons[source], ecode);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc;\n\n        if ( is_pvh_vcpu(v) )\n        {\n            pv_cpuid(regs);\n            rc = 0;\n        }\n        else\n            rc = vmx_do_cpuid(regs);\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        regs->ecx = hvm_msr_tsc_aux(v);\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n    case EXIT_REASON_VMCALL:\n    {\n        int rc;\n        HVMTRACE_1D(VMMCALL, regs->eax);\n        rc = hvm_do_hypercall(regs);\n        if ( rc != HVM_HCALL_preempted )\n        {\n            update_guest_eip(); /* Safe: VMCALL */\n            if ( rc == HVM_HCALL_invalidate )\n                send_invalidate_req();\n        }\n        break;\n    }\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content;\n        if ( hvm_msr_read_intercept(regs->ecx, &msr_content) == X86EMUL_OKAY )\n        {\n            regs->eax = (uint32_t)msr_content;\n            regs->edx = (uint32_t)(msr_content >> 32);\n            update_guest_eip(); /* Safe: RDMSR */\n        }\n        break;\n    }\n    case EXIT_REASON_MSR_WRITE:\n    {\n        uint64_t msr_content;\n        msr_content = ((uint64_t)regs->edx << 32) | (uint32_t)regs->eax;\n        if ( hvm_msr_write_intercept(regs->ecx, msr_content, 1) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: WRMSR */\n        break;\n    }\n\n    case EXIT_REASON_VMXOFF:\n        if ( nvmx_handle_vmxoff(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMXON:\n        if ( nvmx_handle_vmxon(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMCLEAR:\n        if ( nvmx_handle_vmclear(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMPTRLD:\n        if ( nvmx_handle_vmptrld(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMPTRST:\n        if ( nvmx_handle_vmptrst(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMREAD:\n        if ( nvmx_handle_vmread(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n \n    case EXIT_REASON_VMWRITE:\n        if ( nvmx_handle_vmwrite(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMLAUNCH:\n        if ( nvmx_handle_vmlaunch(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMRESUME:\n        if ( nvmx_handle_vmresume(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVEPT:\n        if ( nvmx_handle_invept(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_invvpid(regs) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, HVM_DELIVER_NO_ERROR_CODE);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( unlikely(is_pvh_vcpu(v)) /* PVH fixme */ ||\n                 !handle_mmio() )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm_vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm_vcpu.single_step )\n        {\n            hvm_monitor_debug(regs->eip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx,\n                               (regs->rdx << 32) | regs->_eax) == 0 )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        {\n            struct segment_register ss;\n\n            gdprintk(XENLOG_WARNING, \"Bad vmexit (reason %#lx)\\n\",\n                     exit_reason);\n\n            vmx_get_segment_register(v, x86_seg_ss, &ss);\n            if ( ss.attr.fields.dpl )\n                hvm_inject_hw_exception(TRAP_invalid_op,\n                                        HVM_DELIVER_NO_ERROR_CODE);\n            else\n                domain_crash(v->domain);\n        }\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->_eip )\n    {\n        struct segment_register ss;\n\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        vmx_get_segment_register(v, x86_seg_ss, &ss);\n        if ( ss.attr.fields.dpl )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->_eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}\n",
                            "start_line": 3290,
                            "end_line": 3957,
                            "full_name": "vmx_vmexit_handler",
                            "file_path": "xen/arch/x86/hvm/vmx/vmx.c"
                        }
                    ]
                }
            ],
            "timestamp": "2025-05-29T19:57:36.407742"
        }
    ],
    "debug_trace": [
        "Collected 2 context items. (0 failed)",
        "Performed analysis iteration 1. Needs more context: True. Confidence: 0.75",
        "Collected 1 context items. (0 failed)",
        "Performed analysis iteration 2. Needs more context: True. Confidence: 0.75",
        "Collected 2 context items. (0 failed)",
        "Performed analysis iteration 3. Needs more context: True. Confidence: 0.75",
        "Collected 2 context items. (0 failed)",
        "Performed analysis iteration 4. Needs more context: True. Confidence: 0.75",
        "Collected 2 context items. (0 failed)",
        "Analysis failed: [Analysis ERROR] Root cause analysis failed"
    ],
    "status": "error",
    "error": "Analysis failed: [Analysis ERROR] Root cause analysis failed"
}