{
  "cve_id": "CVE-2022-40138",
  "cwe_ids": [
    "CWE-681"
  ],
  "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H",
  "cvss_is_v3": true,
  "repo_name": "facebook/hermes",
  "commit_msg": "Re-sync with internal repository (#822)\n\nCo-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
  "commit_hash": "6aa825e480d48127b480b08d13adf70033237097",
  "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
  "file_path": "lib/BCGen/HBC/HBC.cpp",
  "func_name": "hbc::generateBytecodeModule",
  "func_before": "std::unique_ptr<BytecodeModule> hbc::generateBytecodeModule(\n    Module *M,\n    Function *lexicalTopLevel,\n    Function *entryPoint,\n    const BytecodeGenerationOptions &options,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGen,\n    std::unique_ptr<BCProviderBase> baseBCProvider) {\n  PerfSection perf(\"Bytecode Generation\");\n  lowerIR(M, options);\n\n  if (options.format == DumpLIR)\n    M->dump();\n\n  BytecodeModuleGenerator BMGen(options);\n\n  if (segment) {\n    BMGen.setSegmentID(*segment);\n  }\n  // Empty if all functions should be generated (i.e. bundle splitting was not\n  // requested).\n  llvh::DenseSet<Function *> functionsToGenerate = segment\n      ? M->getFunctionsInSegment(*segment)\n      : llvh::DenseSet<Function *>{};\n\n  /// \\return true if we should generate function \\p f.\n  std::function<bool(const Function *)> shouldGenerate;\n  if (segment) {\n    shouldGenerate = [entryPoint, &functionsToGenerate](const Function *f) {\n      return f == entryPoint || functionsToGenerate.count(f) > 0;\n    };\n  } else {\n    shouldGenerate = [](const Function *) { return true; };\n  }\n\n  { // Collect all the strings in the bytecode module into a storage.\n    // If we are in delta optimizing mode, start with the string storage from\n    // our base bytecode provider.\n    auto strings = baseBCProvider\n        ? stringAccumulatorFromBCProvider(*baseBCProvider)\n        : UniquingStringLiteralAccumulator{};\n\n    auto addStringOrIdent = [&strings](llvh::StringRef str, bool isIdentifier) {\n      strings.addString(str, isIdentifier);\n    };\n\n    auto addString = [&strings](llvh::StringRef str) {\n      strings.addString(str, /* isIdentifier */ false);\n    };\n\n    traverseLiteralStrings(M, shouldGenerate, addStringOrIdent);\n\n    if (options.stripFunctionNames) {\n      addString(kStrippedFunctionName);\n    }\n    traverseFunctions(M, shouldGenerate, addString, options.stripFunctionNames);\n\n    if (!M->getCJSModulesResolved()) {\n      traverseCJSModuleNames(M, shouldGenerate, addString);\n    }\n\n    BMGen.initializeStringTable(UniquingStringLiteralAccumulator::toTable(\n        std::move(strings), options.optimizationEnabled));\n  }\n\n  // Add each function to BMGen so that each function has a unique ID.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    unsigned index = BMGen.addFunction(&F);\n    if (&F == entryPoint) {\n      BMGen.setEntryPointIndex(index);\n    }\n\n    auto *cjsModule = M->findCJSModule(&F);\n    if (cjsModule) {\n      if (M->getCJSModulesResolved()) {\n        BMGen.addCJSModuleStatic(cjsModule->id, index);\n      } else {\n        BMGen.addCJSModule(index, BMGen.getStringID(cjsModule->filename.str()));\n      }\n    }\n\n    // Add entries to function source table for non-default source.\n    if (!F.isGlobalScope()) {\n      if (auto source = F.getSourceRepresentationStr()) {\n        BMGen.addFunctionSource(index, BMGen.getStringID(*source));\n      }\n    }\n  }\n  assert(BMGen.getEntryPointIndex() != -1 && \"Entry point not added\");\n\n  // Construct the relative function scope depth map.\n  FunctionScopeAnalysis scopeAnalysis{lexicalTopLevel};\n\n  // Allow reusing the debug cache between functions\n  HBCISelDebugCache debugCache;\n\n  // Bytecode generation for each function.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    std::unique_ptr<BytecodeFunctionGenerator> funcGen;\n\n    if (F.isLazy()) {\n      funcGen = BytecodeFunctionGenerator::create(BMGen, 0);\n    } else {\n      HVMRegisterAllocator RA(&F);\n      if (!options.optimizationEnabled) {\n        RA.setFastPassThreshold(kFastRegisterAllocationThreshold);\n        RA.setMemoryLimit(kRegisterAllocationMemoryLimit);\n      }\n      PostOrderAnalysis PO(&F);\n      /// The order of the blocks is reverse-post-order, which is a simply\n      /// topological sort.\n      llvh::SmallVector<BasicBlock *, 16> order(PO.rbegin(), PO.rend());\n      RA.allocate(order);\n\n      if (options.format == DumpRA) {\n        RA.dump();\n      }\n\n      PassManager PM;\n      PM.addPass(new LowerStoreInstrs(RA));\n      PM.addPass(new LowerCalls(RA));\n      if (options.optimizationEnabled) {\n        PM.addPass(new MovElimination(RA));\n        PM.addPass(new RecreateCheapValues(RA));\n        PM.addPass(new LoadConstantValueNumbering(RA));\n      }\n      PM.addPass(new SpillRegisters(RA));\n      if (options.basicBlockProfiling) {\n        // Insert after all other passes so that it sees final basic block\n        // list.\n        PM.addPass(new InsertProfilePoint());\n      }\n      PM.run(&F);\n\n      if (options.format == DumpLRA)\n        RA.dump();\n\n      if (options.format == DumpPostRA)\n        F.dump();\n\n      funcGen =\n          BytecodeFunctionGenerator::create(BMGen, RA.getMaxRegisterUsage());\n      HBCISel hbciSel(&F, funcGen.get(), RA, scopeAnalysis, options);\n      hbciSel.populateDebugCache(debugCache);\n      hbciSel.generate(sourceMapGen);\n      debugCache = hbciSel.getDebugCache();\n    }\n\n    BMGen.setFunctionGenerator(&F, std::move(funcGen));\n  }\n\n  return BMGen.generate();\n}",
  "abstract_func_before": "std::unique_ptr<BytecodeModule> hbc::generateBytecodeModule(\n    Module *VAR_0,\n    Function *VAR_1,\n    Function *VAR_2,\n    const BytecodeGenerationOptions &VAR_3,\n    hermes::OptValue<uint32_t> VAR_4,\n    SourceMapGenerator *VAR_5,\n    std::unique_ptr<BCProviderBase> VAR_6) {\n  PerfSection VAR_7(\"Bytecode Generation\");\n  lowerIR(VAR_0, VAR_3);\n\n  if (VAR_3.format == VAR_8)\n    VAR_0->dump();\n\n  BytecodeModuleGenerator BMGen(options);\n\n  if (VAR_4) {\n    VAR_9.setSegmentID(*VAR_4);\n  }\n  /* COMMENT_0 */\n  /* COMMENT_1 */\n  llvh::DenseSet<Function *> VAR_10 = VAR_4\n      ? VAR_0->getFunctionsInSegment(*VAR_4)\n      : llvh::DenseSet<Function *>{};\n\n  /* COMMENT_2 */\n  std::function<bool(const Function *)> VAR_11;\n  if (VAR_4) {\n    VAR_11 = [VAR_2, &VAR_10](const Function *VAR_12) {\n      return VAR_12 == VAR_2 || VAR_10.count(VAR_12) > 0;\n    };\n  } else {\n    VAR_11 = [](const Function *) { return true; };\n  }\n\n  { /* COMMENT_3 */\n    /* COMMENT_4 */\n    /* COMMENT_5 */\n    auto VAR_13 = VAR_6\n        ? stringAccumulatorFromBCProvider(*VAR_6)\n        : UniquingStringLiteralAccumulator{};\n\n    auto VAR_14 = [&VAR_13](llvh::StringRef VAR_15, bool VAR_16) {\n      VAR_13.addString(VAR_15, VAR_16);\n    };\n\n    auto VAR_17 = [&VAR_13](llvh::StringRef VAR_15) {\n      VAR_13.addString(VAR_15, /* COMMENT_6 */ false);\n    };\n\n    traverseLiteralStrings(VAR_0, VAR_11, VAR_14);\n\n    if (options.stripFunctionNames) {\n      VAR_17(VAR_18);\n    }\n    traverseFunctions(VAR_0, VAR_11, VAR_17, options.stripFunctionNames);\n\n    if (!VAR_0->getCJSModulesResolved()) {\n      traverseCJSModuleNames(VAR_0, VAR_11, VAR_17);\n    }\n\n    VAR_9.initializeStringTable(UniquingStringLiteralAccumulator::toTable(\n        std::move(VAR_13), options.optimizationEnabled));\n  }\n\n  /* COMMENT_7 */\n  for (auto &VAR_19 : *VAR_0) {\n    if (!VAR_11(&VAR_19)) {\n      continue;\n    }\n\n    unsigned VAR_20 = VAR_9.addFunction(&VAR_19);\n    if (&VAR_19 == VAR_2) {\n      VAR_9.setEntryPointIndex(VAR_20);\n    }\n\n    auto *VAR_21 = VAR_0->findCJSModule(&VAR_19);\n    if (VAR_21) {\n      if (VAR_0->getCJSModulesResolved()) {\n        VAR_9.addCJSModuleStatic(VAR_21->id, VAR_20);\n      } else {\n        VAR_9.addCJSModule(VAR_20, VAR_9.getStringID(VAR_21->filename.str()));\n      }\n    }\n\n    /* COMMENT_8 */\n    if (!VAR_19.isGlobalScope()) {\n      if (auto VAR_22 = VAR_19.getSourceRepresentationStr()) {\n        VAR_9.addFunctionSource(VAR_20, VAR_9.getStringID(*VAR_22));\n      }\n    }\n  }\n  assert(VAR_9.getEntryPointIndex() != -1 && \"Entry point not added\");\n\n  /* COMMENT_9 */\n  FunctionScopeAnalysis VAR_23{VAR_1};\n\n  /* COMMENT_10 */\n  HBCISelDebugCache VAR_24;\n\n  /* COMMENT_11 */\n  for (auto &VAR_19 : *VAR_0) {\n    if (!VAR_11(&VAR_19)) {\n      continue;\n    }\n\n    std::unique_ptr<BytecodeFunctionGenerator> VAR_25;\n\n    if (VAR_19.isLazy()) {\n      VAR_25 = BytecodeFunctionGenerator::create(VAR_9, 0);\n    } else {\n      HVMRegisterAllocator VAR_26(&VAR_19);\n      if (!options.optimizationEnabled) {\n        VAR_26.setFastPassThreshold(VAR_27);\n        VAR_26.setMemoryLimit(VAR_28);\n      }\n      PostOrderAnalysis VAR_29(&VAR_19);\n      /* COMMENT_12 */\n      /* COMMENT_13 */\n      llvh::SmallVector<BasicBlock *, 16> VAR_30(VAR_29.rbegin(), VAR_29.rend());\n      VAR_26.allocate(VAR_30);\n\n      if (options.format == VAR_31) {\n        VAR_26.dump();\n      }\n\n      PassManager VAR_32;\n      VAR_32.addPass(new LowerStoreInstrs(VAR_26));\n      VAR_32.addPass(new LowerCalls(VAR_26));\n      if (options.optimizationEnabled) {\n        VAR_32.addPass(new MovElimination(VAR_26));\n        VAR_32.addPass(new RecreateCheapValues(VAR_26));\n        VAR_32.addPass(new LoadConstantValueNumbering(VAR_26));\n      }\n      VAR_32.addPass(new SpillRegisters(VAR_26));\n      if (options.basicBlockProfiling) {\n        /* COMMENT_14 */\n        /* COMMENT_15 */\n        VAR_32.addPass(new InsertProfilePoint());\n      }\n      VAR_32.run(&VAR_19);\n\n      if (options.format == VAR_33)\n        VAR_26.dump();\n\n      if (options.format == VAR_34)\n        VAR_19.dump();\n\n      VAR_25 =\n          BytecodeFunctionGenerator::create(VAR_9, VAR_26.getMaxRegisterUsage());\n      HBCISel VAR_35(&VAR_19, VAR_25.get(), VAR_26, VAR_23, options);\n      VAR_35.populateDebugCache(VAR_24);\n      VAR_35.generate(VAR_5);\n      VAR_24 = VAR_35.getDebugCache();\n    }\n\n    VAR_9.setFunctionGenerator(&VAR_19, std::move(VAR_25));\n  }\n\n  return VAR_9.generate();\n}",
  "func_graph_path_before": "facebook/hermes/6aa825e480d48127b480b08d13adf70033237097/HBC.cpp/vul/before/1.json",
  "func": "std::unique_ptr<BytecodeModule> hbc::generateBytecodeModule(\n    Module *M,\n    Function *lexicalTopLevel,\n    Function *entryPoint,\n    const BytecodeGenerationOptions &options,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGen,\n    std::unique_ptr<BCProviderBase> baseBCProvider) {\n  PerfSection perf(\"Bytecode Generation\");\n  lowerIR(M, options);\n\n  if (options.format == DumpLIR)\n    M->dump();\n\n  BytecodeModuleGenerator BMGen(options);\n\n  if (segment) {\n    BMGen.setSegmentID(*segment);\n  }\n  // Empty if all functions should be generated (i.e. bundle splitting was not\n  // requested).\n  llvh::DenseSet<Function *> functionsToGenerate = segment\n      ? M->getFunctionsInSegment(*segment)\n      : llvh::DenseSet<Function *>{};\n\n  /// \\return true if we should generate function \\p f.\n  std::function<bool(const Function *)> shouldGenerate;\n  if (segment) {\n    shouldGenerate = [entryPoint, &functionsToGenerate](const Function *f) {\n      return f == entryPoint || functionsToGenerate.count(f) > 0;\n    };\n  } else {\n    shouldGenerate = [](const Function *) { return true; };\n  }\n\n  { // Collect all the strings in the bytecode module into a storage.\n    // If we are in delta optimizing mode, start with the string storage from\n    // our base bytecode provider.\n    auto strings = baseBCProvider\n        ? stringAccumulatorFromBCProvider(*baseBCProvider)\n        : UniquingStringLiteralAccumulator{};\n\n    auto addStringOrIdent = [&strings](llvh::StringRef str, bool isIdentifier) {\n      strings.addString(str, isIdentifier);\n    };\n\n    auto addString = [&strings](llvh::StringRef str) {\n      strings.addString(str, /* isIdentifier */ false);\n    };\n\n    traverseLiteralStrings(M, shouldGenerate, addStringOrIdent);\n\n    if (options.stripFunctionNames) {\n      addString(kStrippedFunctionName);\n    }\n    traverseFunctions(M, shouldGenerate, addString, options.stripFunctionNames);\n\n    if (!M->getCJSModulesResolved()) {\n      traverseCJSModuleNames(M, shouldGenerate, addString);\n    }\n\n    BMGen.initializeStringTable(UniquingStringLiteralAccumulator::toTable(\n        std::move(strings), options.optimizationEnabled));\n  }\n\n  // Add each function to BMGen so that each function has a unique ID.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    unsigned index = BMGen.addFunction(&F);\n    if (&F == entryPoint) {\n      BMGen.setEntryPointIndex(index);\n    }\n\n    auto *cjsModule = M->findCJSModule(&F);\n    if (cjsModule) {\n      if (M->getCJSModulesResolved()) {\n        BMGen.addCJSModuleStatic(cjsModule->id, index);\n      } else {\n        BMGen.addCJSModule(index, BMGen.getStringID(cjsModule->filename.str()));\n      }\n    }\n\n    // Add entries to function source table for non-default source.\n    if (!F.isGlobalScope()) {\n      if (auto source = F.getSourceRepresentationStr()) {\n        BMGen.addFunctionSource(index, BMGen.getStringID(*source));\n      }\n    }\n  }\n  assert(BMGen.getEntryPointIndex() != -1 && \"Entry point not added\");\n\n  // Construct the relative function scope depth map.\n  FunctionScopeAnalysis scopeAnalysis{lexicalTopLevel};\n\n  // Allow reusing the debug cache between functions\n  HBCISelDebugCache debugCache;\n\n  // Bytecode generation for each function.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    std::unique_ptr<BytecodeFunctionGenerator> funcGen;\n\n    if (F.isLazy()) {\n      funcGen = BytecodeFunctionGenerator::create(BMGen, 0);\n    } else {\n      HVMRegisterAllocator RA(&F);\n      if (!options.optimizationEnabled) {\n        RA.setFastPassThreshold(kFastRegisterAllocationThreshold);\n        RA.setMemoryLimit(kRegisterAllocationMemoryLimit);\n      }\n      PostOrderAnalysis PO(&F);\n      /// The order of the blocks is reverse-post-order, which is a simply\n      /// topological sort.\n      llvh::SmallVector<BasicBlock *, 16> order(PO.rbegin(), PO.rend());\n      RA.allocate(order);\n\n      if (options.format == DumpRA) {\n        RA.dump();\n      }\n\n      PassManager PM;\n      PM.addPass(new LowerStoreInstrs(RA));\n      PM.addPass(new LowerCalls(RA));\n      if (options.optimizationEnabled) {\n        PM.addPass(new MovElimination(RA));\n        PM.addPass(new RecreateCheapValues(RA));\n        PM.addPass(new LoadConstantValueNumbering(RA));\n      }\n      PM.addPass(new SpillRegisters(RA));\n      if (options.basicBlockProfiling) {\n        // Insert after all other passes so that it sees final basic block\n        // list.\n        PM.addPass(new InsertProfilePoint());\n      }\n      PM.run(&F);\n\n      if (options.format == DumpLRA)\n        RA.dump();\n\n      if (options.format == DumpPostRA)\n        F.dump();\n\n      funcGen =\n          BytecodeFunctionGenerator::create(BMGen, RA.getMaxRegisterUsage());\n      HBCISel hbciSel(&F, funcGen.get(), RA, scopeAnalysis, options);\n      hbciSel.populateDebugCache(debugCache);\n      hbciSel.generate(sourceMapGen);\n      debugCache = hbciSel.getDebugCache();\n    }\n\n    if (funcGen->hasEncodingError()) {\n      M->getContext().getSourceErrorManager().error(\n          F.getSourceRange().Start, \"Error encoding bytecode\");\n      return nullptr;\n    }\n    BMGen.setFunctionGenerator(&F, std::move(funcGen));\n  }\n\n  return BMGen.generate();\n}",
  "abstract_func": "std::unique_ptr<BytecodeModule> hbc::generateBytecodeModule(\n    Module *VAR_0,\n    Function *VAR_1,\n    Function *VAR_2,\n    const BytecodeGenerationOptions &VAR_3,\n    hermes::OptValue<uint32_t> VAR_4,\n    SourceMapGenerator *VAR_5,\n    std::unique_ptr<BCProviderBase> VAR_6) {\n  PerfSection VAR_7(\"Bytecode Generation\");\n  lowerIR(VAR_0, VAR_3);\n\n  if (VAR_3.format == VAR_8)\n    VAR_0->dump();\n\n  BytecodeModuleGenerator BMGen(options);\n\n  if (VAR_4) {\n    VAR_9.setSegmentID(*VAR_4);\n  }\n  /* COMMENT_0 */\n  /* COMMENT_1 */\n  llvh::DenseSet<Function *> VAR_10 = VAR_4\n      ? VAR_0->getFunctionsInSegment(*VAR_4)\n      : llvh::DenseSet<Function *>{};\n\n  /* COMMENT_2 */\n  std::function<bool(const Function *)> VAR_11;\n  if (VAR_4) {\n    VAR_11 = [VAR_2, &VAR_10](const Function *VAR_12) {\n      return VAR_12 == VAR_2 || VAR_10.count(VAR_12) > 0;\n    };\n  } else {\n    VAR_11 = [](const Function *) { return true; };\n  }\n\n  { /* COMMENT_3 */\n    /* COMMENT_4 */\n    /* COMMENT_5 */\n    auto VAR_13 = VAR_6\n        ? stringAccumulatorFromBCProvider(*VAR_6)\n        : UniquingStringLiteralAccumulator{};\n\n    auto VAR_14 = [&VAR_13](llvh::StringRef VAR_15, bool VAR_16) {\n      VAR_13.addString(VAR_15, VAR_16);\n    };\n\n    auto VAR_17 = [&VAR_13](llvh::StringRef VAR_15) {\n      VAR_13.addString(VAR_15, /* COMMENT_6 */ false);\n    };\n\n    traverseLiteralStrings(VAR_0, VAR_11, VAR_14);\n\n    if (options.stripFunctionNames) {\n      VAR_17(VAR_18);\n    }\n    traverseFunctions(VAR_0, VAR_11, VAR_17, options.stripFunctionNames);\n\n    if (!VAR_0->getCJSModulesResolved()) {\n      traverseCJSModuleNames(VAR_0, VAR_11, VAR_17);\n    }\n\n    VAR_9.initializeStringTable(UniquingStringLiteralAccumulator::toTable(\n        std::move(VAR_13), options.optimizationEnabled));\n  }\n\n  /* COMMENT_7 */\n  for (auto &VAR_19 : *VAR_0) {\n    if (!VAR_11(&VAR_19)) {\n      continue;\n    }\n\n    unsigned VAR_20 = VAR_9.addFunction(&VAR_19);\n    if (&VAR_19 == VAR_2) {\n      VAR_9.setEntryPointIndex(VAR_20);\n    }\n\n    auto *VAR_21 = VAR_0->findCJSModule(&VAR_19);\n    if (VAR_21) {\n      if (VAR_0->getCJSModulesResolved()) {\n        VAR_9.addCJSModuleStatic(VAR_21->id, VAR_20);\n      } else {\n        VAR_9.addCJSModule(VAR_20, VAR_9.getStringID(VAR_21->filename.str()));\n      }\n    }\n\n    /* COMMENT_8 */\n    if (!VAR_19.isGlobalScope()) {\n      if (auto VAR_22 = VAR_19.getSourceRepresentationStr()) {\n        VAR_9.addFunctionSource(VAR_20, VAR_9.getStringID(*VAR_22));\n      }\n    }\n  }\n  assert(VAR_9.getEntryPointIndex() != -1 && \"Entry point not added\");\n\n  /* COMMENT_9 */\n  FunctionScopeAnalysis VAR_23{VAR_1};\n\n  /* COMMENT_10 */\n  HBCISelDebugCache VAR_24;\n\n  /* COMMENT_11 */\n  for (auto &VAR_19 : *VAR_0) {\n    if (!VAR_11(&VAR_19)) {\n      continue;\n    }\n\n    std::unique_ptr<BytecodeFunctionGenerator> VAR_25;\n\n    if (VAR_19.isLazy()) {\n      VAR_25 = BytecodeFunctionGenerator::create(VAR_9, 0);\n    } else {\n      HVMRegisterAllocator VAR_26(&VAR_19);\n      if (!options.optimizationEnabled) {\n        VAR_26.setFastPassThreshold(VAR_27);\n        VAR_26.setMemoryLimit(VAR_28);\n      }\n      PostOrderAnalysis VAR_29(&VAR_19);\n      /* COMMENT_12 */\n      /* COMMENT_13 */\n      llvh::SmallVector<BasicBlock *, 16> VAR_30(VAR_29.rbegin(), VAR_29.rend());\n      VAR_26.allocate(VAR_30);\n\n      if (options.format == VAR_31) {\n        VAR_26.dump();\n      }\n\n      PassManager VAR_32;\n      VAR_32.addPass(new LowerStoreInstrs(VAR_26));\n      VAR_32.addPass(new LowerCalls(VAR_26));\n      if (options.optimizationEnabled) {\n        VAR_32.addPass(new MovElimination(VAR_26));\n        VAR_32.addPass(new RecreateCheapValues(VAR_26));\n        VAR_32.addPass(new LoadConstantValueNumbering(VAR_26));\n      }\n      VAR_32.addPass(new SpillRegisters(VAR_26));\n      if (options.basicBlockProfiling) {\n        /* COMMENT_14 */\n        /* COMMENT_15 */\n        VAR_32.addPass(new InsertProfilePoint());\n      }\n      VAR_32.run(&VAR_19);\n\n      if (options.format == VAR_33)\n        VAR_26.dump();\n\n      if (options.format == VAR_34)\n        VAR_19.dump();\n\n      VAR_25 =\n          BytecodeFunctionGenerator::create(VAR_9, VAR_26.getMaxRegisterUsage());\n      HBCISel VAR_35(&VAR_19, VAR_25.get(), VAR_26, VAR_23, options);\n      VAR_35.populateDebugCache(VAR_24);\n      VAR_35.generate(VAR_5);\n      VAR_24 = VAR_35.getDebugCache();\n    }\n\n    if (VAR_25->hasEncodingError()) {\n      VAR_0->getContext().getSourceErrorManager().error(\n          VAR_19.getSourceRange().Start, \"Error encoding bytecode\");\n      return nullptr;\n    }\n    VAR_9.setFunctionGenerator(&VAR_19, std::move(VAR_25));\n  }\n\n  return VAR_9.generate();\n}",
  "func_graph_path": "facebook/hermes/6aa825e480d48127b480b08d13adf70033237097/HBC.cpp/vul/after/1.json",
  "diff_func": "--- func_before\n+++ func_after\n@@ -154,6 +154,11 @@\n       debugCache = hbciSel.getDebugCache();\n     }\n \n+    if (funcGen->hasEncodingError()) {\n+      M->getContext().getSourceErrorManager().error(\n+          F.getSourceRange().Start, \"Error encoding bytecode\");\n+      return nullptr;\n+    }\n     BMGen.setFunctionGenerator(&F, std::move(funcGen));\n   }\n ",
  "diff_line_info": {
    "deleted_lines": [],
    "added_lines": [
      "    if (funcGen->hasEncodingError()) {",
      "      M->getContext().getSourceErrorManager().error(",
      "          F.getSourceRange().Start, \"Error encoding bytecode\");",
      "      return nullptr;",
      "    }"
    ]
  },
  "is_vul": true,
  "pr_url": "https://github.com/facebook/hermes/pull/822",
  "description": {
    "pr_info": {
      "title": "Re-sync with internal repository",
      "number": 822
    },
    "comment": [
      "The internal and external repositories are out of sync. This attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging."
    ]
  },
  "Bug Filter": "Defect Remediation & Feature Upgrades",
  "Bug Filter Confidence": 0.8,
  "Bug Filter Response": "**Final Classification:** Defect Remediation & Feature Upgrades; **Confidence:** 0.8"
}