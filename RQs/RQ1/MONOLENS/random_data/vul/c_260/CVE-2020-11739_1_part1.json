{
    "cve_id": "CVE-2020-11739",
    "cwe_ids": [
        "CWE-362"
    ],
    "cvss_vector": "AV:L/AC:M/Au:N/C:C/I:C/A:C",
    "cvss_is_v3": false,
    "repo_name": "xen-project/xen",
    "commit_msg": "xen/rwlock: Add missing memory barrier in the unlock path of rwlock\n\nThe rwlock unlock paths are using atomic_sub() to release the lock.\nHowever the implementation of atomic_sub() rightfully doesn't contain a\nmemory barrier. On Arm, this means a processor is allowed to re-order\nthe memory access with the preceeding access.\n\nIn other words, the unlock may be seen by another processor before all\nthe memory accesses within the \"critical\" section.\n\nThe rwlock paths already contains barrier indirectly, but they are not\nvery useful without the counterpart in the unlock paths.\n\nThe memory barriers are not necessary on x86 because loads/stores are\nnot re-ordered with lock instructions.\n\nSo add arch_lock_release_barrier() in the unlock paths that will only\nadd memory barrier on Arm.\n\nTake the opportunity to document each lock paths explaining why a\nbarrier is not necessary.\n\nThis is XSA-314.\n\nSigned-off-by: Julien Grall <jgrall@amazon.com>\nAcked-by: Jan Beulich <jbeulich@suse.com>",
    "commit_hash": "6890a04072e664c25447a297fe663b45ecfd6398",
    "git_url": "https://github.com/xen-project/xen/commit/6890a04072e664c25447a297fe663b45ecfd6398",
    "file_path": "xen/include/xen/rwlock.h",
    "func_name": "_write_lock",
    "func_before": "static inline void _write_lock(rwlock_t *lock)\n{\n    /* Optimize for the unfair lock case where the fair flag is 0. */\n    preempt_disable();\n    if ( atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) == 0 )\n        return;\n\n    queue_write_lock_slowpath(lock);\n}",
    "abstract_func_before": "static inline void _write_lock(rwlock_t *VAR_0)\n{\n    /* COMMENT_0 */\n    preempt_disable();\n    if ( atomic_cmpxchg(&VAR_0->cnts, 0, _write_lock_val()) == 0 )\n        return;\n\n    queue_write_lock_slowpath(VAR_0);\n}",
    "func_graph_path_before": "xen-project/xen/6890a04072e664c25447a297fe663b45ecfd6398/rwlock.h/vul/before/2.json",
    "func": "static inline void _write_lock(rwlock_t *lock)\n{\n    preempt_disable();\n    /*\n     * Optimize for the unfair lock case where the fair flag is 0.\n     *\n     * atomic_cmpxchg() is a full barrier so no need for an\n     * arch_lock_acquire_barrier().\n     */\n    if ( atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) == 0 )\n        return;\n\n    queue_write_lock_slowpath(lock);\n    /*\n     * queue_write_lock_slowpath() is using spinlock and therefore is a\n     * full barrier. So no need for an arch_lock_acquire_barrier().\n     */\n}",
    "abstract_func": "static inline void _write_lock(rwlock_t *VAR_0)\n{\n    preempt_disable();\n    /* COMMENT_0 */\n                                                                  \n      \n                                                           \n                                   \n       \n    if ( atomic_cmpxchg(&VAR_0->cnts, 0, _write_lock_val()) == 0 )\n        return;\n\n    queue_write_lock_slowpath(VAR_0);\n    /* COMMENT_6 */\n                                                                       \n                                                                   \n       \n}",
    "func_graph_path": "xen-project/xen/6890a04072e664c25447a297fe663b45ecfd6398/rwlock.h/vul/after/2.json",
    "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,18 @@\n static inline void _write_lock(rwlock_t *lock)\n {\n-    /* Optimize for the unfair lock case where the fair flag is 0. */\n     preempt_disable();\n+    /*\n+     * Optimize for the unfair lock case where the fair flag is 0.\n+     *\n+     * atomic_cmpxchg() is a full barrier so no need for an\n+     * arch_lock_acquire_barrier().\n+     */\n     if ( atomic_cmpxchg(&lock->cnts, 0, _write_lock_val()) == 0 )\n         return;\n \n     queue_write_lock_slowpath(lock);\n+    /*\n+     * queue_write_lock_slowpath() is using spinlock and therefore is a\n+     * full barrier. So no need for an arch_lock_acquire_barrier().\n+     */\n }",
    "diff_line_info": {
        "deleted_lines": [
            "    /* Optimize for the unfair lock case where the fair flag is 0. */"
        ],
        "added_lines": [
            "    /*",
            "     * Optimize for the unfair lock case where the fair flag is 0.",
            "     *",
            "     * atomic_cmpxchg() is a full barrier so no need for an",
            "     * arch_lock_acquire_barrier().",
            "     */",
            "    /*",
            "     * queue_write_lock_slowpath() is using spinlock and therefore is a",
            "     * full barrier. So no need for an arch_lock_acquire_barrier().",
            "     */"
        ]
    },
    "is_vul": true,
    "pr_url": null,
    "description": "no more info"
}