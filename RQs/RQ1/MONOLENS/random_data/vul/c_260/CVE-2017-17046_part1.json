{
    "cve_id": "CVE-2017-17046",
    "cwe_ids": [
        "CWE-200"
    ],
    "cvss_vector": "AV:L/AC:L/Au:N/C:P/I:N/A:N",
    "cvss_is_v3": false,
    "repo_name": "xen-project/xen",
    "commit_msg": "xen/page_alloc: Cover memory unreserved after boot in first_valid_mfn\n\nOn Arm, some regions (e.g Initramfs, Dom0 Kernel...) are marked as\nreserved until the hardware domain is built and they are copied into its\nmemory. Therefore, they will not be added in the boot allocator via\ninit_boot_pages.\n\nInstead, init_xenheap_pages will be called once the region are not used\nanymore.\n\nUpdate first_valid_mfn in both init_heap_pages and init_boot_pages\n(already exist) to cover all the cases.\n\nThis is XSA-245.\n\nSigned-off-by: Julien Grall <julien.grall@arm.com>\n[Adjust comment, added locking around first_valid_mfn update]\nSigned-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>\nAcked-by: Stefano Stabellini <sstabellini@kernel.org>\nReported-and-Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>",
    "commit_hash": "ec6d9023e1f54cdccbf2e4c63cf947f1be2b1e8e",
    "git_url": "https://github.com/xen-project/xen/commit/ec6d9023e1f54cdccbf2e4c63cf947f1be2b1e8e",
    "file_path": "xen/common/page_alloc.c",
    "func_name": "init_heap_pages",
    "func_before": "static void init_heap_pages(\n    struct page_info *pg, unsigned long nr_pages)\n{\n    unsigned long i;\n\n    for ( i = 0; i < nr_pages; i++ )\n    {\n        unsigned int nid = phys_to_nid(page_to_maddr(pg+i));\n\n        if ( unlikely(!avail[nid]) )\n        {\n            unsigned long s = page_to_mfn(pg + i);\n            unsigned long e = page_to_mfn(pg + nr_pages - 1) + 1;\n            bool_t use_tail = (nid == phys_to_nid(pfn_to_paddr(e - 1))) &&\n                              !(s & ((1UL << MAX_ORDER) - 1)) &&\n                              (find_first_set_bit(e) <= find_first_set_bit(s));\n            unsigned long n;\n\n            n = init_node_heap(nid, page_to_mfn(pg+i), nr_pages - i,\n                               &use_tail);\n            BUG_ON(i + n > nr_pages);\n            if ( n && !use_tail )\n            {\n                i += n - 1;\n                continue;\n            }\n            if ( i + n == nr_pages )\n                break;\n            nr_pages -= n;\n        }\n\n        free_heap_pages(pg + i, 0, scrub_debug);\n    }\n}",
    "abstract_func_before": "static void init_heap_pages(\n    struct page_info *VAR_0, unsigned long VAR_1)\n{\n    unsigned long VAR_2;\n\n    for ( VAR_2 = 0; VAR_2 < VAR_1; VAR_2++ )\n    {\n        unsigned int VAR_3 = phys_to_nid(page_to_maddr(VAR_0+VAR_2));\n\n        if ( unlikely(!VAR_4[VAR_3]) )\n        {\n            unsigned long VAR_5 = page_to_mfn(VAR_0 + VAR_2);\n            unsigned long VAR_6 = page_to_mfn(VAR_0 + VAR_1 - 1) + 1;\n            bool_t VAR_7 = (VAR_3 == phys_to_nid(pfn_to_paddr(VAR_6 - 1))) &&\n                              !(VAR_5 & ((1UL << VAR_8) - 1)) &&\n                              (find_first_set_bit(VAR_6) <= find_first_set_bit(VAR_5));\n            unsigned long VAR_9;\n\n            VAR_9 = init_node_heap(VAR_3, page_to_mfn(VAR_0+VAR_2), VAR_1 - VAR_2,\n                               &VAR_7);\n            BUG_ON(VAR_2 + VAR_9 > VAR_1);\n            if ( VAR_9 && !VAR_7 )\n            {\n                VAR_2 += VAR_9 - 1;\n                continue;\n            }\n            if ( VAR_2 + VAR_9 == VAR_1 )\n                break;\n            VAR_1 -= VAR_9;\n        }\n\n        free_heap_pages(VAR_0 + VAR_2, 0, VAR_10);\n    }\n}",
    "func_graph_path_before": "xen-project/xen/ec6d9023e1f54cdccbf2e4c63cf947f1be2b1e8e/page_alloc.c/vul/before/0.json",
    "func": "static void init_heap_pages(\n    struct page_info *pg, unsigned long nr_pages)\n{\n    unsigned long i;\n\n    /*\n     * Some pages may not go through the boot allocator (e.g reserved\n     * memory at boot but released just after --- kernel, initramfs,\n     * etc.).\n     * Update first_valid_mfn to ensure those regions are covered.\n     */\n    spin_lock(&heap_lock);\n    first_valid_mfn = min_t(unsigned long, page_to_mfn(pg), first_valid_mfn);\n    spin_unlock(&heap_lock);\n\n    for ( i = 0; i < nr_pages; i++ )\n    {\n        unsigned int nid = phys_to_nid(page_to_maddr(pg+i));\n\n        if ( unlikely(!avail[nid]) )\n        {\n            unsigned long s = page_to_mfn(pg + i);\n            unsigned long e = page_to_mfn(pg + nr_pages - 1) + 1;\n            bool_t use_tail = (nid == phys_to_nid(pfn_to_paddr(e - 1))) &&\n                              !(s & ((1UL << MAX_ORDER) - 1)) &&\n                              (find_first_set_bit(e) <= find_first_set_bit(s));\n            unsigned long n;\n\n            n = init_node_heap(nid, page_to_mfn(pg+i), nr_pages - i,\n                               &use_tail);\n            BUG_ON(i + n > nr_pages);\n            if ( n && !use_tail )\n            {\n                i += n - 1;\n                continue;\n            }\n            if ( i + n == nr_pages )\n                break;\n            nr_pages -= n;\n        }\n\n        free_heap_pages(pg + i, 0, scrub_debug);\n    }\n}",
    "abstract_func": "static void init_heap_pages(\n    struct page_info *VAR_0, unsigned long VAR_1)\n{\n    unsigned long VAR_2;\n\n    /* COMMENT_0 */\n                                                                     \n                                                                    \n             \n                                                                  \n       \n    spin_lock(&VAR_3);\n    VAR_4 = min_t(unsigned VAR_5, page_to_mfn(VAR_0), VAR_4);\n    spin_unlock(&VAR_3);\n\n    for ( VAR_2 = 0; VAR_2 < VAR_1; VAR_2++ )\n    {\n        unsigned int VAR_6 = phys_to_nid(page_to_maddr(VAR_0+VAR_2));\n\n        if ( unlikely(!VAR_7[VAR_6]) )\n        {\n            unsigned long VAR_8 = page_to_mfn(VAR_0 + VAR_2);\n            unsigned long VAR_9 = page_to_mfn(VAR_0 + VAR_1 - 1) + 1;\n            bool_t VAR_10 = (VAR_6 == phys_to_nid(pfn_to_paddr(VAR_9 - 1))) &&\n                              !(VAR_8 & ((1UL << VAR_11) - 1)) &&\n                              (find_first_set_bit(VAR_9) <= find_first_set_bit(VAR_8));\n            unsigned long VAR_12;\n\n            VAR_12 = init_node_heap(VAR_6, page_to_mfn(VAR_0+VAR_2), VAR_1 - VAR_2,\n                               &VAR_10);\n            BUG_ON(VAR_2 + VAR_12 > VAR_1);\n            if ( VAR_12 && !VAR_10 )\n            {\n                VAR_2 += VAR_12 - 1;\n                continue;\n            }\n            if ( VAR_2 + VAR_12 == VAR_1 )\n                break;\n            VAR_1 -= VAR_12;\n        }\n\n        free_heap_pages(VAR_0 + VAR_2, 0, VAR_13);\n    }\n}",
    "func_graph_path": "xen-project/xen/ec6d9023e1f54cdccbf2e4c63cf947f1be2b1e8e/page_alloc.c/vul/after/0.json",
    "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,16 @@\n     struct page_info *pg, unsigned long nr_pages)\n {\n     unsigned long i;\n+\n+    /*\n+     * Some pages may not go through the boot allocator (e.g reserved\n+     * memory at boot but released just after --- kernel, initramfs,\n+     * etc.).\n+     * Update first_valid_mfn to ensure those regions are covered.\n+     */\n+    spin_lock(&heap_lock);\n+    first_valid_mfn = min_t(unsigned long, page_to_mfn(pg), first_valid_mfn);\n+    spin_unlock(&heap_lock);\n \n     for ( i = 0; i < nr_pages; i++ )\n     {",
    "diff_line_info": {
        "deleted_lines": [],
        "added_lines": [
            "",
            "    /*",
            "     * Some pages may not go through the boot allocator (e.g reserved",
            "     * memory at boot but released just after --- kernel, initramfs,",
            "     * etc.).",
            "     * Update first_valid_mfn to ensure those regions are covered.",
            "     */",
            "    spin_lock(&heap_lock);",
            "    first_valid_mfn = min_t(unsigned long, page_to_mfn(pg), first_valid_mfn);",
            "    spin_unlock(&heap_lock);"
        ]
    },
    "is_vul": true,
    "pr_url": null,
    "description": "no more info"
}