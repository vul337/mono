{
    "cve_id": "CVE-2019-19583",
    "cwe_ids": [
        "CWE-Other"
    ],
    "cvss_vector": "AV:N/AC:L/Au:N/C:N/I:N/A:P",
    "cvss_is_v3": false,
    "repo_name": "xen-project/xen",
    "commit_msg": "x86/vtx: Work around SingleStep + STI/MovSS VMEntry failures\n\nSee patch comment for technical details.\n\nConcerning the timeline, this was first discovered in the aftermath of\nXSA-156 which caused #DB to be intercepted unconditionally, but only in\nits SingleStep + STI form which is restricted to privileged software.\n\nAfter working with Intel and identifying the problematic vmentry check,\nthis workaround was suggested, and the patch was posted in an RFC\nseries.  Outstanding work for that series (not breaking Introspection)\nis still pending, and this fix from it (which wouldn't have been good\nenough in its original form) wasn't committed.\n\nA vmentry failure was reported to xen-devel, and debugging identified\nthis bug in its SingleStep + MovSS form by way of INT1, which does not\ninvolve the use of any privileged instructions, and proving this to be a\nsecurity issue.\n\nThis is XSA-308\n\nReported-by: H\u00e5kon Alstadheim <hakon@alstadheim.priv.no>\nSigned-off-by: Andrew Cooper <andrew.cooper3@citrix.com>\nReviewed-by: Jan Beulich <jbeulich@suse.com>\nAcked-by: Kevin Tian <kevin.tian@intel.com>",
    "commit_hash": "1d3eb8259804e5bec991a3462d69ba6bd80bb40e",
    "git_url": "https://github.com/xen-project/xen/commit/1d3eb8259804e5bec991a3462d69ba6bd80bb40e",
    "file_path": "xen/arch/x86/hvm/vmx/vmx.c",
    "func_name": "vmx_vmexit_handler",
    "func_before": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n    struct domain *currd = v->domain;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        /*\n         * Xen allows the guest to modify some CR4 bits directly, update cached\n         * values to match.\n         */\n        __vmread(GUEST_CR4, &v->arch.hvm.hw_cr[4]);\n        v->arch.hvm.guest_cr[4] &= v->arch.hvm.vmx.cr4_host_mask;\n        v->arch.hvm.guest_cr[4] |= (v->arch.hvm.hw_cr[4] &\n                                    ~v->arch.hvm.vmx.cr4_host_mask);\n\n        __vmread(GUEST_CR3, &v->arch.hvm.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm.guest_cr[3] = v->arch.hvm.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_active(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    regs->eip, regs->rip >> 32, 0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    regs->eip, 0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm.vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm.vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n\n                return;\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm.vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm.vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm.vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            __restore_debug_registers(v);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->rip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->rip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        regs->rax, regs->rbx, regs->rcx,\n                        regs->rdx, regs->rsi, regs->rdi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_active(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm.vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm.vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n\n        hvm_task_switch(exit_qualification, reasons[source], ecode, inst_len,\n                        0 /* EFLAGS.RF already updated. */);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc = hvm_vmexit_cpuid(regs, get_instruction_length());\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        if ( !currd->arch.cpuid->extd.rdtscp )\n        {\n            hvm_inject_hw_exception(TRAP_invalid_op, X86_EVENT_NO_EC);\n            break;\n        }\n\n        regs->rcx = v->arch.msrs->tsc_aux;\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n\n    case EXIT_REASON_VMCALL:\n        HVMTRACE_1D(VMMCALL, regs->eax);\n\n        if ( hvm_hypercall(regs) == HVM_HCALL_completed )\n            update_guest_eip(); /* Safe: VMCALL */\n        break;\n\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content = 0;\n\n        switch ( hvm_msr_read_intercept(regs->ecx, &msr_content) )\n        {\n        case X86EMUL_OKAY:\n            msr_split(regs, msr_content);\n            update_guest_eip(); /* Safe: RDMSR */\n            break;\n\n        case X86EMUL_EXCEPTION:\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            break;\n        }\n        break;\n    }\n\n    case EXIT_REASON_MSR_WRITE:\n        switch ( hvm_msr_write_intercept(regs->ecx, msr_fold(regs), true) )\n        {\n        case X86EMUL_OKAY:\n            update_guest_eip(); /* Safe: WRMSR */\n            break;\n\n        case X86EMUL_EXCEPTION:\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            break;\n        }\n        break;\n\n    case EXIT_REASON_VMXOFF:\n    case EXIT_REASON_VMXON:\n    case EXIT_REASON_VMCLEAR:\n    case EXIT_REASON_VMPTRLD:\n    case EXIT_REASON_VMPTRST:\n    case EXIT_REASON_VMREAD:\n    case EXIT_REASON_VMWRITE:\n    case EXIT_REASON_VMLAUNCH:\n    case EXIT_REASON_VMRESUME:\n    case EXIT_REASON_INVEPT:\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_vmx_insn(regs, exit_reason) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, X86_EVENT_NO_EC);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, X86_EVENT_NO_EC);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( !hvm_emulate_one_insn(x86_insn_is_portio, \"port I/O\") )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm.vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm.single_step )\n        {\n            hvm_monitor_debug(regs->rip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx, msr_fold(regs)) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        vmx_handle_descriptor_access(exit_reason);\n        break;\n\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        gprintk(XENLOG_ERR, \"Unexpected vmexit: reason %lu\\n\", exit_reason);\n\n        if ( vmx_get_cpl() )\n            hvm_inject_hw_exception(TRAP_invalid_op,\n                                    X86_EVENT_NO_EC);\n        else\n            domain_crash(v->domain);\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->eip )\n    {\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        if ( vmx_get_cpl() )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}",
    "abstract_func_before": "void vmx_vmexit_handler(struct cpu_user_regs *VAR_0)\n{\n    unsigned long VAR_1, VAR_2, VAR_3, VAR_4 = 0;\n    unsigned int VAR_5 = 0, VAR_6;\n    struct vcpu *VAR_7 = VAR_8;\n    struct domain *VAR_9 = VAR_7->domain;\n\n    __vmread(VAR_10,    &VAR_0->rip);\n    __vmread(VAR_11,    &VAR_0->rsp);\n    __vmread(VAR_12, &VAR_0->rflags);\n\n    hvm_invalidate_regs_fields(VAR_0);\n\n    if ( paging_mode_hap(VAR_7->domain) )\n    {\n        /* COMMENT_0 */\n                                                                               \n                           \n           \n        __vmread(VAR_13, &VAR_7->arch.hvm.hw_cr[4]);\n        VAR_7->arch.hvm.guest_cr[4] &= VAR_7->arch.hvm.vmx.cr4_host_mask;\n        VAR_7->arch.hvm.guest_cr[4] |= (VAR_7->arch.hvm.hw_cr[4] &\n                                    ~VAR_7->arch.hvm.vmx.cr4_host_mask);\n\n        __vmread(VAR_14, &VAR_7->arch.hvm.hw_cr[3]);\n        if ( vmx_unrestricted_guest(VAR_7) || hvm_paging_enabled(VAR_7) )\n            VAR_7->arch.hvm.guest_cr[3] = VAR_7->arch.hvm.hw_cr[3];\n    }\n\n    __vmread(VAR_15, &VAR_2);\n\n    if ( hvm_long_mode_active(VAR_7) )\n        HVMTRACE_ND(VAR_16, 0, 1/* COMMENT_4 */, 3, VAR_2,\n                    VAR_0->eip, VAR_0->rip >> 32, 0, 0, 0);\n    else\n        HVMTRACE_ND(VAR_17, 0, 1/* COMMENT_4 */, 2, VAR_2,\n                    VAR_0->eip, 0, 0, 0, 0);\n\n    perfc_incra(VAR_18, VAR_2);\n\n    /* COMMENT_5 */\n    switch ( (uint16_t)VAR_2 )\n    {\n    case VAR_19:\n        vmx_do_extint(VAR_0);\n        break;\n    case VAR_20:\n        __vmread(VAR_21, &VAR_4);\n        BUG_ON(!(VAR_4 & VAR_22));\n        VAR_5 = VAR_4 & VAR_23;\n        if ( VAR_5 == VAR_24 )\n            do_machine_check(VAR_0);\n        if ( (VAR_5 == VAR_25) &&\n             ((VAR_4 & VAR_26) ==\n              MASK_INSR(VAR_27, VAR_26)) )\n        {\n            VAR_28[VAR_25](VAR_0);\n            enable_nmis();\n        }\n        break;\n    case VAR_29:\n        do_machine_check(VAR_0);\n        break;\n    }\n\n    /* COMMENT_6 */\n    local_irq_enable();\n\n    /* COMMENT_7 */\n                                                                   \n                                                                    \n       \n    if ( altp2m_active(VAR_7->domain) &&\n        (VAR_7->arch.hvm.vmx.secondary_exec_control &\n        VAR_30) )\n    {\n        unsigned long VAR_31;\n\n        if ( VAR_7->arch.hvm.vmx.secondary_exec_control &\n            VAR_32 )\n            __vmread(VAR_33, &VAR_31);\n        else\n        {\n            unsigned long VAR_34;\n\n            __vmread(VAR_35, &VAR_34);\n\n            if ( (VAR_31 = p2m_find_altp2m_by_eptp(VAR_7->domain, VAR_34)) ==\n                 VAR_36 )\n            {\n                gdprintk(VAR_37, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(VAR_7->domain);\n\n                return;\n            }\n        }\n\n        if ( VAR_31 != vcpu_altp2m(VAR_7).p2midx )\n        {\n            BUG_ON(VAR_31 >= VAR_38);\n            atomic_dec(&p2m_get_altp2m(VAR_7)->active_vcpus);\n            vcpu_altp2m(VAR_7).p2midx = VAR_31;\n            atomic_inc(&p2m_get_altp2m(VAR_7)->active_vcpus);\n        }\n    }\n\n    /* COMMENT_11 */\n                                               \n       \n    vcpu_nestedhvm(VAR_7).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(VAR_7) )\n    {\n        paging_update_nestedmode(VAR_7);\n        if ( nvmx_n2_vmexit_handler(VAR_0, VAR_2) )\n            goto out;\n    }\n\n    if ( unlikely(VAR_2 & VAR_39) )\n        return vmx_failed_vmentry(VAR_2, VAR_0);\n\n    if ( VAR_7->arch.hvm.vmx.vmx_realmode )\n    {\n        /* COMMENT_14 */\n        VAR_0->eflags &= ~(VAR_40 | VAR_41);\n        VAR_0->eflags |= (VAR_7->arch.hvm.vmx.vm86_saved_eflags & VAR_41);\n\n        /* COMMENT_15 */\n                                                                \n        switch ( VAR_2 )\n        {\n        case VAR_20:\n            if ( VAR_5 != VAR_42\n                 && VAR_5 != VAR_25 \n                 && VAR_5 != VAR_24 ) \n            {\n        default:\n                perfc_incr(VAR_43);\n                VAR_7->arch.hvm.vmx.vmx_emulate = 1;\n                HVMTRACE_0D(VAR_44);\n                return;\n            }\n        case VAR_19:\n        case VAR_45:\n        case VAR_46:\n        case VAR_47:\n        case VAR_48:\n        case VAR_29:\n        case VAR_49:\n        case VAR_50:\n        case VAR_51:\n        case VAR_52:\n        case VAR_53:\n        case VAR_54:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(VAR_55, &VAR_3);\n    if ( VAR_2 != VAR_56 )\n        vmx_idtv_reinject(VAR_3);\n\n    switch ( VAR_2 )\n    {\n        unsigned long VAR_57;\n\n    case VAR_20:\n    {\n        /* COMMENT_17 */\n                                                               \n                                                                  \n                  \n           \n\n        /* COMMENT_22 */\n                                                                               \n                                                                            \n                                                                            \n           \n        if ( unlikely(VAR_4 & VAR_58) &&\n             !(VAR_3 & VAR_22) &&\n             (VAR_5 != VAR_59) )\n        {\n            unsigned long VAR_60;\n\n            __vmread(VAR_61, &VAR_60);\n            __vmwrite(VAR_61,\n                      VAR_60 | VAR_62);\n        }\n\n        perfc_incra(VAR_63, VAR_5);\n\n        switch ( VAR_5 )\n        {\n        case VAR_64:\n            /* COMMENT_27 */\n                                                                  \n                                                                      \n               \n            __vmread(VAR_65, &VAR_1);\n            HVMTRACE_1D(VAR_66, VAR_1);\n            __restore_debug_registers(VAR_7);\n            write_debugreg(6, VAR_1 | VAR_67);\n            if ( !VAR_7->domain->debugger_attached )\n            {\n                unsigned long VAR_68 = 0;\n                int VAR_69;\n                unsigned long VAR_70 = MASK_EXTR(VAR_4,\n                                                    VAR_26);\n\n                if ( VAR_70 >= VAR_71 )\n                    __vmread(VAR_72, &VAR_68);\n\n                VAR_69 = hvm_monitor_debug(VAR_0->rip,\n                                       VAR_73,\n                                       VAR_70, VAR_68);\n\n                if ( VAR_69 < 0 )\n                    goto exit_and_crash;\n                if ( !VAR_69 )\n                    vmx_propagate_intr(VAR_4);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case VAR_74:\n            HVMTRACE_1D(VAR_75, VAR_5);\n            if ( !VAR_7->domain->debugger_attached )\n            {\n                unsigned long VAR_68;\n                int VAR_69;\n\n                __vmread(VAR_72, &VAR_68);\n                VAR_69 = hvm_monitor_debug(VAR_0->rip,\n                                       VAR_76,\n                                       VAR_77,\n                                       VAR_68);\n\n                if ( VAR_69 < 0 )\n                    goto exit_and_crash;\n                if ( !VAR_69 )\n                    vmx_propagate_intr(VAR_4);\n            }\n            else\n            {\n                update_guest_eip(); /* COMMENT_31 */\n                VAR_7->arch.gdbsx_vcpu_event = VAR_74;\n                domain_pause_for_debugger();\n            }\n            break;\n        case VAR_78:\n            HVMTRACE_1D(VAR_75, VAR_5);\n            vmx_fpu_dirty_intercept();\n            break;\n        case VAR_42:\n            __vmread(VAR_65, &VAR_1);\n            __vmread(VAR_79, &VAR_57);\n            VAR_0->error_code = VAR_57;\n\n            HVM_DBG_LOG(VAR_80,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        VAR_0->rax, VAR_0->rbx, VAR_0->rcx,\n                        VAR_0->rdx, VAR_0->rsi, VAR_0->rdi);\n\n            if ( paging_fault(VAR_1, VAR_0) )\n            {\n                if ( trace_will_trace_event(VAR_81) )\n                    break;\n                if ( hvm_long_mode_active(VAR_7) )\n                    HVMTRACE_LONG_2D(VAR_82, VAR_0->error_code,\n                                     TRC_PAR_LONG(VAR_1) );\n                else\n                    HVMTRACE_2D(VAR_82,\n                                VAR_0->error_code, VAR_1 );\n                break;\n            }\n\n            hvm_inject_page_fault(VAR_0->error_code, VAR_1);\n            break;\n        case VAR_83:\n            HVMTRACE_1D(VAR_75, VAR_5);\n            vmx_propagate_intr(VAR_4);\n            break;\n        case VAR_25:\n            if ( MASK_EXTR(VAR_4, VAR_26) !=\n                 VAR_27 )\n                goto exit_and_crash;\n            HVMTRACE_0D(VAR_84);\n            /* COMMENT_32 */\n            break;\n        case VAR_24:\n            HVMTRACE_0D(VAR_85);\n            /* COMMENT_32 */\n            break;\n        case VAR_86:\n            HVMTRACE_1D(VAR_75, VAR_5);\n            hvm_ud_intercept(VAR_0);\n            break;\n        default:\n            HVMTRACE_1D(VAR_75, VAR_5);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case VAR_19:\n        /* COMMENT_32 */\n        break;\n    case VAR_87:\n        hvm_triple_fault();\n        break;\n    case VAR_47:\n        /* COMMENT_33 */\n        VAR_7->arch.hvm.vmx.exec_control &= ~VAR_88;\n        vmx_update_cpu_exec_control(VAR_7);\n        break;\n    case VAR_48:\n        /* COMMENT_34 */\n        VAR_7->arch.hvm.vmx.exec_control &= ~VAR_89;\n        vmx_update_cpu_exec_control(VAR_7);\n        break;\n    case VAR_56: {\n        static const enum hvm_task_switch_reason VAR_90[] = {\n            VAR_91, VAR_92, VAR_93, VAR_91\n        };\n        unsigned int VAR_94, VAR_95;\n\n        __vmread(VAR_65, &VAR_1);\n        VAR_95 = (VAR_1 >> 30) & 3;\n        /* COMMENT_35 */\n        WARN_ON((VAR_95 == 3) && !(VAR_3 & VAR_22));\n        /* COMMENT_36 */\n                                                                       \n                                                            \n                                                                            \n           \n        VAR_94 = ((VAR_95 != 3) ||        /* COMMENT_41 */\n                    (MASK_EXTR(VAR_3, VAR_26)\n                     > 3)) /* COMMENT_42 */\n            ? get_instruction_length() /* COMMENT_43 */ : 0;\n        if ( (VAR_95 == 3) && (VAR_3 & VAR_96) )\n            __vmread(VAR_97, &VAR_57);\n        else\n             VAR_57 = -1;\n\n        hvm_task_switch(VAR_1, VAR_90[VAR_95], VAR_57, VAR_94,\n                        0 /* COMMENT_44 */);\n        break;\n    }\n    case VAR_98:\n    {\n        int VAR_69 = hvm_vmexit_cpuid(VAR_0, get_instruction_length());\n\n        /* COMMENT_45 */\n                                                  \n                                   \n                                                                \n           \n        if ( VAR_69 < 0 )\n            goto exit_and_crash;\n        if ( !VAR_69 )\n            update_guest_eip(); /* COMMENT_50 */\n        break;\n    }\n    case VAR_99:\n        update_guest_eip(); /* COMMENT_51 */\n        hvm_hlt(VAR_0->eflags);\n        break;\n    case VAR_100:\n        update_guest_eip(); /* COMMENT_52 */\n        __vmread(VAR_65, &VAR_1);\n        vmx_invlpg_intercept(VAR_1);\n        break;\n    case VAR_101:\n        if ( !VAR_9->arch.cpuid->extd.rdtscp )\n        {\n            hvm_inject_hw_exception(VAR_86, VAR_102);\n            break;\n        }\n\n        VAR_0->rcx = VAR_7->arch.msrs->tsc_aux;\n        /* COMMENT_53 */\n    case VAR_103:\n        update_guest_eip(); /* COMMENT_54 */\n        hvm_rdtsc_intercept(VAR_0);\n        break;\n\n    case VAR_104:\n        HVMTRACE_1D(VAR_105, VAR_0->eax);\n\n        if ( hvm_hypercall(VAR_0) == VAR_106 )\n            update_guest_eip(); /* COMMENT_55 */\n        break;\n\n    case VAR_107:\n    {\n        __vmread(VAR_65, &VAR_1);\n        if ( vmx_cr_access(VAR_1) == VAR_108 )\n            update_guest_eip(); /* COMMENT_56 */\n        break;\n    }\n    case VAR_109:\n        __vmread(VAR_65, &VAR_1);\n        vmx_dr_access(VAR_1, VAR_0);\n        break;\n    case VAR_110:\n    {\n        uint64_t VAR_111 = 0;\n\n        switch ( hvm_msr_read_intercept(VAR_0->ecx, &VAR_111) )\n        {\n        case VAR_108:\n            msr_split(VAR_0, VAR_111);\n            update_guest_eip(); /* COMMENT_57 */\n            break;\n\n        case VAR_112:\n            hvm_inject_hw_exception(VAR_113, 0);\n            break;\n        }\n        break;\n    }\n\n    case VAR_114:\n        switch ( hvm_msr_write_intercept(VAR_0->ecx, msr_fold(VAR_0), true) )\n        {\n        case VAR_108:\n            update_guest_eip(); /* COMMENT_58 */\n            break;\n\n        case VAR_112:\n            hvm_inject_hw_exception(VAR_113, 0);\n            break;\n        }\n        break;\n\n    case VAR_115:\n    case VAR_116:\n    case VAR_117:\n    case VAR_118:\n    case VAR_119:\n    case VAR_120:\n    case VAR_121:\n    case VAR_122:\n    case VAR_123:\n    case VAR_53:\n    case VAR_54:\n        if ( nvmx_handle_vmx_insn(VAR_0, VAR_2) == VAR_108 )\n            update_guest_eip();\n        break;\n\n    case VAR_124:\n        if ( vmx_vmfunc_intercept(VAR_0) != VAR_108 )\n            hvm_inject_hw_exception(VAR_86, VAR_102);\n        else\n            update_guest_eip();\n        break;\n\n    case VAR_125:\n    case VAR_126:\n    case VAR_49:\n        /* COMMENT_59 */\n                                                                           \n                                                                           \n                            \n           \n        WARN_ON(VAR_2 == VAR_49);\n        hvm_inject_hw_exception(VAR_86, VAR_102);\n        break;\n\n    case VAR_127:\n        break;\n\n    case VAR_128:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(VAR_113, 0);\n        break;\n\n    case VAR_129:\n        __vmread(VAR_65, &VAR_1);\n\n        ASSERT(VAR_130);\n\n        vlapic_handle_EOI(vcpu_vlapic(VAR_7), VAR_1);\n        break;\n\n    case VAR_131:\n        __vmread(VAR_65, &VAR_1);\n        if ( VAR_1 & 0x10 )\n        {\n            /* COMMENT_64 */\n            if ( !hvm_emulate_one_insn(VAR_132, \"port I/O\") )\n                hvm_inject_hw_exception(VAR_113, 0);\n        }\n        else\n        {\n            /* COMMENT_65 */\n            uint16_t VAR_133 = (VAR_1 >> 16) & 0xFFFF;\n            int VAR_134 = (VAR_1 & 0x07) + 1;\n            int VAR_135 = (VAR_1 & 0x08) ? VAR_136 : VAR_137;\n            if ( handle_pio(VAR_133, VAR_134, VAR_135) )\n                update_guest_eip(); /* COMMENT_66 */\n        }\n        break;\n\n    case VAR_138:\n    case VAR_139:\n    {\n        update_guest_eip(); /* COMMENT_67 */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case VAR_140:\n    {\n        paddr_t VAR_141;\n\n        __vmread(VAR_142, &VAR_141);\n        __vmread(VAR_65, &VAR_1);\n        ept_handle_violation(VAR_1, VAR_141);\n        break;\n    }\n\n    case VAR_143:\n    {\n        paddr_t VAR_141;\n\n        __vmread(VAR_142, &VAR_141);\n        if ( !ept_handle_misconfig(VAR_141) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case VAR_144:\n        VAR_7->arch.hvm.vmx.exec_control &= ~VAR_145;\n        vmx_update_cpu_exec_control(VAR_7);\n        if ( VAR_7->arch.hvm.single_step )\n        {\n            hvm_monitor_debug(VAR_0->rip,\n                              VAR_146,\n                              0, 0);\n\n            if ( VAR_7->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case VAR_147:\n        perfc_incr(VAR_148);\n        do_sched_op(VAR_149, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case VAR_150:\n        if ( hvm_handle_xsetbv(VAR_0->ecx, msr_fold(VAR_0)) == VAR_108 )\n            update_guest_eip(); /* COMMENT_68 */\n        break;\n\n    case VAR_151:\n        vmx_handle_apic_write();\n        break;\n\n    case VAR_152:\n        vmx_vcpu_flush_pml_buffer(VAR_7);\n        break;\n\n    case VAR_153:\n        vmx_handle_xsaves();\n        break;\n\n    case VAR_154:\n        vmx_handle_xrstors();\n        break;\n\n    case VAR_50:\n    case VAR_51:\n        vmx_handle_descriptor_access(VAR_2);\n        break;\n\n    case VAR_52:\n    case VAR_155:\n    /* COMMENT_53 */\n    default:\n    exit_and_crash:\n        gprintk(VAR_37, \"Unexpected vmexit: reason %lu\\n\", VAR_2);\n\n        if ( vmx_get_cpl() )\n            hvm_inject_hw_exception(VAR_86,\n                                    VAR_102);\n        else\n            domain_crash(VAR_7->domain);\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(VAR_7) )\n        nvmx_idtv_handling();\n\n    /* COMMENT_69 */\n                                                                        \n                                                                     \n                                                                             \n                                                                           \n                                                                           \n                                                                     \n                                                                           \n                                          \n       \n    VAR_6 = vmx_guest_x86_mode(VAR_7);\n    if ( VAR_6 == 8 ? !is_canonical_address(VAR_0->rip)\n                   : VAR_0->rip != VAR_0->eip )\n    {\n        gprintk(VAR_156, \"Bad rIP %lx for mode %u\\n\", VAR_0->rip, VAR_6);\n\n        if ( vmx_get_cpl() )\n        {\n            __vmread(VAR_157, &VAR_4);\n            if ( !(VAR_4 & VAR_22) )\n                hvm_inject_hw_exception(VAR_113, 0);\n            /* COMMENT_79 */\n            if ( VAR_6 == 8 )\n                VAR_0->rip = (long)(VAR_0->rip << (64 - VAR_158)) >>\n                            (64 - VAR_158);\n            else\n                VAR_0->rip = VAR_0->eip;\n        }\n        else\n            domain_crash(VAR_7->domain);\n    }\n}",
    "func_graph_path_before": "xen-project/xen/1d3eb8259804e5bec991a3462d69ba6bd80bb40e/vmx.c/vul/before/0.json",
    "func": "void vmx_vmexit_handler(struct cpu_user_regs *regs)\n{\n    unsigned long exit_qualification, exit_reason, idtv_info, intr_info = 0;\n    unsigned int vector = 0, mode;\n    struct vcpu *v = current;\n    struct domain *currd = v->domain;\n\n    __vmread(GUEST_RIP,    &regs->rip);\n    __vmread(GUEST_RSP,    &regs->rsp);\n    __vmread(GUEST_RFLAGS, &regs->rflags);\n\n    hvm_invalidate_regs_fields(regs);\n\n    if ( paging_mode_hap(v->domain) )\n    {\n        /*\n         * Xen allows the guest to modify some CR4 bits directly, update cached\n         * values to match.\n         */\n        __vmread(GUEST_CR4, &v->arch.hvm.hw_cr[4]);\n        v->arch.hvm.guest_cr[4] &= v->arch.hvm.vmx.cr4_host_mask;\n        v->arch.hvm.guest_cr[4] |= (v->arch.hvm.hw_cr[4] &\n                                    ~v->arch.hvm.vmx.cr4_host_mask);\n\n        __vmread(GUEST_CR3, &v->arch.hvm.hw_cr[3]);\n        if ( vmx_unrestricted_guest(v) || hvm_paging_enabled(v) )\n            v->arch.hvm.guest_cr[3] = v->arch.hvm.hw_cr[3];\n    }\n\n    __vmread(VM_EXIT_REASON, &exit_reason);\n\n    if ( hvm_long_mode_active(v) )\n        HVMTRACE_ND(VMEXIT64, 0, 1/*cycles*/, 3, exit_reason,\n                    regs->eip, regs->rip >> 32, 0, 0, 0);\n    else\n        HVMTRACE_ND(VMEXIT, 0, 1/*cycles*/, 2, exit_reason,\n                    regs->eip, 0, 0, 0, 0);\n\n    perfc_incra(vmexits, exit_reason);\n\n    /* Handle the interrupt we missed before allowing any more in. */\n    switch ( (uint16_t)exit_reason )\n    {\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        vmx_do_extint(regs);\n        break;\n    case EXIT_REASON_EXCEPTION_NMI:\n        __vmread(VM_EXIT_INTR_INFO, &intr_info);\n        BUG_ON(!(intr_info & INTR_INFO_VALID_MASK));\n        vector = intr_info & INTR_INFO_VECTOR_MASK;\n        if ( vector == TRAP_machine_check )\n            do_machine_check(regs);\n        if ( (vector == TRAP_nmi) &&\n             ((intr_info & INTR_INFO_INTR_TYPE_MASK) ==\n              MASK_INSR(X86_EVENTTYPE_NMI, INTR_INFO_INTR_TYPE_MASK)) )\n        {\n            exception_table[TRAP_nmi](regs);\n            enable_nmis();\n        }\n        break;\n    case EXIT_REASON_MCE_DURING_VMENTRY:\n        do_machine_check(regs);\n        break;\n    }\n\n    /* Now enable interrupts so it's safe to take locks. */\n    local_irq_enable();\n\n    /*\n     * If the guest has the ability to switch EPTP without an exit,\n     * figure out whether it has done so and update the altp2m data.\n     */\n    if ( altp2m_active(v->domain) &&\n        (v->arch.hvm.vmx.secondary_exec_control &\n        SECONDARY_EXEC_ENABLE_VM_FUNCTIONS) )\n    {\n        unsigned long idx;\n\n        if ( v->arch.hvm.vmx.secondary_exec_control &\n            SECONDARY_EXEC_ENABLE_VIRT_EXCEPTIONS )\n            __vmread(EPTP_INDEX, &idx);\n        else\n        {\n            unsigned long eptp;\n\n            __vmread(EPT_POINTER, &eptp);\n\n            if ( (idx = p2m_find_altp2m_by_eptp(v->domain, eptp)) ==\n                 INVALID_ALTP2M )\n            {\n                gdprintk(XENLOG_ERR, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(v->domain);\n\n                return;\n            }\n        }\n\n        if ( idx != vcpu_altp2m(v).p2midx )\n        {\n            BUG_ON(idx >= MAX_ALTP2M);\n            atomic_dec(&p2m_get_altp2m(v)->active_vcpus);\n            vcpu_altp2m(v).p2midx = idx;\n            atomic_inc(&p2m_get_altp2m(v)->active_vcpus);\n        }\n    }\n\n    /* XXX: This looks ugly, but we need a mechanism to ensure\n     * any pending vmresume has really happened\n     */\n    vcpu_nestedhvm(v).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n    {\n        paging_update_nestedmode(v);\n        if ( nvmx_n2_vmexit_handler(regs, exit_reason) )\n            goto out;\n    }\n\n    if ( unlikely(exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) )\n        return vmx_failed_vmentry(exit_reason, regs);\n\n    if ( v->arch.hvm.vmx.vmx_realmode )\n    {\n        /* Put RFLAGS back the way the guest wants it */\n        regs->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IOPL);\n        regs->eflags |= (v->arch.hvm.vmx.vm86_saved_eflags & X86_EFLAGS_IOPL);\n\n        /* Unless this exit was for an interrupt, we've hit something\n         * vm86 can't handle.  Try again, using the emulator. */\n        switch ( exit_reason )\n        {\n        case EXIT_REASON_EXCEPTION_NMI:\n            if ( vector != TRAP_page_fault\n                 && vector != TRAP_nmi \n                 && vector != TRAP_machine_check ) \n            {\n        default:\n                perfc_incr(realmode_exits);\n                v->arch.hvm.vmx.vmx_emulate = 1;\n                HVMTRACE_0D(REALMODE_EMULATE);\n                return;\n            }\n        case EXIT_REASON_EXTERNAL_INTERRUPT:\n        case EXIT_REASON_INIT:\n        case EXIT_REASON_SIPI:\n        case EXIT_REASON_PENDING_VIRT_INTR:\n        case EXIT_REASON_PENDING_VIRT_NMI:\n        case EXIT_REASON_MCE_DURING_VMENTRY:\n        case EXIT_REASON_GETSEC:\n        case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n        case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n        case EXIT_REASON_INVEPT:\n        case EXIT_REASON_INVVPID:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(IDT_VECTORING_INFO, &idtv_info);\n    if ( exit_reason != EXIT_REASON_TASK_SWITCH )\n        vmx_idtv_reinject(idtv_info);\n\n    switch ( exit_reason )\n    {\n        unsigned long ecode;\n\n    case EXIT_REASON_EXCEPTION_NMI:\n    {\n        /*\n         * We don't set the software-interrupt exiting (INT n).\n         * (1) We can get an exception (e.g. #PG) in the guest, or\n         * (2) NMI\n         */\n\n        /*\n         * Re-set the NMI shadow if vmexit caused by a guest IRET fault (see 3B\n         * 25.7.1.2, \"Resuming Guest Software after Handling an Exception\").\n         * (NB. If we emulate this IRET for any reason, we should re-clear!)\n         */\n        if ( unlikely(intr_info & INTR_INFO_NMI_UNBLOCKED_BY_IRET) &&\n             !(idtv_info & INTR_INFO_VALID_MASK) &&\n             (vector != TRAP_double_fault) )\n        {\n            unsigned long guest_info;\n\n            __vmread(GUEST_INTERRUPTIBILITY_INFO, &guest_info);\n            __vmwrite(GUEST_INTERRUPTIBILITY_INFO,\n                      guest_info | VMX_INTR_SHADOW_NMI);\n        }\n\n        perfc_incra(cause_vector, vector);\n\n        switch ( vector )\n        {\n        case TRAP_debug:\n            /*\n             * Updates DR6 where debugger can peek (See 3B 23.2.1,\n             * Table 23-1, \"Exit Qualification for Debug Exceptions\").\n             */\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n            __restore_debug_registers(v);\n            write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n\n            /*\n             * Work around SingleStep + STI/MovSS VMEntry failures.\n             *\n             * We intercept #DB unconditionally to work around CVE-2015-8104 /\n             * XSA-156 (guest-kernel induced host DoS).\n             *\n             * STI/MovSS shadows block/defer interrupts/exceptions (exact\n             * details are complicated and poorly documented).  Debug\n             * exceptions delayed for any reason are stored in the\n             * PENDING_DBG_EXCEPTIONS field.\n             *\n             * The falling edge of PENDING_DBG causes #DB to be delivered,\n             * resulting in a VMExit, as #DB is intercepted.  The VMCS still\n             * reports blocked-by-STI/MovSS.\n             *\n             * The VMEntry checks when EFLAGS.TF is set don't like a VMCS in\n             * this state.  Despite a #DB queued in VMENTRY_INTR_INFO, the\n             * state is rejected as DR6.BS isn't pending.  Fix this up.\n             */\n            if ( unlikely(regs->eflags & X86_EFLAGS_TF) )\n            {\n                unsigned long int_info;\n\n                __vmread(GUEST_INTERRUPTIBILITY_INFO, &int_info);\n\n                if ( int_info & (VMX_INTR_SHADOW_STI | VMX_INTR_SHADOW_MOV_SS) )\n                {\n                    unsigned long pending_dbg;\n\n                    __vmread(GUEST_PENDING_DBG_EXCEPTIONS, &pending_dbg);\n                    __vmwrite(GUEST_PENDING_DBG_EXCEPTIONS,\n                              pending_dbg | DR_STEP);\n                }\n            }\n\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len = 0;\n                int rc;\n                unsigned long trap_type = MASK_EXTR(intr_info,\n                                                    INTR_INFO_INTR_TYPE_MASK);\n\n                if ( trap_type >= X86_EVENTTYPE_SW_INTERRUPT )\n                    __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n\n                rc = hvm_monitor_debug(regs->rip,\n                                       HVM_MONITOR_DEBUG_EXCEPTION,\n                                       trap_type, insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case TRAP_int3:\n            HVMTRACE_1D(TRAP, vector);\n            if ( !v->domain->debugger_attached )\n            {\n                unsigned long insn_len;\n                int rc;\n\n                __vmread(VM_EXIT_INSTRUCTION_LEN, &insn_len);\n                rc = hvm_monitor_debug(regs->rip,\n                                       HVM_MONITOR_SOFTWARE_BREAKPOINT,\n                                       X86_EVENTTYPE_SW_EXCEPTION,\n                                       insn_len);\n\n                if ( rc < 0 )\n                    goto exit_and_crash;\n                if ( !rc )\n                    vmx_propagate_intr(intr_info);\n            }\n            else\n            {\n                update_guest_eip(); /* Safe: INT3 */\n                v->arch.gdbsx_vcpu_event = TRAP_int3;\n                domain_pause_for_debugger();\n            }\n            break;\n        case TRAP_no_device:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_fpu_dirty_intercept();\n            break;\n        case TRAP_page_fault:\n            __vmread(EXIT_QUALIFICATION, &exit_qualification);\n            __vmread(VM_EXIT_INTR_ERROR_CODE, &ecode);\n            regs->error_code = ecode;\n\n            HVM_DBG_LOG(DBG_LEVEL_VMMU,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        regs->rax, regs->rbx, regs->rcx,\n                        regs->rdx, regs->rsi, regs->rdi);\n\n            if ( paging_fault(exit_qualification, regs) )\n            {\n                if ( trace_will_trace_event(TRC_SHADOW) )\n                    break;\n                if ( hvm_long_mode_active(v) )\n                    HVMTRACE_LONG_2D(PF_XEN, regs->error_code,\n                                     TRC_PAR_LONG(exit_qualification) );\n                else\n                    HVMTRACE_2D(PF_XEN,\n                                regs->error_code, exit_qualification );\n                break;\n            }\n\n            hvm_inject_page_fault(regs->error_code, exit_qualification);\n            break;\n        case TRAP_alignment_check:\n            HVMTRACE_1D(TRAP, vector);\n            vmx_propagate_intr(intr_info);\n            break;\n        case TRAP_nmi:\n            if ( MASK_EXTR(intr_info, INTR_INFO_INTR_TYPE_MASK) !=\n                 X86_EVENTTYPE_NMI )\n                goto exit_and_crash;\n            HVMTRACE_0D(NMI);\n            /* Already handled above. */\n            break;\n        case TRAP_machine_check:\n            HVMTRACE_0D(MCE);\n            /* Already handled above. */\n            break;\n        case TRAP_invalid_op:\n            HVMTRACE_1D(TRAP, vector);\n            hvm_ud_intercept(regs);\n            break;\n        default:\n            HVMTRACE_1D(TRAP, vector);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case EXIT_REASON_EXTERNAL_INTERRUPT:\n        /* Already handled above. */\n        break;\n    case EXIT_REASON_TRIPLE_FAULT:\n        hvm_triple_fault();\n        break;\n    case EXIT_REASON_PENDING_VIRT_INTR:\n        /* Disable the interrupt window. */\n        v->arch.hvm.vmx.exec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_PENDING_VIRT_NMI:\n        /* Disable the NMI window. */\n        v->arch.hvm.vmx.exec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n        vmx_update_cpu_exec_control(v);\n        break;\n    case EXIT_REASON_TASK_SWITCH: {\n        static const enum hvm_task_switch_reason reasons[] = {\n            TSW_call_or_int, TSW_iret, TSW_jmp, TSW_call_or_int\n        };\n        unsigned int inst_len, source;\n\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        source = (exit_qualification >> 30) & 3;\n        /* Vectored event should fill in interrupt information. */\n        WARN_ON((source == 3) && !(idtv_info & INTR_INFO_VALID_MASK));\n        /*\n         * In the following cases there is an instruction to skip over:\n         *  - TSW is due to a CALL, IRET or JMP instruction.\n         *  - TSW is a vectored event due to a SW exception or SW interrupt.\n         */\n        inst_len = ((source != 3) ||        /* CALL, IRET, or JMP? */\n                    (MASK_EXTR(idtv_info, INTR_INFO_INTR_TYPE_MASK)\n                     > 3)) /* IntrType > 3? */\n            ? get_instruction_length() /* Safe: SDM 3B 23.2.4 */ : 0;\n        if ( (source == 3) && (idtv_info & INTR_INFO_DELIVER_CODE_MASK) )\n            __vmread(IDT_VECTORING_ERROR_CODE, &ecode);\n        else\n             ecode = -1;\n\n        hvm_task_switch(exit_qualification, reasons[source], ecode, inst_len,\n                        0 /* EFLAGS.RF already updated. */);\n        break;\n    }\n    case EXIT_REASON_CPUID:\n    {\n        int rc = hvm_vmexit_cpuid(regs, get_instruction_length());\n\n        /*\n         * rc < 0 error in monitor/vm_event, crash\n         * !rc    continue normally\n         * rc > 0 paused waiting for response, work here is done\n         */\n        if ( rc < 0 )\n            goto exit_and_crash;\n        if ( !rc )\n            update_guest_eip(); /* Safe: CPUID */\n        break;\n    }\n    case EXIT_REASON_HLT:\n        update_guest_eip(); /* Safe: HLT */\n        hvm_hlt(regs->eflags);\n        break;\n    case EXIT_REASON_INVLPG:\n        update_guest_eip(); /* Safe: INVLPG */\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_invlpg_intercept(exit_qualification);\n        break;\n    case EXIT_REASON_RDTSCP:\n        if ( !currd->arch.cpuid->extd.rdtscp )\n        {\n            hvm_inject_hw_exception(TRAP_invalid_op, X86_EVENT_NO_EC);\n            break;\n        }\n\n        regs->rcx = v->arch.msrs->tsc_aux;\n        /* fall through */\n    case EXIT_REASON_RDTSC:\n        update_guest_eip(); /* Safe: RDTSC, RDTSCP */\n        hvm_rdtsc_intercept(regs);\n        break;\n\n    case EXIT_REASON_VMCALL:\n        HVMTRACE_1D(VMMCALL, regs->eax);\n\n        if ( hvm_hypercall(regs) == HVM_HCALL_completed )\n            update_guest_eip(); /* Safe: VMCALL */\n        break;\n\n    case EXIT_REASON_CR_ACCESS:\n    {\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( vmx_cr_access(exit_qualification) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: MOV Cn, LMSW, CLTS */\n        break;\n    }\n    case EXIT_REASON_DR_ACCESS:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        vmx_dr_access(exit_qualification, regs);\n        break;\n    case EXIT_REASON_MSR_READ:\n    {\n        uint64_t msr_content = 0;\n\n        switch ( hvm_msr_read_intercept(regs->ecx, &msr_content) )\n        {\n        case X86EMUL_OKAY:\n            msr_split(regs, msr_content);\n            update_guest_eip(); /* Safe: RDMSR */\n            break;\n\n        case X86EMUL_EXCEPTION:\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            break;\n        }\n        break;\n    }\n\n    case EXIT_REASON_MSR_WRITE:\n        switch ( hvm_msr_write_intercept(regs->ecx, msr_fold(regs), true) )\n        {\n        case X86EMUL_OKAY:\n            update_guest_eip(); /* Safe: WRMSR */\n            break;\n\n        case X86EMUL_EXCEPTION:\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            break;\n        }\n        break;\n\n    case EXIT_REASON_VMXOFF:\n    case EXIT_REASON_VMXON:\n    case EXIT_REASON_VMCLEAR:\n    case EXIT_REASON_VMPTRLD:\n    case EXIT_REASON_VMPTRST:\n    case EXIT_REASON_VMREAD:\n    case EXIT_REASON_VMWRITE:\n    case EXIT_REASON_VMLAUNCH:\n    case EXIT_REASON_VMRESUME:\n    case EXIT_REASON_INVEPT:\n    case EXIT_REASON_INVVPID:\n        if ( nvmx_handle_vmx_insn(regs, exit_reason) == X86EMUL_OKAY )\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_VMFUNC:\n        if ( vmx_vmfunc_intercept(regs) != X86EMUL_OKAY )\n            hvm_inject_hw_exception(TRAP_invalid_op, X86_EVENT_NO_EC);\n        else\n            update_guest_eip();\n        break;\n\n    case EXIT_REASON_MWAIT_INSTRUCTION:\n    case EXIT_REASON_MONITOR_INSTRUCTION:\n    case EXIT_REASON_GETSEC:\n        /*\n         * We should never exit on GETSEC because CR4.SMXE is always 0 when\n         * running in guest context, and the CPU checks that before getting\n         * as far as vmexit.\n         */\n        WARN_ON(exit_reason == EXIT_REASON_GETSEC);\n        hvm_inject_hw_exception(TRAP_invalid_op, X86_EVENT_NO_EC);\n        break;\n\n    case EXIT_REASON_TPR_BELOW_THRESHOLD:\n        break;\n\n    case EXIT_REASON_APIC_ACCESS:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        break;\n\n    case EXIT_REASON_EOI_INDUCED:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n\n        ASSERT(cpu_has_vmx_virtual_intr_delivery);\n\n        vlapic_handle_EOI(vcpu_vlapic(v), exit_qualification);\n        break;\n\n    case EXIT_REASON_IO_INSTRUCTION:\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        if ( exit_qualification & 0x10 )\n        {\n            /* INS, OUTS */\n            if ( !hvm_emulate_one_insn(x86_insn_is_portio, \"port I/O\") )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n        }\n        else\n        {\n            /* IN, OUT */\n            uint16_t port = (exit_qualification >> 16) & 0xFFFF;\n            int bytes = (exit_qualification & 0x07) + 1;\n            int dir = (exit_qualification & 0x08) ? IOREQ_READ : IOREQ_WRITE;\n            if ( handle_pio(port, bytes, dir) )\n                update_guest_eip(); /* Safe: IN, OUT */\n        }\n        break;\n\n    case EXIT_REASON_INVD:\n    case EXIT_REASON_WBINVD:\n    {\n        update_guest_eip(); /* Safe: INVD, WBINVD */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case EXIT_REASON_EPT_VIOLATION:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        __vmread(EXIT_QUALIFICATION, &exit_qualification);\n        ept_handle_violation(exit_qualification, gpa);\n        break;\n    }\n\n    case EXIT_REASON_EPT_MISCONFIG:\n    {\n        paddr_t gpa;\n\n        __vmread(GUEST_PHYSICAL_ADDRESS, &gpa);\n        if ( !ept_handle_misconfig(gpa) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case EXIT_REASON_MONITOR_TRAP_FLAG:\n        v->arch.hvm.vmx.exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;\n        vmx_update_cpu_exec_control(v);\n        if ( v->arch.hvm.single_step )\n        {\n            hvm_monitor_debug(regs->rip,\n                              HVM_MONITOR_SINGLESTEP_BREAKPOINT,\n                              0, 0);\n\n            if ( v->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case EXIT_REASON_PAUSE_INSTRUCTION:\n        perfc_incr(pauseloop_exits);\n        do_sched_op(SCHEDOP_yield, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case EXIT_REASON_XSETBV:\n        if ( hvm_handle_xsetbv(regs->ecx, msr_fold(regs)) == X86EMUL_OKAY )\n            update_guest_eip(); /* Safe: XSETBV */\n        break;\n\n    case EXIT_REASON_APIC_WRITE:\n        vmx_handle_apic_write();\n        break;\n\n    case EXIT_REASON_PML_FULL:\n        vmx_vcpu_flush_pml_buffer(v);\n        break;\n\n    case EXIT_REASON_XSAVES:\n        vmx_handle_xsaves();\n        break;\n\n    case EXIT_REASON_XRSTORS:\n        vmx_handle_xrstors();\n        break;\n\n    case EXIT_REASON_ACCESS_GDTR_OR_IDTR:\n    case EXIT_REASON_ACCESS_LDTR_OR_TR:\n        vmx_handle_descriptor_access(exit_reason);\n        break;\n\n    case EXIT_REASON_VMX_PREEMPTION_TIMER_EXPIRED:\n    case EXIT_REASON_INVPCID:\n    /* fall through */\n    default:\n    exit_and_crash:\n        gprintk(XENLOG_ERR, \"Unexpected vmexit: reason %lu\\n\", exit_reason);\n\n        if ( vmx_get_cpl() )\n            hvm_inject_hw_exception(TRAP_invalid_op,\n                                    X86_EVENT_NO_EC);\n        else\n            domain_crash(v->domain);\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(v) )\n        nvmx_idtv_handling();\n\n    /*\n     * VM entry will fail (causing the guest to get crashed) if rIP (and\n     * rFLAGS, but we don't have an issue there) doesn't meet certain\n     * criteria. As we must not allow less than fully privileged mode to have\n     * such an effect on the domain, we correct rIP in that case (accepting\n     * this not being architecturally correct behavior, as the injected #GP\n     * fault will then not see the correct [invalid] return address).\n     * And since we know the guest will crash, we crash it right away if it\n     * already is in most privileged mode.\n     */\n    mode = vmx_guest_x86_mode(v);\n    if ( mode == 8 ? !is_canonical_address(regs->rip)\n                   : regs->rip != regs->eip )\n    {\n        gprintk(XENLOG_WARNING, \"Bad rIP %lx for mode %u\\n\", regs->rip, mode);\n\n        if ( vmx_get_cpl() )\n        {\n            __vmread(VM_ENTRY_INTR_INFO, &intr_info);\n            if ( !(intr_info & INTR_INFO_VALID_MASK) )\n                hvm_inject_hw_exception(TRAP_gp_fault, 0);\n            /* Need to fix rIP nevertheless. */\n            if ( mode == 8 )\n                regs->rip = (long)(regs->rip << (64 - VADDR_BITS)) >>\n                            (64 - VADDR_BITS);\n            else\n                regs->rip = regs->eip;\n        }\n        else\n            domain_crash(v->domain);\n    }\n}",
    "abstract_func": "void vmx_vmexit_handler(struct cpu_user_regs *VAR_0)\n{\n    unsigned long VAR_1, VAR_2, VAR_3, VAR_4 = 0;\n    unsigned int VAR_5 = 0, VAR_6;\n    struct vcpu *VAR_7 = VAR_8;\n    struct domain *VAR_9 = VAR_7->domain;\n\n    __vmread(VAR_10,    &VAR_0->rip);\n    __vmread(VAR_11,    &VAR_0->rsp);\n    __vmread(VAR_12, &VAR_0->rflags);\n\n    hvm_invalidate_regs_fields(VAR_0);\n\n    if ( paging_mode_hap(VAR_7->domain) )\n    {\n        /* COMMENT_0 */\n                                                                               \n                           \n           \n        __vmread(VAR_13, &VAR_7->arch.hvm.hw_cr[4]);\n        VAR_7->arch.hvm.guest_cr[4] &= VAR_7->arch.hvm.vmx.cr4_host_mask;\n        VAR_7->arch.hvm.guest_cr[4] |= (VAR_7->arch.hvm.hw_cr[4] &\n                                    ~VAR_7->arch.hvm.vmx.cr4_host_mask);\n\n        __vmread(VAR_14, &VAR_7->arch.hvm.hw_cr[3]);\n        if ( vmx_unrestricted_guest(VAR_7) || hvm_paging_enabled(VAR_7) )\n            VAR_7->arch.hvm.guest_cr[3] = VAR_7->arch.hvm.hw_cr[3];\n    }\n\n    __vmread(VAR_15, &VAR_2);\n\n    if ( hvm_long_mode_active(VAR_7) )\n        HVMTRACE_ND(VAR_16, 0, 1/* COMMENT_4 */, 3, VAR_2,\n                    VAR_0->eip, VAR_0->rip >> 32, 0, 0, 0);\n    else\n        HVMTRACE_ND(VAR_17, 0, 1/* COMMENT_4 */, 2, VAR_2,\n                    VAR_0->eip, 0, 0, 0, 0);\n\n    perfc_incra(VAR_18, VAR_2);\n\n    /* COMMENT_5 */\n    switch ( (uint16_t)VAR_2 )\n    {\n    case VAR_19:\n        vmx_do_extint(VAR_0);\n        break;\n    case VAR_20:\n        __vmread(VAR_21, &VAR_4);\n        BUG_ON(!(VAR_4 & VAR_22));\n        VAR_5 = VAR_4 & VAR_23;\n        if ( VAR_5 == VAR_24 )\n            do_machine_check(VAR_0);\n        if ( (VAR_5 == VAR_25) &&\n             ((VAR_4 & VAR_26) ==\n              MASK_INSR(VAR_27, VAR_26)) )\n        {\n            VAR_28[VAR_25](VAR_0);\n            enable_nmis();\n        }\n        break;\n    case VAR_29:\n        do_machine_check(VAR_0);\n        break;\n    }\n\n    /* COMMENT_6 */\n    local_irq_enable();\n\n    /* COMMENT_7 */\n                                                                   \n                                                                    \n       \n    if ( altp2m_active(VAR_7->domain) &&\n        (VAR_7->arch.hvm.vmx.secondary_exec_control &\n        VAR_30) )\n    {\n        unsigned long VAR_31;\n\n        if ( VAR_7->arch.hvm.vmx.secondary_exec_control &\n            VAR_32 )\n            __vmread(VAR_33, &VAR_31);\n        else\n        {\n            unsigned long VAR_34;\n\n            __vmread(VAR_35, &VAR_34);\n\n            if ( (VAR_31 = p2m_find_altp2m_by_eptp(VAR_7->domain, VAR_34)) ==\n                 VAR_36 )\n            {\n                gdprintk(VAR_37, \"EPTP not found in alternate p2m list\\n\");\n                domain_crash(VAR_7->domain);\n\n                return;\n            }\n        }\n\n        if ( VAR_31 != vcpu_altp2m(VAR_7).p2midx )\n        {\n            BUG_ON(VAR_31 >= VAR_38);\n            atomic_dec(&p2m_get_altp2m(VAR_7)->active_vcpus);\n            vcpu_altp2m(VAR_7).p2midx = VAR_31;\n            atomic_inc(&p2m_get_altp2m(VAR_7)->active_vcpus);\n        }\n    }\n\n    /* COMMENT_11 */\n                                               \n       \n    vcpu_nestedhvm(VAR_7).nv_vmswitch_in_progress = 0;\n    if ( nestedhvm_vcpu_in_guestmode(VAR_7) )\n    {\n        paging_update_nestedmode(VAR_7);\n        if ( nvmx_n2_vmexit_handler(VAR_0, VAR_2) )\n            goto out;\n    }\n\n    if ( unlikely(VAR_2 & VAR_39) )\n        return vmx_failed_vmentry(VAR_2, VAR_0);\n\n    if ( VAR_7->arch.hvm.vmx.vmx_realmode )\n    {\n        /* COMMENT_14 */\n        VAR_0->eflags &= ~(VAR_40 | VAR_41);\n        VAR_0->eflags |= (VAR_7->arch.hvm.vmx.vm86_saved_eflags & VAR_41);\n\n        /* COMMENT_15 */\n                                                                \n        switch ( VAR_2 )\n        {\n        case VAR_20:\n            if ( VAR_5 != VAR_42\n                 && VAR_5 != VAR_25 \n                 && VAR_5 != VAR_24 ) \n            {\n        default:\n                perfc_incr(VAR_43);\n                VAR_7->arch.hvm.vmx.vmx_emulate = 1;\n                HVMTRACE_0D(VAR_44);\n                return;\n            }\n        case VAR_19:\n        case VAR_45:\n        case VAR_46:\n        case VAR_47:\n        case VAR_48:\n        case VAR_29:\n        case VAR_49:\n        case VAR_50:\n        case VAR_51:\n        case VAR_52:\n        case VAR_53:\n        case VAR_54:\n            break;\n        }\n    }\n\n    hvm_maybe_deassert_evtchn_irq();\n\n    __vmread(VAR_55, &VAR_3);\n    if ( VAR_2 != VAR_56 )\n        vmx_idtv_reinject(VAR_3);\n\n    switch ( VAR_2 )\n    {\n        unsigned long VAR_57;\n\n    case VAR_20:\n    {\n        /* COMMENT_17 */\n                                                               \n                                                                  \n                  \n           \n\n        /* COMMENT_22 */\n                                                                               \n                                                                            \n                                                                            \n           \n        if ( unlikely(VAR_4 & VAR_58) &&\n             !(VAR_3 & VAR_22) &&\n             (VAR_5 != VAR_59) )\n        {\n            unsigned long VAR_60;\n\n            __vmread(VAR_61, &VAR_60);\n            __vmwrite(VAR_61,\n                      VAR_60 | VAR_62);\n        }\n\n        perfc_incra(VAR_63, VAR_5);\n\n        switch ( VAR_5 )\n        {\n        case VAR_64:\n            /* COMMENT_27 */\n                                                                  \n                                                                      \n               \n            __vmread(VAR_65, &VAR_1);\n            HVMTRACE_1D(VAR_66, VAR_1);\n            __restore_debug_registers(VAR_7);\n            write_debugreg(6, VAR_1 | VAR_67);\n\n            /* COMMENT_31 */\n                                                                   \n              \n                                                                              \n                                                       \n              \n                                                                         \n                                                                     \n                                                                  \n                                            \n              \n                                                                          \n                                                                            \n                                            \n              \n                                                                            \n                                                                          \n                                                                       \n               \n            if ( unlikely(VAR_0->eflags & VAR_68) )\n            {\n                unsigned long VAR_69;\n\n                __vmread(VAR_61, &VAR_69);\n\n                if ( VAR_69 & (VAR_70 | VAR_71) )\n                {\n                    unsigned long VAR_72;\n\n                    __vmread(VAR_73, &VAR_72);\n                    __vmwrite(VAR_73,\n                              VAR_72 | VAR_74);\n                }\n            }\n\n            if ( !VAR_7->domain->debugger_attached )\n            {\n                unsigned long VAR_75 = 0;\n                int VAR_76;\n                unsigned long VAR_77 = MASK_EXTR(VAR_4,\n                                                    VAR_26);\n\n                if ( VAR_77 >= VAR_78 )\n                    __vmread(VAR_79, &VAR_75);\n\n                VAR_76 = hvm_monitor_debug(VAR_0->rip,\n                                       VAR_80,\n                                       VAR_77, VAR_75);\n\n                if ( VAR_76 < 0 )\n                    goto exit_and_crash;\n                if ( !VAR_76 )\n                    vmx_propagate_intr(VAR_4);\n            }\n            else\n                domain_pause_for_debugger();\n            break;\n        case VAR_81:\n            HVMTRACE_1D(VAR_82, VAR_5);\n            if ( !VAR_7->domain->debugger_attached )\n            {\n                unsigned long VAR_75;\n                int VAR_76;\n\n                __vmread(VAR_79, &VAR_75);\n                VAR_76 = hvm_monitor_debug(VAR_0->rip,\n                                       VAR_83,\n                                       VAR_84,\n                                       VAR_75);\n\n                if ( VAR_76 < 0 )\n                    goto exit_and_crash;\n                if ( !VAR_76 )\n                    vmx_propagate_intr(VAR_4);\n            }\n            else\n            {\n                update_guest_eip(); /* COMMENT_47 */\n                VAR_7->arch.gdbsx_vcpu_event = VAR_81;\n                domain_pause_for_debugger();\n            }\n            break;\n        case VAR_85:\n            HVMTRACE_1D(VAR_82, VAR_5);\n            vmx_fpu_dirty_intercept();\n            break;\n        case VAR_42:\n            __vmread(VAR_65, &VAR_1);\n            __vmread(VAR_86, &VAR_57);\n            VAR_0->error_code = VAR_57;\n\n            HVM_DBG_LOG(VAR_87,\n                        \"eax=%lx, ebx=%lx, ecx=%lx, edx=%lx, esi=%lx, edi=%lx\",\n                        VAR_0->rax, VAR_0->rbx, VAR_0->rcx,\n                        VAR_0->rdx, VAR_0->rsi, VAR_0->rdi);\n\n            if ( paging_fault(VAR_1, VAR_0) )\n            {\n                if ( trace_will_trace_event(VAR_88) )\n                    break;\n                if ( hvm_long_mode_active(VAR_7) )\n                    HVMTRACE_LONG_2D(VAR_89, VAR_0->error_code,\n                                     TRC_PAR_LONG(VAR_1) );\n                else\n                    HVMTRACE_2D(VAR_89,\n                                VAR_0->error_code, VAR_1 );\n                break;\n            }\n\n            hvm_inject_page_fault(VAR_0->error_code, VAR_1);\n            break;\n        case VAR_90:\n            HVMTRACE_1D(VAR_82, VAR_5);\n            vmx_propagate_intr(VAR_4);\n            break;\n        case VAR_25:\n            if ( MASK_EXTR(VAR_4, VAR_26) !=\n                 VAR_27 )\n                goto exit_and_crash;\n            HVMTRACE_0D(VAR_91);\n            /* COMMENT_48 */\n            break;\n        case VAR_24:\n            HVMTRACE_0D(VAR_92);\n            /* COMMENT_48 */\n            break;\n        case VAR_93:\n            HVMTRACE_1D(VAR_82, VAR_5);\n            hvm_ud_intercept(VAR_0);\n            break;\n        default:\n            HVMTRACE_1D(VAR_82, VAR_5);\n            goto exit_and_crash;\n        }\n        break;\n    }\n    case VAR_19:\n        /* COMMENT_48 */\n        break;\n    case VAR_94:\n        hvm_triple_fault();\n        break;\n    case VAR_47:\n        /* COMMENT_49 */\n        VAR_7->arch.hvm.vmx.exec_control &= ~VAR_95;\n        vmx_update_cpu_exec_control(VAR_7);\n        break;\n    case VAR_48:\n        /* COMMENT_50 */\n        VAR_7->arch.hvm.vmx.exec_control &= ~VAR_96;\n        vmx_update_cpu_exec_control(VAR_7);\n        break;\n    case VAR_56: {\n        static const enum hvm_task_switch_reason VAR_97[] = {\n            VAR_98, VAR_99, VAR_100, VAR_98\n        };\n        unsigned int VAR_101, VAR_102;\n\n        __vmread(VAR_65, &VAR_1);\n        VAR_102 = (VAR_1 >> 30) & 3;\n        /* COMMENT_51 */\n        WARN_ON((VAR_102 == 3) && !(VAR_3 & VAR_22));\n        /* COMMENT_52 */\n                                                                       \n                                                            \n                                                                            \n           \n        VAR_101 = ((VAR_102 != 3) ||        /* COMMENT_57 */\n                    (MASK_EXTR(VAR_3, VAR_26)\n                     > 3)) /* COMMENT_58 */\n            ? get_instruction_length() /* COMMENT_59 */ : 0;\n        if ( (VAR_102 == 3) && (VAR_3 & VAR_103) )\n            __vmread(VAR_104, &VAR_57);\n        else\n             VAR_57 = -1;\n\n        hvm_task_switch(VAR_1, VAR_97[VAR_102], VAR_57, VAR_101,\n                        0 /* COMMENT_60 */);\n        break;\n    }\n    case VAR_105:\n    {\n        int VAR_76 = hvm_vmexit_cpuid(VAR_0, get_instruction_length());\n\n        /* COMMENT_61 */\n                                                  \n                                   \n                                                                \n           \n        if ( VAR_76 < 0 )\n            goto exit_and_crash;\n        if ( !VAR_76 )\n            update_guest_eip(); /* COMMENT_66 */\n        break;\n    }\n    case VAR_106:\n        update_guest_eip(); /* COMMENT_67 */\n        hvm_hlt(VAR_0->eflags);\n        break;\n    case VAR_107:\n        update_guest_eip(); /* COMMENT_68 */\n        __vmread(VAR_65, &VAR_1);\n        vmx_invlpg_intercept(VAR_1);\n        break;\n    case VAR_108:\n        if ( !VAR_9->arch.cpuid->extd.rdtscp )\n        {\n            hvm_inject_hw_exception(VAR_93, VAR_109);\n            break;\n        }\n\n        VAR_0->rcx = VAR_7->arch.msrs->tsc_aux;\n        /* COMMENT_69 */\n    case VAR_110:\n        update_guest_eip(); /* COMMENT_70 */\n        hvm_rdtsc_intercept(VAR_0);\n        break;\n\n    case VAR_111:\n        HVMTRACE_1D(VAR_112, VAR_0->eax);\n\n        if ( hvm_hypercall(VAR_0) == VAR_113 )\n            update_guest_eip(); /* COMMENT_71 */\n        break;\n\n    case VAR_114:\n    {\n        __vmread(VAR_65, &VAR_1);\n        if ( vmx_cr_access(VAR_1) == VAR_115 )\n            update_guest_eip(); /* COMMENT_72 */\n        break;\n    }\n    case VAR_116:\n        __vmread(VAR_65, &VAR_1);\n        vmx_dr_access(VAR_1, VAR_0);\n        break;\n    case VAR_117:\n    {\n        uint64_t VAR_118 = 0;\n\n        switch ( hvm_msr_read_intercept(VAR_0->ecx, &VAR_118) )\n        {\n        case VAR_115:\n            msr_split(VAR_0, VAR_118);\n            update_guest_eip(); /* COMMENT_73 */\n            break;\n\n        case VAR_119:\n            hvm_inject_hw_exception(VAR_120, 0);\n            break;\n        }\n        break;\n    }\n\n    case VAR_121:\n        switch ( hvm_msr_write_intercept(VAR_0->ecx, msr_fold(VAR_0), true) )\n        {\n        case VAR_115:\n            update_guest_eip(); /* COMMENT_74 */\n            break;\n\n        case VAR_119:\n            hvm_inject_hw_exception(VAR_120, 0);\n            break;\n        }\n        break;\n\n    case VAR_122:\n    case VAR_123:\n    case VAR_124:\n    case VAR_125:\n    case VAR_126:\n    case VAR_127:\n    case VAR_128:\n    case VAR_129:\n    case VAR_130:\n    case VAR_53:\n    case VAR_54:\n        if ( nvmx_handle_vmx_insn(VAR_0, VAR_2) == VAR_115 )\n            update_guest_eip();\n        break;\n\n    case VAR_131:\n        if ( vmx_vmfunc_intercept(VAR_0) != VAR_115 )\n            hvm_inject_hw_exception(VAR_93, VAR_109);\n        else\n            update_guest_eip();\n        break;\n\n    case VAR_132:\n    case VAR_133:\n    case VAR_49:\n        /* COMMENT_75 */\n                                                                           \n                                                                           \n                            \n           \n        WARN_ON(VAR_2 == VAR_49);\n        hvm_inject_hw_exception(VAR_93, VAR_109);\n        break;\n\n    case VAR_134:\n        break;\n\n    case VAR_135:\n        if ( !vmx_handle_eoi_write() && !handle_mmio() )\n            hvm_inject_hw_exception(VAR_120, 0);\n        break;\n\n    case VAR_136:\n        __vmread(VAR_65, &VAR_1);\n\n        ASSERT(VAR_137);\n\n        vlapic_handle_EOI(vcpu_vlapic(VAR_7), VAR_1);\n        break;\n\n    case VAR_138:\n        __vmread(VAR_65, &VAR_1);\n        if ( VAR_1 & 0x10 )\n        {\n            /* COMMENT_80 */\n            if ( !hvm_emulate_one_insn(VAR_139, \"port I/O\") )\n                hvm_inject_hw_exception(VAR_120, 0);\n        }\n        else\n        {\n            /* COMMENT_81 */\n            uint16_t VAR_140 = (VAR_1 >> 16) & 0xFFFF;\n            int VAR_141 = (VAR_1 & 0x07) + 1;\n            int VAR_142 = (VAR_1 & 0x08) ? VAR_143 : VAR_144;\n            if ( handle_pio(VAR_140, VAR_141, VAR_142) )\n                update_guest_eip(); /* COMMENT_82 */\n        }\n        break;\n\n    case VAR_145:\n    case VAR_146:\n    {\n        update_guest_eip(); /* COMMENT_83 */\n        vmx_wbinvd_intercept();\n        break;\n    }\n\n    case VAR_147:\n    {\n        paddr_t VAR_148;\n\n        __vmread(VAR_149, &VAR_148);\n        __vmread(VAR_65, &VAR_1);\n        ept_handle_violation(VAR_1, VAR_148);\n        break;\n    }\n\n    case VAR_150:\n    {\n        paddr_t VAR_148;\n\n        __vmread(VAR_149, &VAR_148);\n        if ( !ept_handle_misconfig(VAR_148) )\n            goto exit_and_crash;\n        break;\n    }\n\n    case VAR_151:\n        VAR_7->arch.hvm.vmx.exec_control &= ~VAR_152;\n        vmx_update_cpu_exec_control(VAR_7);\n        if ( VAR_7->arch.hvm.single_step )\n        {\n            hvm_monitor_debug(VAR_0->rip,\n                              VAR_153,\n                              0, 0);\n\n            if ( VAR_7->domain->debugger_attached )\n                domain_pause_for_debugger();\n        }\n\n        break;\n\n    case VAR_154:\n        perfc_incr(VAR_155);\n        do_sched_op(VAR_156, guest_handle_from_ptr(NULL, void));\n        break;\n\n    case VAR_157:\n        if ( hvm_handle_xsetbv(VAR_0->ecx, msr_fold(VAR_0)) == VAR_115 )\n            update_guest_eip(); /* COMMENT_84 */\n        break;\n\n    case VAR_158:\n        vmx_handle_apic_write();\n        break;\n\n    case VAR_159:\n        vmx_vcpu_flush_pml_buffer(VAR_7);\n        break;\n\n    case VAR_160:\n        vmx_handle_xsaves();\n        break;\n\n    case VAR_161:\n        vmx_handle_xrstors();\n        break;\n\n    case VAR_50:\n    case VAR_51:\n        vmx_handle_descriptor_access(VAR_2);\n        break;\n\n    case VAR_52:\n    case VAR_162:\n    /* COMMENT_69 */\n    default:\n    exit_and_crash:\n        gprintk(VAR_37, \"Unexpected vmexit: reason %lu\\n\", VAR_2);\n\n        if ( vmx_get_cpl() )\n            hvm_inject_hw_exception(VAR_93,\n                                    VAR_109);\n        else\n            domain_crash(VAR_7->domain);\n        break;\n    }\n\nout:\n    if ( nestedhvm_vcpu_in_guestmode(VAR_7) )\n        nvmx_idtv_handling();\n\n    /* COMMENT_85 */\n                                                                        \n                                                                     \n                                                                             \n                                                                           \n                                                                           \n                                                                     \n                                                                           \n                                          \n       \n    VAR_6 = vmx_guest_x86_mode(VAR_7);\n    if ( VAR_6 == 8 ? !is_canonical_address(VAR_0->rip)\n                   : VAR_0->rip != VAR_0->eip )\n    {\n        gprintk(VAR_163, \"Bad rIP %lx for mode %u\\n\", VAR_0->rip, VAR_6);\n\n        if ( vmx_get_cpl() )\n        {\n            __vmread(VAR_164, &VAR_4);\n            if ( !(VAR_4 & VAR_22) )\n                hvm_inject_hw_exception(VAR_120, 0);\n            /* COMMENT_95 */\n            if ( VAR_6 == 8 )\n                VAR_0->rip = (long)(VAR_0->rip << (64 - VAR_165)) >>\n                            (64 - VAR_165);\n            else\n                VAR_0->rip = VAR_0->eip;\n        }\n        else\n            domain_crash(VAR_7->domain);\n    }\n}",
    "func_graph_path": "xen-project/xen/1d3eb8259804e5bec991a3462d69ba6bd80bb40e/vmx.c/vul/after/0.json",
    "diff_func": "--- func_before\n+++ func_after\n@@ -202,6 +202,42 @@\n             HVMTRACE_1D(TRAP_DEBUG, exit_qualification);\n             __restore_debug_registers(v);\n             write_debugreg(6, exit_qualification | DR_STATUS_RESERVED_ONE);\n+\n+            /*\n+             * Work around SingleStep + STI/MovSS VMEntry failures.\n+             *\n+             * We intercept #DB unconditionally to work around CVE-2015-8104 /\n+             * XSA-156 (guest-kernel induced host DoS).\n+             *\n+             * STI/MovSS shadows block/defer interrupts/exceptions (exact\n+             * details are complicated and poorly documented).  Debug\n+             * exceptions delayed for any reason are stored in the\n+             * PENDING_DBG_EXCEPTIONS field.\n+             *\n+             * The falling edge of PENDING_DBG causes #DB to be delivered,\n+             * resulting in a VMExit, as #DB is intercepted.  The VMCS still\n+             * reports blocked-by-STI/MovSS.\n+             *\n+             * The VMEntry checks when EFLAGS.TF is set don't like a VMCS in\n+             * this state.  Despite a #DB queued in VMENTRY_INTR_INFO, the\n+             * state is rejected as DR6.BS isn't pending.  Fix this up.\n+             */\n+            if ( unlikely(regs->eflags & X86_EFLAGS_TF) )\n+            {\n+                unsigned long int_info;\n+\n+                __vmread(GUEST_INTERRUPTIBILITY_INFO, &int_info);\n+\n+                if ( int_info & (VMX_INTR_SHADOW_STI | VMX_INTR_SHADOW_MOV_SS) )\n+                {\n+                    unsigned long pending_dbg;\n+\n+                    __vmread(GUEST_PENDING_DBG_EXCEPTIONS, &pending_dbg);\n+                    __vmwrite(GUEST_PENDING_DBG_EXCEPTIONS,\n+                              pending_dbg | DR_STEP);\n+                }\n+            }\n+\n             if ( !v->domain->debugger_attached )\n             {\n                 unsigned long insn_len = 0;",
    "diff_line_info": {
        "deleted_lines": [],
        "added_lines": [
            "",
            "            /*",
            "             * Work around SingleStep + STI/MovSS VMEntry failures.",
            "             *",
            "             * We intercept #DB unconditionally to work around CVE-2015-8104 /",
            "             * XSA-156 (guest-kernel induced host DoS).",
            "             *",
            "             * STI/MovSS shadows block/defer interrupts/exceptions (exact",
            "             * details are complicated and poorly documented).  Debug",
            "             * exceptions delayed for any reason are stored in the",
            "             * PENDING_DBG_EXCEPTIONS field.",
            "             *",
            "             * The falling edge of PENDING_DBG causes #DB to be delivered,",
            "             * resulting in a VMExit, as #DB is intercepted.  The VMCS still",
            "             * reports blocked-by-STI/MovSS.",
            "             *",
            "             * The VMEntry checks when EFLAGS.TF is set don't like a VMCS in",
            "             * this state.  Despite a #DB queued in VMENTRY_INTR_INFO, the",
            "             * state is rejected as DR6.BS isn't pending.  Fix this up.",
            "             */",
            "            if ( unlikely(regs->eflags & X86_EFLAGS_TF) )",
            "            {",
            "                unsigned long int_info;",
            "",
            "                __vmread(GUEST_INTERRUPTIBILITY_INFO, &int_info);",
            "",
            "                if ( int_info & (VMX_INTR_SHADOW_STI | VMX_INTR_SHADOW_MOV_SS) )",
            "                {",
            "                    unsigned long pending_dbg;",
            "",
            "                    __vmread(GUEST_PENDING_DBG_EXCEPTIONS, &pending_dbg);",
            "                    __vmwrite(GUEST_PENDING_DBG_EXCEPTIONS,",
            "                              pending_dbg | DR_STEP);",
            "                }",
            "            }",
            ""
        ]
    },
    "is_vul": true,
    "pr_url": null,
    "description": "no more info"
}