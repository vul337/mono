{
    "cve_id": "CVE-2022-35987",
    "cwe_ids": [
        "CWE-617"
    ],
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H",
    "cvss_is_v3": true,
    "repo_name": "tensorflow",
    "commit_msg": "Fix security vulnerability with DenseBincountOp\n\nPiperOrigin-RevId: 460826735",
    "commit_hash": "bf4c14353c2328636a18bfad1e151052c81d5f43",
    "git_url": "https://github.com/tensorflow/tensorflow/commit/bf4c14353c2328636a18bfad1e151052c81d5f43",
    "file_path": "tensorflow/compiler/tf2xla/kernels/bincount_op.cc",
    "func_name": "Compile",
    "func_before": "void Compile(XlaOpKernelContext* ctx) override {\n    int64_t output_size;\n    xla::XlaOp output_size_param = ctx->Input(\"size\");\n    StatusOr<xla::Shape> output_shape_or =\n        ctx->builder()->GetShape(output_size_param);\n    OP_REQUIRES_OK(ctx, output_shape_or.status());\n    auto output_shape_param = output_shape_or.ValueOrDie();\n    auto output_rank = output_shape_param.rank();\n    OP_REQUIRES(ctx, output_rank == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        output_rank));\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntScalar(\"size\", &output_size));\n    OP_REQUIRES(ctx, output_size >= 0,\n                errors::InvalidArgument(\"size (\", output_size,\n                                        \") must be non-negative\"));\n    xla::XlaOp idx, updates, output;\n    xla::XlaOp input = ctx->Input(0);\n    auto input_xla_type = ctx->input_xla_type(0);\n    xla::PrimitiveType dtype = ctx->InputXlaType(\"weights\");\n    auto zero = xla::Zero(ctx->builder(), dtype);\n    auto one = xla::One(ctx->builder(), dtype);\n    StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n    OP_REQUIRES_OK(ctx, input_shape_or.status());\n    auto input_shape = input_shape_or.ValueOrDie();\n    auto size = input_shape.dimensions(0);\n\n    if (!size) {\n      output = xla::Broadcast(zero, {output_size});\n      ctx->SetOutput(0, output);\n      return;\n    }\n    auto rank = input_shape.rank();\n\n    OP_REQUIRES(ctx, rank <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", rank));\n\n    xla::XlaOp weights = ctx->Input(2);\n    StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n    OP_REQUIRES_OK(ctx, weights_shape_or.status());\n\n    auto weights_shape = weights_shape_or.ValueOrDie();\n    auto weights_size = weights_shape.dimensions(0);\n    bool has_weights = false;\n    if (weights_size) {\n      has_weights = true;\n    }\n    xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n    xla::ScatterDimensionNumbers scatter_dnums;\n    scatter_dnums.set_index_vector_dim(1);\n    scatter_dnums.add_inserted_window_dims(0);\n    scatter_dnums.add_scatter_dims_to_operand_dims(0);\n\n    if (rank == 2) {\n      output_shape = xla::ShapeUtil::MakeShape(dtype, {size, output_size});\n      scatter_dnums.add_inserted_window_dims(1);\n      scatter_dnums.add_scatter_dims_to_operand_dims(1);\n      auto i_shape =\n          xla::ShapeUtil::MakeShape(input_xla_type, {input_shape.dimensions()});\n      auto i = xla::Iota(ctx->builder(), i_shape, 0);\n      i = xla::Reshape(\n          i, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      auto j = xla::Reshape(\n          input, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      std::vector<xla::XlaOp> iotas_to_concat;\n      iotas_to_concat.push_back(i);\n      iotas_to_concat.push_back(j);\n      idx = xla::ConcatInDim(ctx->builder(), iotas_to_concat, 1);\n      updates = xla::Broadcast(\n          one, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n      output = xla::Broadcast(\n          zero, {output_shape.dimensions(0), output_shape.dimensions(1)});\n      if (has_weights && !binary_output_) {\n        weights = xla::Reshape(\n            weights, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n        updates = weights;\n      }\n    } else {\n      input = xla::Reshape(input, {size, 1});\n      idx = xla::Reshape(input, {size, 1});\n      updates = xla::Broadcast(one, {size});\n      output = xla::Broadcast(zero, {output_size});\n      if (has_weights && !binary_output_) {\n        updates = weights;\n      }\n    }\n\n    xla::XlaComputation assn_computation = [&] {\n      std::unique_ptr<xla::XlaBuilder> subb =\n          ctx->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape param_shape = xla::ShapeUtil::MakeShape(dtype, {});\n      auto p0 = xla::Parameter(subb.get(), 0, param_shape, \"p0\");\n      auto p1 = xla::Parameter(subb.get(), 1, param_shape, \"p1\");\n      if (!binary_output_) {\n        xla::Add(p0, p1);\n      }\n      return subb->BuildAndNoteError();\n    }();\n    output = xla::Scatter(output, idx, updates, assn_computation, scatter_dnums,\n                          false, false);\n    ctx->SetOutput(0, output);\n  }",
    "abstract_func_before": "void Compile(XlaOpKernelContext* VAR_0) override {\n    int64_t VAR_1;\n    xla::XlaOp VAR_2 = VAR_0->Input(\"size\");\n    StatusOr<xla::Shape> VAR_3 =\n        VAR_0->builder()->GetShape(VAR_2);\n    OP_REQUIRES_OK(VAR_0, VAR_3.status());\n    auto VAR_4 = VAR_3.ValueOrDie();\n    auto VAR_5 = VAR_4.rank();\n    OP_REQUIRES(VAR_0, VAR_5 == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        VAR_5));\n    OP_REQUIRES_OK(VAR_0, VAR_0->ConstantInputAsIntScalar(\"size\", &VAR_1));\n    OP_REQUIRES(VAR_0, VAR_1 >= 0,\n                errors::InvalidArgument(\"size (\", VAR_1,\n                                        \") must be non-negative\"));\n    xla::XlaOp VAR_6, VAR_7, VAR_8;\n    xla::XlaOp VAR_9 = VAR_0->Input(0);\n    auto VAR_10 = VAR_0->input_xla_type(0);\n    xla::PrimitiveType VAR_11 = VAR_0->InputXlaType(\"weights\");\n    auto VAR_12 = xla::Zero(VAR_0->builder(), VAR_11);\n    auto VAR_13 = xla::One(VAR_0->builder(), VAR_11);\n    StatusOr<xla::Shape> VAR_14 = VAR_0->builder()->GetShape(VAR_9);\n    OP_REQUIRES_OK(VAR_0, VAR_14.status());\n    auto VAR_15 = VAR_14.ValueOrDie();\n    auto VAR_16 = VAR_15.dimensions(0);\n\n    if (!VAR_16) {\n      VAR_8 = xla::Broadcast(VAR_12, {VAR_1});\n      VAR_0->SetOutput(0, VAR_8);\n      return;\n    }\n    auto VAR_17 = VAR_15.rank();\n\n    OP_REQUIRES(VAR_0, VAR_17 <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", VAR_17));\n\n    xla::XlaOp VAR_18 = VAR_0->Input(2);\n    StatusOr<xla::Shape> VAR_19 = VAR_0->builder()->GetShape(VAR_18);\n    OP_REQUIRES_OK(VAR_0, VAR_19.status());\n\n    auto VAR_20 = VAR_19.ValueOrDie();\n    auto VAR_21 = VAR_20.dimensions(0);\n    bool VAR_22 = false;\n    if (VAR_21) {\n      VAR_22 = true;\n    }\n    xla::Shape VAR_23 = xla::ShapeUtil::MakeShape(VAR_11, {VAR_1});\n    xla::ScatterDimensionNumbers VAR_24;\n    VAR_24.set_index_vector_dim(1);\n    VAR_24.add_inserted_window_dims(0);\n    VAR_24.add_scatter_dims_to_operand_dims(0);\n\n    if (VAR_17 == 2) {\n      VAR_23 = xla::ShapeUtil::MakeShape(VAR_11, {VAR_16, VAR_1});\n      VAR_24.add_inserted_window_dims(1);\n      VAR_24.add_scatter_dims_to_operand_dims(1);\n      auto VAR_25 =\n          xla::ShapeUtil::MakeShape(VAR_10, {VAR_15.dimensions()});\n      auto VAR_26 = xla::Iota(VAR_0->builder(), VAR_25, 0);\n      VAR_26 = xla::Reshape(\n          VAR_26, {VAR_15.dimensions(0) * VAR_15.dimensions(1), 1});\n      auto VAR_27 = xla::Reshape(\n          VAR_9, {VAR_15.dimensions(0) * VAR_15.dimensions(1), 1});\n      std::vector<xla::XlaOp> VAR_28;\n      VAR_28.push_back(VAR_26);\n      VAR_28.push_back(VAR_27);\n      VAR_6 = xla::ConcatInDim(VAR_0->builder(), VAR_28, 1);\n      VAR_7 = xla::Broadcast(\n          VAR_13, {VAR_15.dimensions(0) * VAR_15.dimensions(1)});\n      VAR_8 = xla::Broadcast(\n          VAR_12, {VAR_23.dimensions(0), VAR_23.dimensions(1)});\n      if (VAR_22 && !VAR_29) {\n        VAR_18 = xla::Reshape(\n            VAR_18, {VAR_15.dimensions(0) * VAR_15.dimensions(1)});\n        VAR_7 = VAR_18;\n      }\n    } else {\n      VAR_9 = xla::Reshape(VAR_9, {VAR_16, 1});\n      VAR_6 = xla::Reshape(VAR_9, {VAR_16, 1});\n      VAR_7 = xla::Broadcast(VAR_13, {VAR_16});\n      VAR_8 = xla::Broadcast(VAR_12, {VAR_1});\n      if (VAR_22 && !VAR_29) {\n        VAR_7 = VAR_18;\n      }\n    }\n\n    xla::XlaComputation VAR_30 = [&] {\n      std::unique_ptr<xla::XlaBuilder> VAR_31 =\n          VAR_0->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape VAR_32 = xla::ShapeUtil::MakeShape(VAR_11, {});\n      auto VAR_33 = xla::Parameter(VAR_31.get(), 0, VAR_32, \"p0\");\n      auto VAR_34 = xla::Parameter(VAR_31.get(), 1, VAR_32, \"p1\");\n      if (!VAR_29) {\n        xla::Add(VAR_33, VAR_34);\n      }\n      return VAR_31->BuildAndNoteError();\n    }();\n    VAR_8 = xla::Scatter(VAR_8, VAR_6, VAR_7, VAR_30, VAR_24,\n                          false, false);\n    VAR_0->SetOutput(0, VAR_8);\n  }",
    "func_graph_path_before": null,
    "func": "void Compile(XlaOpKernelContext* ctx) override {\n    int64_t output_size;\n    xla::XlaOp output_size_param = ctx->Input(\"size\");\n    StatusOr<xla::Shape> output_shape_or =\n        ctx->builder()->GetShape(output_size_param);\n    OP_REQUIRES_OK(ctx, output_shape_or.status());\n    auto output_shape_param = output_shape_or.ValueOrDie();\n    auto output_rank = output_shape_param.rank();\n    OP_REQUIRES(ctx, output_rank == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        output_rank));\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntScalar(\"size\", &output_size));\n    OP_REQUIRES(ctx, output_size >= 0,\n                errors::InvalidArgument(\"size (\", output_size,\n                                        \") must be non-negative\"));\n    xla::XlaOp idx, updates, output;\n    xla::XlaOp input = ctx->Input(0);\n    auto input_xla_type = ctx->input_xla_type(0);\n    xla::PrimitiveType dtype = ctx->InputXlaType(\"weights\");\n    auto zero = xla::Zero(ctx->builder(), dtype);\n    auto one = xla::One(ctx->builder(), dtype);\n    StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n    OP_REQUIRES_OK(ctx, input_shape_or.status());\n    auto input_shape = input_shape_or.ValueOrDie();\n    auto size = input_shape.dimensions(0);\n\n    if (!size) {\n      output = xla::Broadcast(zero, {output_size});\n      ctx->SetOutput(0, output);\n      return;\n    }\n    auto rank = input_shape.rank();\n\n    OP_REQUIRES(ctx, rank <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", rank));\n\n    xla::XlaOp weights = ctx->Input(2);\n    StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n    OP_REQUIRES_OK(ctx, weights_shape_or.status());\n\n    auto weights_shape = weights_shape_or.ValueOrDie();\n    OP_REQUIRES(ctx,\n                xla::ShapeUtil::CompatibleIgnoringElementType(weights_shape,\n                                                              input_shape) ||\n                    (weights_shape.dimensions_size() > 0 &&\n                     weights_shape.dimensions(0) == 0),\n                errors::InvalidArgument(\n                    \"`weights` must be the same shape as `arr` or a length-0 \"\n                    \"`Tensor`, in which case it acts as all weights equal to \"\n                    \"1. Received \",\n                    weights_shape.DebugString()));\n\n    auto weights_size = weights_shape.dimensions(0);\n    bool has_weights = false;\n    if (weights_size) {\n      has_weights = true;\n    }\n    xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n    xla::ScatterDimensionNumbers scatter_dnums;\n    scatter_dnums.set_index_vector_dim(1);\n    scatter_dnums.add_inserted_window_dims(0);\n    scatter_dnums.add_scatter_dims_to_operand_dims(0);\n\n    if (rank == 2) {\n      output_shape = xla::ShapeUtil::MakeShape(dtype, {size, output_size});\n      scatter_dnums.add_inserted_window_dims(1);\n      scatter_dnums.add_scatter_dims_to_operand_dims(1);\n      auto i_shape =\n          xla::ShapeUtil::MakeShape(input_xla_type, {input_shape.dimensions()});\n      auto i = xla::Iota(ctx->builder(), i_shape, 0);\n      i = xla::Reshape(\n          i, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      auto j = xla::Reshape(\n          input, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      std::vector<xla::XlaOp> iotas_to_concat;\n      iotas_to_concat.push_back(i);\n      iotas_to_concat.push_back(j);\n      idx = xla::ConcatInDim(ctx->builder(), iotas_to_concat, 1);\n      updates = xla::Broadcast(\n          one, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n      output = xla::Broadcast(\n          zero, {output_shape.dimensions(0), output_shape.dimensions(1)});\n      if (has_weights && !binary_output_) {\n        weights = xla::Reshape(\n            weights, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n        updates = weights;\n      }\n    } else {\n      input = xla::Reshape(input, {size, 1});\n      idx = xla::Reshape(input, {size, 1});\n      updates = xla::Broadcast(one, {size});\n      output = xla::Broadcast(zero, {output_size});\n      if (has_weights && !binary_output_) {\n        updates = weights;\n      }\n    }\n\n    xla::XlaComputation assn_computation = [&] {\n      std::unique_ptr<xla::XlaBuilder> subb =\n          ctx->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape param_shape = xla::ShapeUtil::MakeShape(dtype, {});\n      auto p0 = xla::Parameter(subb.get(), 0, param_shape, \"p0\");\n      auto p1 = xla::Parameter(subb.get(), 1, param_shape, \"p1\");\n      if (!binary_output_) {\n        xla::Add(p0, p1);\n      }\n      return subb->BuildAndNoteError();\n    }();\n    output = xla::Scatter(output, idx, updates, assn_computation, scatter_dnums,\n                          false, false);\n    ctx->SetOutput(0, output);\n  }",
    "abstract_func": "void Compile(XlaOpKernelContext* VAR_0) override {\n    int64_t VAR_1;\n    xla::XlaOp VAR_2 = VAR_0->Input(\"size\");\n    StatusOr<xla::Shape> VAR_3 =\n        VAR_0->builder()->GetShape(VAR_2);\n    OP_REQUIRES_OK(VAR_0, VAR_3.status());\n    auto VAR_4 = VAR_3.ValueOrDie();\n    auto VAR_5 = VAR_4.rank();\n    OP_REQUIRES(VAR_0, VAR_5 == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        VAR_5));\n    OP_REQUIRES_OK(VAR_0, VAR_0->ConstantInputAsIntScalar(\"size\", &VAR_1));\n    OP_REQUIRES(VAR_0, VAR_1 >= 0,\n                errors::InvalidArgument(\"size (\", VAR_1,\n                                        \") must be non-negative\"));\n    xla::XlaOp VAR_6, VAR_7, VAR_8;\n    xla::XlaOp VAR_9 = VAR_0->Input(0);\n    auto VAR_10 = VAR_0->input_xla_type(0);\n    xla::PrimitiveType VAR_11 = VAR_0->InputXlaType(\"weights\");\n    auto VAR_12 = xla::Zero(VAR_0->builder(), VAR_11);\n    auto VAR_13 = xla::One(VAR_0->builder(), VAR_11);\n    StatusOr<xla::Shape> VAR_14 = VAR_0->builder()->GetShape(VAR_9);\n    OP_REQUIRES_OK(VAR_0, VAR_14.status());\n    auto VAR_15 = VAR_14.ValueOrDie();\n    auto VAR_16 = VAR_15.dimensions(0);\n\n    if (!VAR_16) {\n      VAR_8 = xla::Broadcast(VAR_12, {VAR_1});\n      VAR_0->SetOutput(0, VAR_8);\n      return;\n    }\n    auto VAR_17 = VAR_15.rank();\n\n    OP_REQUIRES(VAR_0, VAR_17 <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", VAR_17));\n\n    xla::XlaOp VAR_18 = VAR_0->Input(2);\n    StatusOr<xla::Shape> VAR_19 = VAR_0->builder()->GetShape(VAR_18);\n    OP_REQUIRES_OK(VAR_0, VAR_19.status());\n\n    auto VAR_20 = VAR_19.ValueOrDie();\n    OP_REQUIRES(VAR_0,\n                xla::ShapeUtil::CompatibleIgnoringElementType(VAR_20,\n                                                              VAR_15) ||\n                    (VAR_20.dimensions_size() > 0 &&\n                     VAR_20.dimensions(0) == 0),\n                errors::InvalidArgument(\n                    \"`weights` must be the same shape as `arr` or a length-0 \"\n                    \"`Tensor`, in which case it acts as all weights equal to \"\n                    \"1. Received \",\n                    VAR_20.DebugString()));\n\n    auto VAR_21 = VAR_20.dimensions(0);\n    bool VAR_22 = false;\n    if (VAR_21) {\n      VAR_22 = true;\n    }\n    xla::Shape VAR_23 = xla::ShapeUtil::MakeShape(VAR_11, {VAR_1});\n    xla::ScatterDimensionNumbers VAR_24;\n    VAR_24.set_index_vector_dim(1);\n    VAR_24.add_inserted_window_dims(0);\n    VAR_24.add_scatter_dims_to_operand_dims(0);\n\n    if (VAR_17 == 2) {\n      VAR_23 = xla::ShapeUtil::MakeShape(VAR_11, {VAR_16, VAR_1});\n      VAR_24.add_inserted_window_dims(1);\n      VAR_24.add_scatter_dims_to_operand_dims(1);\n      auto VAR_25 =\n          xla::ShapeUtil::MakeShape(VAR_10, {VAR_15.dimensions()});\n      auto VAR_26 = xla::Iota(VAR_0->builder(), VAR_25, 0);\n      VAR_26 = xla::Reshape(\n          VAR_26, {VAR_15.dimensions(0) * VAR_15.dimensions(1), 1});\n      auto VAR_27 = xla::Reshape(\n          VAR_9, {VAR_15.dimensions(0) * VAR_15.dimensions(1), 1});\n      std::vector<xla::XlaOp> VAR_28;\n      VAR_28.push_back(VAR_26);\n      VAR_28.push_back(VAR_27);\n      VAR_6 = xla::ConcatInDim(VAR_0->builder(), VAR_28, 1);\n      VAR_7 = xla::Broadcast(\n          VAR_13, {VAR_15.dimensions(0) * VAR_15.dimensions(1)});\n      VAR_8 = xla::Broadcast(\n          VAR_12, {VAR_23.dimensions(0), VAR_23.dimensions(1)});\n      if (VAR_22 && !VAR_29) {\n        VAR_18 = xla::Reshape(\n            VAR_18, {VAR_15.dimensions(0) * VAR_15.dimensions(1)});\n        VAR_7 = VAR_18;\n      }\n    } else {\n      VAR_9 = xla::Reshape(VAR_9, {VAR_16, 1});\n      VAR_6 = xla::Reshape(VAR_9, {VAR_16, 1});\n      VAR_7 = xla::Broadcast(VAR_13, {VAR_16});\n      VAR_8 = xla::Broadcast(VAR_12, {VAR_1});\n      if (VAR_22 && !VAR_29) {\n        VAR_7 = VAR_18;\n      }\n    }\n\n    xla::XlaComputation VAR_30 = [&] {\n      std::unique_ptr<xla::XlaBuilder> VAR_31 =\n          VAR_0->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape VAR_32 = xla::ShapeUtil::MakeShape(VAR_11, {});\n      auto VAR_33 = xla::Parameter(VAR_31.get(), 0, VAR_32, \"p0\");\n      auto VAR_34 = xla::Parameter(VAR_31.get(), 1, VAR_32, \"p1\");\n      if (!VAR_29) {\n        xla::Add(VAR_33, VAR_34);\n      }\n      return VAR_31->BuildAndNoteError();\n    }();\n    VAR_8 = xla::Scatter(VAR_8, VAR_6, VAR_7, VAR_30, VAR_24,\n                          false, false);\n    VAR_0->SetOutput(0, VAR_8);\n  }",
    "func_graph_path": null,
    "diff_func": "--- func_before\n+++ func_after\n@@ -40,6 +40,17 @@\n     OP_REQUIRES_OK(ctx, weights_shape_or.status());\n \n     auto weights_shape = weights_shape_or.ValueOrDie();\n+    OP_REQUIRES(ctx,\n+                xla::ShapeUtil::CompatibleIgnoringElementType(weights_shape,\n+                                                              input_shape) ||\n+                    (weights_shape.dimensions_size() > 0 &&\n+                     weights_shape.dimensions(0) == 0),\n+                errors::InvalidArgument(\n+                    \"`weights` must be the same shape as `arr` or a length-0 \"\n+                    \"`Tensor`, in which case it acts as all weights equal to \"\n+                    \"1. Received \",\n+                    weights_shape.DebugString()));\n+\n     auto weights_size = weights_shape.dimensions(0);\n     bool has_weights = false;\n     if (weights_size) {",
    "diff_line_info": {
        "deleted_lines": [],
        "added_lines": [
            "    OP_REQUIRES(ctx,",
            "                xla::ShapeUtil::CompatibleIgnoringElementType(weights_shape,",
            "                                                              input_shape) ||",
            "                    (weights_shape.dimensions_size() > 0 &&",
            "                     weights_shape.dimensions(0) == 0),",
            "                errors::InvalidArgument(",
            "                    \"`weights` must be the same shape as `arr` or a length-0 \"",
            "                    \"`Tensor`, in which case it acts as all weights equal to \"",
            "                    \"1. Received \",",
            "                    weights_shape.DebugString()));",
            ""
        ]
    },
    "is_vul": true,
    "pr_url": null,
    "description": "no more info"
}