{
    "cve_id": "CVE-2020-15191",
    "cwe_ids": [
        "CWE-252",
        "CWE-908",
        "CWE-20"
    ],
    "cvss_vector": "AV:N/AC:L/Au:N/C:N/I:N/A:P",
    "cvss_is_v3": false,
    "repo_name": "tensorflow",
    "commit_msg": "Fix multiple vulnerabilities in `tf.experimental.dlpack.to_dlpack`.\n\nWe have a use after free caused by memory coruption, a segmentation fault caused by memory corruption, several memory leaks and an undefined behavior when taking the reference of a nullptr.\n\nPiperOrigin-RevId: 332568894\nChange-Id: Ife0fc05e103b35325094ae5d822ee5fdea764572",
    "commit_hash": "22e07fb204386768e5bcbea563641ea11f96ceb8",
    "git_url": "https://github.com/tensorflow/tensorflow/commit/22e07fb204386768e5bcbea563641ea11f96ceb8",
    "file_path": "tensorflow/c/eager/dlpack.cc",
    "func_name": "TFE_HandleToDLPack",
    "func_before": "void* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {\n  const Tensor* tensor = GetTensorFromHandle(h, status);\n  TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());\n  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()\n\n  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);\n  tf_dlm_tensor_ctx->reference = tensor_ref;\n\n  DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;\n  dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;\n  dlm_tensor->deleter = &DLManagedTensorDeleter;\n  dlm_tensor->dl_tensor.ctx = GetDlContext(h, status);\n  int ndim = tensor->dims();\n  dlm_tensor->dl_tensor.ndim = ndim;\n  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);\n  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);\n\n  std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;\n  std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;\n  shape_arr->resize(ndim);\n  stride_arr->resize(ndim, 1);\n  for (int i = 0; i < ndim; i++) {\n    (*shape_arr)[i] = tensor->dim_size(i);\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];\n  }\n\n  dlm_tensor->dl_tensor.shape = &(*shape_arr)[0];\n  // There are two ways to represent compact row-major data\n  // 1) nullptr indicates tensor is compact and row-majored.\n  // 2) fill in the strides array as the real case for compact row-major data.\n  // Here we choose option 2, since some frameworks didn't handle the strides\n  // argument properly.\n  dlm_tensor->dl_tensor.strides = &(*stride_arr)[0];\n  dlm_tensor->dl_tensor.byte_offset =\n      0;  // TF doesn't handle the strides and byte_offsets here\n  return static_cast<void*>(dlm_tensor);\n}",
    "abstract_func_before": "void* TFE_HandleToDLPack(TFE_TensorHandle* VAR_0, TF_Status* VAR_1) {\n  const Tensor* VAR_2 = GetTensorFromHandle(VAR_0, VAR_1);\n  TF_DataType VAR_3 = VAR_4<TF_DataType>(VAR_2->dtype());\n  TensorReference VAR_5(*VAR_2);  /* COMMENT_0 */\n\n  auto* VAR_6 = new TfDlManagedTensorCtx(VAR_5);\n  VAR_6->reference = VAR_5;\n\n  DLManagedTensor* VAR_7 = &VAR_6->tensor;\n  VAR_7->manager_ctx = VAR_6;\n  VAR_7->deleter = &VAR_8;\n  VAR_7->dl_tensor.ctx = GetDlContext(VAR_0, VAR_1);\n  int VAR_9 = VAR_2->dims();\n  VAR_7->dl_tensor.ndim = VAR_9;\n  VAR_7->dl_tensor.data = TFE_TensorHandleDevicePointer(VAR_0, VAR_1);\n  VAR_7->dl_tensor.dtype = GetDlDataType(VAR_3, VAR_1);\n\n  std::vector<int64_t>* VAR_10 = &VAR_6->shape;\n  std::vector<int64_t>* VAR_11 = &VAR_6->strides;\n  VAR_10->resize(VAR_9);\n  VAR_11->resize(VAR_9, 1);\n  for (int VAR_12 = 0; VAR_12 < VAR_9; VAR_12++) {\n    (*VAR_10)[VAR_12] = VAR_2->dim_size(VAR_12);\n  }\n  for (int VAR_12 = VAR_9 - 2; VAR_12 >= 0; --VAR_12) {\n    (*VAR_11)[VAR_12] = (*VAR_10)[VAR_12 + 1] * (*VAR_11)[VAR_12 + 1];\n  }\n\n  VAR_7->dl_tensor.shape = &(*VAR_10)[0];\n  /* COMMENT_1 */\n  /* COMMENT_2 */\n  /* COMMENT_3 */\n  /* COMMENT_4 */\n  /* COMMENT_5 */\n  VAR_7->dl_tensor.strides = &(*VAR_11)[0];\n  VAR_7->dl_tensor.byte_offset =\n      0;  /* COMMENT_6 */\n  return VAR_4<void*>(VAR_7);\n}",
    "func_graph_path_before": "tensorflow/22e07fb204386768e5bcbea563641ea11f96ceb8/dlpack.cc/vul/before/0.json",
    "func": "void* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {\n  auto tf_dlm_context = GetDlContext(h, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  auto* tf_dlm_data = TFE_TensorHandleDevicePointer(h, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  const Tensor* tensor = GetTensorFromHandle(h, status);\n  TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());\n\n  auto tf_dlm_type = GetDlDataType(data_type, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()\n  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);\n  tf_dlm_tensor_ctx->reference = tensor_ref;\n\n  DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;\n  dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;\n  dlm_tensor->deleter = &DLManagedTensorDeleter;\n  dlm_tensor->dl_tensor.ctx = tf_dlm_context;\n  int ndim = tensor->dims();\n  dlm_tensor->dl_tensor.ndim = ndim;\n  dlm_tensor->dl_tensor.data = tf_dlm_data;\n  dlm_tensor->dl_tensor.dtype = tf_dlm_type;\n\n  std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;\n  std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;\n  shape_arr->resize(ndim);\n  stride_arr->resize(ndim, 1);\n  for (int i = 0; i < ndim; i++) {\n    (*shape_arr)[i] = tensor->dim_size(i);\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];\n  }\n\n  dlm_tensor->dl_tensor.shape = shape_arr->data();\n  // There are two ways to represent compact row-major data\n  // 1) nullptr indicates tensor is compact and row-majored.\n  // 2) fill in the strides array as the real case for compact row-major data.\n  // Here we choose option 2, since some frameworks didn't handle the strides\n  // argument properly.\n  dlm_tensor->dl_tensor.strides = stride_arr->data();\n\n  dlm_tensor->dl_tensor.byte_offset =\n      0;  // TF doesn't handle the strides and byte_offsets here\n  return static_cast<void*>(dlm_tensor);\n}",
    "abstract_func": "void* TFE_HandleToDLPack(TFE_TensorHandle* VAR_0, TF_Status* VAR_1) {\n  auto VAR_2 = GetDlContext(VAR_0, VAR_1);\n  if (!VAR_1->status.ok()) {\n    return nullptr;\n  }\n\n  auto* VAR_3 = TFE_TensorHandleDevicePointer(VAR_0, VAR_1);\n  if (!VAR_1->status.ok()) {\n    return nullptr;\n  }\n\n  const Tensor* VAR_4 = GetTensorFromHandle(VAR_0, VAR_1);\n  TF_DataType VAR_5 = VAR_6<TF_DataType>(VAR_4->dtype());\n\n  auto VAR_7 = GetDlDataType(VAR_5, VAR_1);\n  if (!VAR_1->status.ok()) {\n    return nullptr;\n  }\n\n  TensorReference VAR_8(*VAR_4);  /* COMMENT_0 */\n  auto* VAR_9 = new TfDlManagedTensorCtx(VAR_8);\n  VAR_9->reference = VAR_8;\n\n  DLManagedTensor* VAR_10 = &VAR_9->tensor;\n  VAR_10->manager_ctx = VAR_9;\n  VAR_10->deleter = &VAR_11;\n  VAR_10->dl_tensor.ctx = VAR_2;\n  int VAR_12 = VAR_4->dims();\n  VAR_10->dl_tensor.ndim = VAR_12;\n  VAR_10->dl_tensor.data = VAR_3;\n  VAR_10->dl_tensor.dtype = VAR_7;\n\n  std::vector<int64_t>* VAR_13 = &VAR_9->shape;\n  std::vector<int64_t>* VAR_14 = &VAR_9->strides;\n  VAR_13->resize(VAR_12);\n  VAR_14->resize(VAR_12, 1);\n  for (int VAR_15 = 0; VAR_15 < VAR_12; VAR_15++) {\n    (*VAR_13)[VAR_15] = VAR_4->dim_size(VAR_15);\n  }\n  for (int VAR_15 = VAR_12 - 2; VAR_15 >= 0; --VAR_15) {\n    (*VAR_14)[VAR_15] = (*VAR_13)[VAR_15 + 1] * (*VAR_14)[VAR_15 + 1];\n  }\n\n  VAR_10->dl_tensor.shape = VAR_13->data();\n  /* COMMENT_1 */\n  /* COMMENT_2 */\n  /* COMMENT_3 */\n  /* COMMENT_4 */\n  /* COMMENT_5 */\n  VAR_10->dl_tensor.strides = VAR_14->data();\n\n  VAR_10->dl_tensor.byte_offset =\n      0;  /* COMMENT_6 */\n  return VAR_6<void*>(VAR_10);\n}",
    "func_graph_path": "tensorflow/22e07fb204386768e5bcbea563641ea11f96ceb8/dlpack.cc/vul/after/0.json",
    "diff_func": "--- func_before\n+++ func_after\n@@ -1,19 +1,34 @@\n void* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {\n+  auto tf_dlm_context = GetDlContext(h, status);\n+  if (!status->status.ok()) {\n+    return nullptr;\n+  }\n+\n+  auto* tf_dlm_data = TFE_TensorHandleDevicePointer(h, status);\n+  if (!status->status.ok()) {\n+    return nullptr;\n+  }\n+\n   const Tensor* tensor = GetTensorFromHandle(h, status);\n   TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());\n+\n+  auto tf_dlm_type = GetDlDataType(data_type, status);\n+  if (!status->status.ok()) {\n+    return nullptr;\n+  }\n+\n   TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()\n-\n   auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);\n   tf_dlm_tensor_ctx->reference = tensor_ref;\n \n   DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;\n   dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;\n   dlm_tensor->deleter = &DLManagedTensorDeleter;\n-  dlm_tensor->dl_tensor.ctx = GetDlContext(h, status);\n+  dlm_tensor->dl_tensor.ctx = tf_dlm_context;\n   int ndim = tensor->dims();\n   dlm_tensor->dl_tensor.ndim = ndim;\n-  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);\n-  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);\n+  dlm_tensor->dl_tensor.data = tf_dlm_data;\n+  dlm_tensor->dl_tensor.dtype = tf_dlm_type;\n \n   std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;\n   std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;\n@@ -26,13 +41,14 @@\n     (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];\n   }\n \n-  dlm_tensor->dl_tensor.shape = &(*shape_arr)[0];\n+  dlm_tensor->dl_tensor.shape = shape_arr->data();\n   // There are two ways to represent compact row-major data\n   // 1) nullptr indicates tensor is compact and row-majored.\n   // 2) fill in the strides array as the real case for compact row-major data.\n   // Here we choose option 2, since some frameworks didn't handle the strides\n   // argument properly.\n-  dlm_tensor->dl_tensor.strides = &(*stride_arr)[0];\n+  dlm_tensor->dl_tensor.strides = stride_arr->data();\n+\n   dlm_tensor->dl_tensor.byte_offset =\n       0;  // TF doesn't handle the strides and byte_offsets here\n   return static_cast<void*>(dlm_tensor);",
    "diff_line_info": {
        "deleted_lines": [
            "",
            "  dlm_tensor->dl_tensor.ctx = GetDlContext(h, status);",
            "  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);",
            "  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);",
            "  dlm_tensor->dl_tensor.shape = &(*shape_arr)[0];",
            "  dlm_tensor->dl_tensor.strides = &(*stride_arr)[0];"
        ],
        "added_lines": [
            "  auto tf_dlm_context = GetDlContext(h, status);",
            "  if (!status->status.ok()) {",
            "    return nullptr;",
            "  }",
            "",
            "  auto* tf_dlm_data = TFE_TensorHandleDevicePointer(h, status);",
            "  if (!status->status.ok()) {",
            "    return nullptr;",
            "  }",
            "",
            "",
            "  auto tf_dlm_type = GetDlDataType(data_type, status);",
            "  if (!status->status.ok()) {",
            "    return nullptr;",
            "  }",
            "",
            "  dlm_tensor->dl_tensor.ctx = tf_dlm_context;",
            "  dlm_tensor->dl_tensor.data = tf_dlm_data;",
            "  dlm_tensor->dl_tensor.dtype = tf_dlm_type;",
            "  dlm_tensor->dl_tensor.shape = shape_arr->data();",
            "  dlm_tensor->dl_tensor.strides = stride_arr->data();",
            ""
        ]
    },
    "is_vul": true,
    "pr_url": null,
    "description": "no more info"
}