{
    "cve_id": "CVE-2022-41898",
    "cwe_ids": [
        "CWE-20"
    ],
    "cvss_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H",
    "cvss_is_v3": true,
    "repo_name": "tensorflow",
    "commit_msg": "Fix empty input crash for SparseFillEmptyRowsGrad.\n\nPiperOrigin-RevId: 478085721",
    "commit_hash": "af4a6a3c8b95022c351edae94560acc61253a1b8",
    "git_url": "https://github.com/tensorflow/tensorflow/commit/af4a6a3c8b95022c351edae94560acc61253a1b8",
    "file_path": "tensorflow/core/kernels/sparse_fill_empty_rows_op_gpu.cu.cc",
    "func_name": "operator()",
    "func_before": "Status operator()(OpKernelContext* context, const Tensor& default_value_t,\n                    const Tensor& indices_t, const Tensor& values_t,\n                    const Tensor& dense_shape_t,\n                    typename AsyncOpKernel::DoneCallback done) {\n    const int kEmptyRowIndicatorOutput = 2;\n\n    const auto default_value = default_value_t.scalar<T>();\n    const auto indices = indices_t.matrix<Tindex>();\n    const auto values = values_t.vec<T>();\n    const auto dense_shape = dense_shape_t.vec<Tindex>();\n\n    const Tindex N = indices_t.shape().dim_size(0);\n    const int rank = indices_t.shape().dim_size(1);\n    const Tindex dense_rows = dense_shape(0);  // Must be on the host\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    se::Stream* stream = context->op_device_context()->stream();\n    if (!stream) return errors::Internal(\"No GPU stream available.\");\n\n    if (dense_rows == 0) {\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      TF_RETURN_IF_ERROR(AllocateOutputsExceptEmptyRowIndicator(\n          context, N, rank, /*num_empty_rows=*/0, &output_indices,\n          &output_values, &reverse_index_map));\n      if (context->output_required(kEmptyRowIndicatorOutput)) {\n        Tensor* unused = nullptr;\n        TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                    TensorShape({0}), &unused));\n      }\n      done();\n      return OkStatus();\n    }\n\n    // The algorithm as currently implemented is summarized as follows:\n    // 1) Compute elements_per_row (using GpuAtomicAdd).\n    // 2) Compute input_row_ends (the end index of each row) by computing the\n    //    prefix sum of elements_per_row.\n    // 3) Compute empty_row_indicator = (elements_per_row == 0).\n    // 4) Compute num_empty_rows_through (the no. empty rows up to and including\n    //    each row) by computing the prefix sum of empty_row_indicator.\n    // 5) Synchronize and allocate outputs (the sync is done implicitly by\n    //    enqueueing the remainder of the computation onto the stream as a host\n    //    callback).\n    // 6) If rows are not ordered:\n    //      Compute input_index_map by argsorting row indices.\n    // 7) Scatter input elements into output_indices and output_values using\n    //    input_index_map and num_empty_rows_through, leaving spaces for the\n    //    new values that will be inserted.\n    // 8) Scatter new default values into output_indices and output_values using\n    //    num_new_rows_through, input_row_ends, and empty_row_indicator.\n\n    // Summary of temporary allocations:\n    //   Tindex elements_per_row[dense_rows]\n    //   int rows_are_not_ordered[1]\n    //   Tindex row_indices[N]      (if rows_are_not_ordered)\n    //   Tindex input_index_map[N]  (if rows_are_not_ordered)\n    //   Tindex input_row_ends[dense_rows]\n    //   bool empty_row_indicator[dense_rows]\n    //   Tindex num_empty_rows_through[dense_rows]\n    //   Workspace for inclusive sums.\n    //   Workspace for radix sort.\n\n    Tensor elements_per_row_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &elements_per_row_t));\n    auto elements_per_row = elements_per_row_t.flat<Tindex>();\n    se::DeviceMemoryBase elements_per_row_gpu_memory(\n        elements_per_row.data(), dense_rows * sizeof(Tindex));\n    if (!stream\n             ->ThenMemZero(&elements_per_row_gpu_memory,\n                           dense_rows * sizeof(Tindex))\n             .ok()) {\n      return errors::Internal(\"Failed to zero elements_per_row\");\n    }\n    Tensor rows_are_not_ordered_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &rows_are_not_ordered_t));\n    auto rows_are_not_ordered_gpu = rows_are_not_ordered_t.flat<int>();\n    se::DeviceMemoryBase rows_are_not_ordered_gpu_memory(\n        rows_are_not_ordered_gpu.data(), sizeof(int));\n    if (!stream->ThenMemZero(&rows_are_not_ordered_gpu_memory, sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to zero rows_are_not_ordered\");\n    }\n    Tensor first_invalid_index_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &first_invalid_index_t));\n    auto first_invalid_index_gpu = first_invalid_index_t.flat<int>();\n    constexpr const int kAllIndicesValid = std::numeric_limits<int>::max();\n    se::DeviceMemoryBase first_invalid_index_gpu_memory(\n        first_invalid_index_gpu.data(), sizeof(int));\n    if (!stream\n             ->ThenMemset32(&first_invalid_index_gpu_memory, kAllIndicesValid,\n                            sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to initialize first_invalid_index\");\n    }\n\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          CountElementsPerRowKernel<Tindex>, /*device=*/device, /*size=*/N,\n          dense_rows, rank, indices, elements_per_row, rows_are_not_ordered_gpu,\n          first_invalid_index_gpu));\n    }\n\n    Tensor input_row_ends_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &input_row_ends_t));\n    auto input_row_ends = input_row_ends_t.flat<Tindex>();\n\n    TF_RETURN_IF_ERROR(GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                                             /*input=*/elements_per_row.data(),\n                                             /*output=*/input_row_ends.data()));\n\n    Tensor empty_row_indicator_t;\n    bool* empty_row_indicator;\n    if (context->output_required(kEmptyRowIndicatorOutput)) {\n      Tensor* empty_row_indicator_t_ptr = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                  TensorShape({dense_rows}),\n                                                  &empty_row_indicator_t_ptr));\n      empty_row_indicator = empty_row_indicator_t_ptr->vec<bool>().data();\n    } else {\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DT_BOOL, TensorShape({dense_rows}), &empty_row_indicator_t));\n      empty_row_indicator = empty_row_indicator_t.vec<bool>().data();\n    }\n\n    TF_RETURN_IF_ERROR(wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n                                        /*device=*/device, /*size=*/dense_rows,\n                                        elements_per_row, empty_row_indicator));\n\n    // For each row, the number of empty rows up to and including that row.\n    Tensor num_empty_rows_through_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &num_empty_rows_through_t));\n    auto num_empty_rows_through = num_empty_rows_through_t.flat<Tindex>();\n\n    gpuprim::TransformInputIterator<Tindex, CastFunctor<Tindex>, bool*>\n        empty_row_indicator_cast(empty_row_indicator, {});\n\n    // The inclusive sum in CUB does not work do the right thing if\n    // `empty_row_indicator` is passed in as a `bool *`.\n    TF_RETURN_IF_ERROR(\n        GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                              /*input=*/empty_row_indicator_cast,\n                              /*output=*/num_empty_rows_through.data()));\n\n    ScratchSpace<Tindex> num_empty_rows_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(num_empty_rows_host.mutable_data(),\n                          se::DeviceMemoryBase(\n                              num_empty_rows_through.data() + (dense_rows - 1),\n                              sizeof(*num_empty_rows_host.data())),\n                          sizeof(*num_empty_rows_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy num_empty_rows to host\");\n    }\n\n    ScratchSpace<int> rows_are_not_ordered_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(rows_are_not_ordered_host.mutable_data(),\n                          rows_are_not_ordered_gpu_memory,\n                          sizeof(*rows_are_not_ordered_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy rows_are_not_ordered to host\");\n    }\n\n    ScratchSpace<int> first_invalid_index_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(first_invalid_index_host.mutable_data(),\n                          first_invalid_index_gpu_memory,\n                          sizeof(*first_invalid_index_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy first_invalid_index to host\");\n    }\n\n    // We must wait for num_empty_rows and rows_are_not_ordered to be copied to\n    // the host, so we enqueue the remainder of the computation onto the stream\n    // asynchronously to avoid stalling execution.\n    auto async_finish_computation =\n        [this, context, kAllIndicesValid, index_type, N, rank, indices, values,\n         default_value, dense_rows, num_empty_rows_host,\n         rows_are_not_ordered_host, first_invalid_index_host,\n         num_empty_rows_through_t, num_empty_rows_through, input_row_ends_t,\n         input_row_ends, empty_row_indicator_t, empty_row_indicator,\n         done]() -> void {\n      DCHECK(done);  // Crash OK\n\n      // Ensure that within the callback, the proper GPU settings are\n      // configured.\n      auto stream = context->op_device_context()->stream();\n      ScopedActivateExecutorContext scoped_activation{stream->parent()};\n\n      int first_invalid_index = *first_invalid_index_host.data();\n      OP_REQUIRES_ASYNC(context, first_invalid_index == kAllIndicesValid,\n                        errors::InvalidArgument(\"indices(\", first_invalid_index,\n                                                \", 0) is invalid.\"),\n                        done);\n\n      Tindex num_empty_rows = *num_empty_rows_host.data();\n\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          AllocateOutputsExceptEmptyRowIndicator(\n              context, N, rank, num_empty_rows, &output_indices, &output_values,\n              &reverse_index_map),\n          done);\n\n      const GPUDevice& device = context->eigen_device<GPUDevice>();\n\n      Tindex* input_index_map = nullptr;\n      Tensor input_index_map_t;\n      int rows_are_not_ordered = *rows_are_not_ordered_host.data();\n      if (rows_are_not_ordered) {\n        OP_REQUIRES_OK_ASYNC(context,\n                             ArgSortByRows(context, device, N, rank, dense_rows,\n                                           indices, &input_index_map_t),\n                             done);\n        input_index_map = input_index_map_t.vec<Tindex>().data();\n      }\n\n      if (N > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterInputElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/N, dense_rows, rank,\n                             input_index_map, indices, values,\n                             num_empty_rows_through, output_indices,\n                             output_values, reverse_index_map),\n            done);\n      }\n\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n                           /*device=*/device, /*size=*/dense_rows, rank,\n                           default_value, num_empty_rows_through,\n                           input_row_ends, empty_row_indicator, output_indices,\n                           output_values),\n          done);\n\n      done();\n    };\n\n    context->device()\n        ->tensorflow_accelerator_device_info()\n        ->event_mgr->ThenExecute(stream, async_finish_computation);\n    return OkStatus();\n  }",
    "abstract_func_before": "Status operator()(OpKernelContext* VAR_0, const Tensor& VAR_1,\n                    const Tensor& VAR_2, const Tensor& VAR_3,\n                    const Tensor& VAR_4,\n                    typename AsyncOpKernel::DoneCallback VAR_5) {\n    const int VAR_6 = 2;\n\n    const auto VAR_7 = VAR_1.scalar<T>();\n    const auto VAR_8 = VAR_2.matrix<Tindex>();\n    const auto VAR_9 = VAR_3.vec<T>();\n    const auto VAR_10 = VAR_4.vec<Tindex>();\n\n    const Tindex VAR_11 = VAR_2.shape().dim_size(0);\n    const int VAR_12 = VAR_2.shape().dim_size(1);\n    const Tindex VAR_13 = VAR_10(0);  /* COMMENT_0 */\n    DataType VAR_14 = DataTypeToEnum<Tindex>::value;\n    const GPUDevice& VAR_15 = VAR_0->eigen_device<GPUDevice>();\n    se::Stream* VAR_16 = VAR_0->op_device_context()->stream();\n    if (!VAR_16) return errors::Internal(\"No GPU stream available.\");\n\n    if (VAR_13 == 0) {\n      Tindex* VAR_17;\n      T* VAR_18;\n      Tindex* VAR_19;\n      TF_RETURN_IF_ERROR(AllocateOutputsExceptEmptyRowIndicator(\n          VAR_0, VAR_11, VAR_12, /* COMMENT_1 */0, &VAR_17,\n          &VAR_18, &VAR_19));\n      if (VAR_0->output_required(VAR_6)) {\n        Tensor* VAR_20 = nullptr;\n        TF_RETURN_IF_ERROR(VAR_0->allocate_output(VAR_6,\n                                                    TensorShape({0}), &VAR_20));\n      }\n      VAR_5();\n      return OkStatus();\n    }\n\n    /* COMMENT_2 */\n    /* COMMENT_3 */\n    /* COMMENT_4 */\n    /* COMMENT_5 */\n    /* COMMENT_6 */\n    /* COMMENT_7 */\n    /* COMMENT_8 */\n    /* COMMENT_9 */\n    /* COMMENT_10 */\n    /* COMMENT_11 */\n    /* COMMENT_12 */\n    /* COMMENT_13 */\n    /* COMMENT_14 */\n    /* COMMENT_15 */\n    /* COMMENT_16 */\n    /* COMMENT_17 */\n    /* COMMENT_18 */\n\n    /* COMMENT_19 */\n    /* COMMENT_20 */\n    /* COMMENT_21 */\n    /* COMMENT_22 */\n    /* COMMENT_23 */\n    /* COMMENT_24 */\n    /* COMMENT_25 */\n    /* COMMENT_26 */\n    /* COMMENT_27 */\n    /* COMMENT_28 */\n\n    Tensor VAR_21;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n        VAR_14, TensorShape({VAR_13}), &VAR_21));\n    auto VAR_22 = VAR_21.flat<Tindex>();\n    se::DeviceMemoryBase VAR_23(\n        VAR_22.data(), VAR_13 * sizeof(Tindex));\n    if (!VAR_16\n             ->ThenMemZero(&VAR_23,\n                           VAR_13 * sizeof(Tindex))\n             .ok()) {\n      return errors::Internal(\"Failed to zero elements_per_row\");\n    }\n    Tensor VAR_24;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(VAR_25, TensorShape({1}),\n                                              &VAR_24));\n    auto VAR_26 = VAR_24.flat<int>();\n    se::DeviceMemoryBase VAR_27(\n        VAR_26.data(), sizeof(int));\n    if (!VAR_16->ThenMemZero(&VAR_27, sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to zero rows_are_not_ordered\");\n    }\n    Tensor VAR_28;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(VAR_25, TensorShape({1}),\n                                              &VAR_28));\n    auto VAR_29 = VAR_28.flat<int>();\n    constexpr const int VAR_30 = std::numeric_limits<int>::max();\n    se::DeviceMemoryBase VAR_31(\n        VAR_29.data(), sizeof(int));\n    if (!VAR_16\n             ->ThenMemset32(&VAR_31, VAR_30,\n                            sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to initialize first_invalid_index\");\n    }\n\n    if (VAR_11 > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          VAR_32<Tindex>, /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_11,\n          VAR_13, VAR_12, VAR_8, VAR_22, VAR_26,\n          VAR_29));\n    }\n\n    Tensor VAR_33;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n        VAR_14, TensorShape({VAR_13}), &VAR_33));\n    auto VAR_34 = VAR_33.flat<Tindex>();\n\n    TF_RETURN_IF_ERROR(GpuInclusivePrefixSum(VAR_0, /* COMMENT_30 */VAR_13,\n                                             /* COMMENT_31 */VAR_22.data(),\n                                             /* COMMENT_32 */VAR_34.data()));\n\n    Tensor VAR_35;\n    bool* VAR_36;\n    if (VAR_0->output_required(VAR_6)) {\n      Tensor* VAR_37 = nullptr;\n      TF_RETURN_IF_ERROR(VAR_0->allocate_output(VAR_6,\n                                                  TensorShape({VAR_13}),\n                                                  &VAR_37));\n      VAR_36 = VAR_37->vec<bool>().data();\n    } else {\n      TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n          VAR_38, TensorShape({VAR_13}), &VAR_35));\n      VAR_36 = VAR_35.vec<bool>().data();\n    }\n\n    TF_RETURN_IF_ERROR(wrap_kernel_call(VAR_39<Tindex>,\n                                        /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_13,\n                                        VAR_22, VAR_36));\n\n    /* COMMENT_33 */\n    Tensor VAR_40;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n        VAR_14, TensorShape({VAR_13}), &VAR_40));\n    auto VAR_41 = VAR_40.flat<Tindex>();\n\n    gpuprim::TransformInputIterator<Tindex, CastFunctor<Tindex>, bool*>\n        VAR_42(VAR_36, {});\n\n    /* COMMENT_34 */\n    /* COMMENT_35 */\n    TF_RETURN_IF_ERROR(\n        GpuInclusivePrefixSum(VAR_0, /* COMMENT_30 */VAR_13,\n                              /* COMMENT_31 */VAR_42,\n                              /* COMMENT_32 */VAR_41.data()));\n\n    ScratchSpace<Tindex> VAR_43(VAR_0, 1, /* COMMENT_36 */true);\n    if (!VAR_16\n             ->ThenMemcpy(VAR_43.mutable_data(),\n                          se::DeviceMemoryBase(\n                              VAR_41.data() + (VAR_13 - 1),\n                              sizeof(*VAR_43.data())),\n                          sizeof(*VAR_43.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy num_empty_rows to host\");\n    }\n\n    ScratchSpace<int> VAR_44(VAR_0, 1, /* COMMENT_36 */true);\n    if (!VAR_16\n             ->ThenMemcpy(VAR_44.mutable_data(),\n                          VAR_27,\n                          sizeof(*VAR_44.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy rows_are_not_ordered to host\");\n    }\n\n    ScratchSpace<int> VAR_45(VAR_0, 1, /* COMMENT_36 */true);\n    if (!VAR_16\n             ->ThenMemcpy(VAR_45.mutable_data(),\n                          VAR_31,\n                          sizeof(*VAR_45.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy first_invalid_index to host\");\n    }\n\n    /* COMMENT_37 */\n    /* COMMENT_38 */\n    /* COMMENT_39 */\n    auto VAR_46 =\n        [this, VAR_0, VAR_30, VAR_14, VAR_11, VAR_12, VAR_8, VAR_9,\n         VAR_7, VAR_13, VAR_43,\n         VAR_44, VAR_45,\n         VAR_40, VAR_41, VAR_33,\n         VAR_34, VAR_35, VAR_36,\n         VAR_5]() -> void {\n      DCHECK(VAR_5);  /* COMMENT_40 */\n\n      /* COMMENT_41 */\n      /* COMMENT_42 */\n      auto VAR_16 = VAR_0->op_device_context()->stream();\n      ScopedActivateExecutorContext VAR_47{VAR_16->parent()};\n\n      int VAR_48 = *VAR_45.data();\n      OP_REQUIRES_ASYNC(VAR_0, VAR_48 == VAR_30,\n                        errors::InvalidArgument(\"indices(\", VAR_48,\n                                                \", 0) is invalid.\"),\n                        VAR_5);\n\n      Tindex VAR_49 = *VAR_43.data();\n\n      Tindex* VAR_17;\n      T* VAR_18;\n      Tindex* VAR_19;\n      OP_REQUIRES_OK_ASYNC(\n          VAR_0,\n          AllocateOutputsExceptEmptyRowIndicator(\n              VAR_0, VAR_11, VAR_12, VAR_49, &VAR_17, &VAR_18,\n              &VAR_19),\n          VAR_5);\n\n      const GPUDevice& VAR_15 = VAR_0->eigen_device<GPUDevice>();\n\n      Tindex* VAR_50 = nullptr;\n      Tensor VAR_51;\n      int VAR_52 = *VAR_44.data();\n      if (VAR_52) {\n        OP_REQUIRES_OK_ASYNC(VAR_0,\n                             ArgSortByRows(VAR_0, VAR_15, VAR_11, VAR_12, VAR_13,\n                                           VAR_8, &VAR_51),\n                             VAR_5);\n        VAR_50 = VAR_51.vec<Tindex>().data();\n      }\n\n      if (VAR_11 > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            VAR_0,\n            wrap_kernel_call(VAR_53<T, Tindex>,\n                             /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_11, VAR_13, VAR_12,\n                             VAR_50, VAR_8, VAR_9,\n                             VAR_41, VAR_17,\n                             VAR_18, VAR_19),\n            VAR_5);\n      }\n\n      OP_REQUIRES_OK_ASYNC(\n          VAR_0,\n          wrap_kernel_call(VAR_54<T, Tindex>,\n                           /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_13, VAR_12,\n                           VAR_7, VAR_41,\n                           VAR_34, VAR_36, VAR_17,\n                           VAR_18),\n          VAR_5);\n\n      VAR_5();\n    };\n\n    VAR_0->device()\n        ->tensorflow_accelerator_device_info()\n        ->event_mgr->ThenExecute(VAR_16, VAR_46);\n    return OkStatus();\n  }",
    "func_graph_path_before": "tensorflow/af4a6a3c8b95022c351edae94560acc61253a1b8/sparse_fill_empty_rows_op_gpu.cu.cc/vul/before/1.json",
    "func": "Status operator()(OpKernelContext* context, const Tensor& default_value_t,\n                    const Tensor& indices_t, const Tensor& values_t,\n                    const Tensor& dense_shape_t,\n                    typename AsyncOpKernel::DoneCallback done) {\n    const int kEmptyRowIndicatorOutput = 2;\n\n    const auto default_value = default_value_t.scalar<T>();\n    const auto indices = indices_t.matrix<Tindex>();\n    const auto values = values_t.vec<T>();\n    const auto dense_shape = dense_shape_t.vec<Tindex>();\n\n    const Tindex N = indices_t.shape().dim_size(0);\n    const int rank = indices_t.shape().dim_size(1);\n    const Tindex dense_rows = dense_shape(0);  // Must be on the host\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    se::Stream* stream = context->op_device_context()->stream();\n    if (!stream) return errors::Internal(\"No GPU stream available.\");\n\n    if (dense_rows == 0) {\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      TF_RETURN_IF_ERROR(AllocateOutputsExceptEmptyRowIndicator(\n          context, N, rank, /*num_empty_rows=*/0, &output_indices,\n          &output_values, &reverse_index_map));\n      if (context->output_required(kEmptyRowIndicatorOutput)) {\n        Tensor* unused = nullptr;\n        TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                    TensorShape({0}), &unused));\n      }\n      done();\n      return OkStatus();\n    }\n\n    // The algorithm as currently implemented is summarized as follows:\n    // 1) Compute elements_per_row (using GpuAtomicAdd).\n    // 2) Compute input_row_ends (the end index of each row) by computing the\n    //    prefix sum of elements_per_row.\n    // 3) Compute empty_row_indicator = (elements_per_row == 0).\n    // 4) Compute num_empty_rows_through (the no. empty rows up to and including\n    //    each row) by computing the prefix sum of empty_row_indicator.\n    // 5) Synchronize and allocate outputs (the sync is done implicitly by\n    //    enqueueing the remainder of the computation onto the stream as a host\n    //    callback).\n    // 6) If rows are not ordered:\n    //      Compute input_index_map by argsorting row indices.\n    // 7) Scatter input elements into output_indices and output_values using\n    //    input_index_map and num_empty_rows_through, leaving spaces for the\n    //    new values that will be inserted.\n    // 8) Scatter new default values into output_indices and output_values using\n    //    num_new_rows_through, input_row_ends, and empty_row_indicator.\n\n    // Summary of temporary allocations:\n    //   Tindex elements_per_row[dense_rows]\n    //   int rows_are_not_ordered[1]\n    //   Tindex row_indices[N]      (if rows_are_not_ordered)\n    //   Tindex input_index_map[N]  (if rows_are_not_ordered)\n    //   Tindex input_row_ends[dense_rows]\n    //   bool empty_row_indicator[dense_rows]\n    //   Tindex num_empty_rows_through[dense_rows]\n    //   Workspace for inclusive sums.\n    //   Workspace for radix sort.\n\n    Tensor elements_per_row_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &elements_per_row_t));\n    auto elements_per_row = elements_per_row_t.flat<Tindex>();\n    se::DeviceMemoryBase elements_per_row_gpu_memory(\n        elements_per_row.data(), dense_rows * sizeof(Tindex));\n    if (!stream\n             ->ThenMemZero(&elements_per_row_gpu_memory,\n                           dense_rows * sizeof(Tindex))\n             .ok()) {\n      return errors::Internal(\"Failed to zero elements_per_row\");\n    }\n    Tensor rows_are_not_ordered_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &rows_are_not_ordered_t));\n    auto rows_are_not_ordered_gpu = rows_are_not_ordered_t.flat<int>();\n    se::DeviceMemoryBase rows_are_not_ordered_gpu_memory(\n        rows_are_not_ordered_gpu.data(), sizeof(int));\n    if (!stream->ThenMemZero(&rows_are_not_ordered_gpu_memory, sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to zero rows_are_not_ordered\");\n    }\n    Tensor first_invalid_index_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &first_invalid_index_t));\n    auto first_invalid_index_gpu = first_invalid_index_t.flat<int>();\n    constexpr const int kAllIndicesValid = std::numeric_limits<int>::max();\n    se::DeviceMemoryBase first_invalid_index_gpu_memory(\n        first_invalid_index_gpu.data(), sizeof(int));\n    if (!stream\n             ->ThenMemset32(&first_invalid_index_gpu_memory, kAllIndicesValid,\n                            sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to initialize first_invalid_index\");\n    }\n\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          CountElementsPerRowKernel<Tindex>, /*device=*/device, /*size=*/N,\n          dense_rows, rank, indices, elements_per_row, rows_are_not_ordered_gpu,\n          first_invalid_index_gpu));\n    }\n\n    Tensor input_row_ends_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &input_row_ends_t));\n    auto input_row_ends = input_row_ends_t.flat<Tindex>();\n\n    TF_RETURN_IF_ERROR(GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                                             /*input=*/elements_per_row.data(),\n                                             /*output=*/input_row_ends.data()));\n\n    Tensor empty_row_indicator_t;\n    bool* empty_row_indicator;\n    if (context->output_required(kEmptyRowIndicatorOutput)) {\n      Tensor* empty_row_indicator_t_ptr = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                  TensorShape({dense_rows}),\n                                                  &empty_row_indicator_t_ptr));\n      empty_row_indicator = empty_row_indicator_t_ptr->vec<bool>().data();\n    } else {\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DT_BOOL, TensorShape({dense_rows}), &empty_row_indicator_t));\n      empty_row_indicator = empty_row_indicator_t.vec<bool>().data();\n    }\n\n    if (dense_rows > 0) {\n      TF_RETURN_IF_ERROR(\n          wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n                           /*device=*/device, /*size=*/dense_rows,\n                           elements_per_row, empty_row_indicator));\n    }\n\n    // For each row, the number of empty rows up to and including that row.\n    Tensor num_empty_rows_through_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &num_empty_rows_through_t));\n    auto num_empty_rows_through = num_empty_rows_through_t.flat<Tindex>();\n\n    gpuprim::TransformInputIterator<Tindex, CastFunctor<Tindex>, bool*>\n        empty_row_indicator_cast(empty_row_indicator, {});\n\n    // The inclusive sum in CUB does not work do the right thing if\n    // `empty_row_indicator` is passed in as a `bool *`.\n    TF_RETURN_IF_ERROR(\n        GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                              /*input=*/empty_row_indicator_cast,\n                              /*output=*/num_empty_rows_through.data()));\n\n    ScratchSpace<Tindex> num_empty_rows_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(num_empty_rows_host.mutable_data(),\n                          se::DeviceMemoryBase(\n                              num_empty_rows_through.data() + (dense_rows - 1),\n                              sizeof(*num_empty_rows_host.data())),\n                          sizeof(*num_empty_rows_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy num_empty_rows to host\");\n    }\n\n    ScratchSpace<int> rows_are_not_ordered_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(rows_are_not_ordered_host.mutable_data(),\n                          rows_are_not_ordered_gpu_memory,\n                          sizeof(*rows_are_not_ordered_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy rows_are_not_ordered to host\");\n    }\n\n    ScratchSpace<int> first_invalid_index_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(first_invalid_index_host.mutable_data(),\n                          first_invalid_index_gpu_memory,\n                          sizeof(*first_invalid_index_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy first_invalid_index to host\");\n    }\n\n    // We must wait for num_empty_rows and rows_are_not_ordered to be copied to\n    // the host, so we enqueue the remainder of the computation onto the stream\n    // asynchronously to avoid stalling execution.\n    auto async_finish_computation =\n        [this, context, kAllIndicesValid, index_type, N, rank, indices, values,\n         default_value, dense_rows, num_empty_rows_host,\n         rows_are_not_ordered_host, first_invalid_index_host,\n         num_empty_rows_through_t, num_empty_rows_through, input_row_ends_t,\n         input_row_ends, empty_row_indicator_t, empty_row_indicator,\n         done]() -> void {\n      DCHECK(done);  // Crash OK\n\n      // Ensure that within the callback, the proper GPU settings are\n      // configured.\n      auto stream = context->op_device_context()->stream();\n      ScopedActivateExecutorContext scoped_activation{stream->parent()};\n\n      int first_invalid_index = *first_invalid_index_host.data();\n      OP_REQUIRES_ASYNC(context, first_invalid_index == kAllIndicesValid,\n                        errors::InvalidArgument(\"indices(\", first_invalid_index,\n                                                \", 0) is invalid.\"),\n                        done);\n\n      Tindex num_empty_rows = *num_empty_rows_host.data();\n\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          AllocateOutputsExceptEmptyRowIndicator(\n              context, N, rank, num_empty_rows, &output_indices, &output_values,\n              &reverse_index_map),\n          done);\n\n      const GPUDevice& device = context->eigen_device<GPUDevice>();\n\n      Tindex* input_index_map = nullptr;\n      Tensor input_index_map_t;\n      int rows_are_not_ordered = *rows_are_not_ordered_host.data();\n      if (rows_are_not_ordered) {\n        OP_REQUIRES_OK_ASYNC(context,\n                             ArgSortByRows(context, device, N, rank, dense_rows,\n                                           indices, &input_index_map_t),\n                             done);\n        input_index_map = input_index_map_t.vec<Tindex>().data();\n      }\n\n      if (N > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterInputElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/N, dense_rows, rank,\n                             input_index_map, indices, values,\n                             num_empty_rows_through, output_indices,\n                             output_values, reverse_index_map),\n            done);\n      }\n\n      if (dense_rows > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/dense_rows, rank,\n                             default_value, num_empty_rows_through,\n                             input_row_ends, empty_row_indicator,\n                             output_indices, output_values),\n            done);\n      }\n\n      done();\n    };\n\n    context->device()\n        ->tensorflow_accelerator_device_info()\n        ->event_mgr->ThenExecute(stream, async_finish_computation);\n    return OkStatus();\n  }",
    "abstract_func": "Status operator()(OpKernelContext* VAR_0, const Tensor& VAR_1,\n                    const Tensor& VAR_2, const Tensor& VAR_3,\n                    const Tensor& VAR_4,\n                    typename AsyncOpKernel::DoneCallback VAR_5) {\n    const int VAR_6 = 2;\n\n    const auto VAR_7 = VAR_1.scalar<T>();\n    const auto VAR_8 = VAR_2.matrix<Tindex>();\n    const auto VAR_9 = VAR_3.vec<T>();\n    const auto VAR_10 = VAR_4.vec<Tindex>();\n\n    const Tindex VAR_11 = VAR_2.shape().dim_size(0);\n    const int VAR_12 = VAR_2.shape().dim_size(1);\n    const Tindex VAR_13 = VAR_10(0);  /* COMMENT_0 */\n    DataType VAR_14 = DataTypeToEnum<Tindex>::value;\n    const GPUDevice& VAR_15 = VAR_0->eigen_device<GPUDevice>();\n    se::Stream* VAR_16 = VAR_0->op_device_context()->stream();\n    if (!VAR_16) return errors::Internal(\"No GPU stream available.\");\n\n    if (VAR_13 == 0) {\n      Tindex* VAR_17;\n      T* VAR_18;\n      Tindex* VAR_19;\n      TF_RETURN_IF_ERROR(AllocateOutputsExceptEmptyRowIndicator(\n          VAR_0, VAR_11, VAR_12, /* COMMENT_1 */0, &VAR_17,\n          &VAR_18, &VAR_19));\n      if (VAR_0->output_required(VAR_6)) {\n        Tensor* VAR_20 = nullptr;\n        TF_RETURN_IF_ERROR(VAR_0->allocate_output(VAR_6,\n                                                    TensorShape({0}), &VAR_20));\n      }\n      VAR_5();\n      return OkStatus();\n    }\n\n    /* COMMENT_2 */\n    /* COMMENT_3 */\n    /* COMMENT_4 */\n    /* COMMENT_5 */\n    /* COMMENT_6 */\n    /* COMMENT_7 */\n    /* COMMENT_8 */\n    /* COMMENT_9 */\n    /* COMMENT_10 */\n    /* COMMENT_11 */\n    /* COMMENT_12 */\n    /* COMMENT_13 */\n    /* COMMENT_14 */\n    /* COMMENT_15 */\n    /* COMMENT_16 */\n    /* COMMENT_17 */\n    /* COMMENT_18 */\n\n    /* COMMENT_19 */\n    /* COMMENT_20 */\n    /* COMMENT_21 */\n    /* COMMENT_22 */\n    /* COMMENT_23 */\n    /* COMMENT_24 */\n    /* COMMENT_25 */\n    /* COMMENT_26 */\n    /* COMMENT_27 */\n    /* COMMENT_28 */\n\n    Tensor VAR_21;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n        VAR_14, TensorShape({VAR_13}), &VAR_21));\n    auto VAR_22 = VAR_21.flat<Tindex>();\n    se::DeviceMemoryBase VAR_23(\n        VAR_22.data(), VAR_13 * sizeof(Tindex));\n    if (!VAR_16\n             ->ThenMemZero(&VAR_23,\n                           VAR_13 * sizeof(Tindex))\n             .ok()) {\n      return errors::Internal(\"Failed to zero elements_per_row\");\n    }\n    Tensor VAR_24;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(VAR_25, TensorShape({1}),\n                                              &VAR_24));\n    auto VAR_26 = VAR_24.flat<int>();\n    se::DeviceMemoryBase VAR_27(\n        VAR_26.data(), sizeof(int));\n    if (!VAR_16->ThenMemZero(&VAR_27, sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to zero rows_are_not_ordered\");\n    }\n    Tensor VAR_28;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(VAR_25, TensorShape({1}),\n                                              &VAR_28));\n    auto VAR_29 = VAR_28.flat<int>();\n    constexpr const int VAR_30 = std::numeric_limits<int>::max();\n    se::DeviceMemoryBase VAR_31(\n        VAR_29.data(), sizeof(int));\n    if (!VAR_16\n             ->ThenMemset32(&VAR_31, VAR_30,\n                            sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to initialize first_invalid_index\");\n    }\n\n    if (VAR_11 > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          VAR_32<Tindex>, /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_11,\n          VAR_13, VAR_12, VAR_8, VAR_22, VAR_26,\n          VAR_29));\n    }\n\n    Tensor VAR_33;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n        VAR_14, TensorShape({VAR_13}), &VAR_33));\n    auto VAR_34 = VAR_33.flat<Tindex>();\n\n    TF_RETURN_IF_ERROR(GpuInclusivePrefixSum(VAR_0, /* COMMENT_30 */VAR_13,\n                                             /* COMMENT_31 */VAR_22.data(),\n                                             /* COMMENT_32 */VAR_34.data()));\n\n    Tensor VAR_35;\n    bool* VAR_36;\n    if (VAR_0->output_required(VAR_6)) {\n      Tensor* VAR_37 = nullptr;\n      TF_RETURN_IF_ERROR(VAR_0->allocate_output(VAR_6,\n                                                  TensorShape({VAR_13}),\n                                                  &VAR_37));\n      VAR_36 = VAR_37->vec<bool>().data();\n    } else {\n      TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n          VAR_38, TensorShape({VAR_13}), &VAR_35));\n      VAR_36 = VAR_35.vec<bool>().data();\n    }\n\n    if (VAR_13 > 0) {\n      TF_RETURN_IF_ERROR(\n          wrap_kernel_call(VAR_39<Tindex>,\n                           /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_13,\n                           VAR_22, VAR_36));\n    }\n\n    /* COMMENT_33 */\n    Tensor VAR_40;\n    TF_RETURN_IF_ERROR(VAR_0->allocate_temp(\n        VAR_14, TensorShape({VAR_13}), &VAR_40));\n    auto VAR_41 = VAR_40.flat<Tindex>();\n\n    gpuprim::TransformInputIterator<Tindex, CastFunctor<Tindex>, bool*>\n        VAR_42(VAR_36, {});\n\n    /* COMMENT_34 */\n    /* COMMENT_35 */\n    TF_RETURN_IF_ERROR(\n        GpuInclusivePrefixSum(VAR_0, /* COMMENT_30 */VAR_13,\n                              /* COMMENT_31 */VAR_42,\n                              /* COMMENT_32 */VAR_41.data()));\n\n    ScratchSpace<Tindex> VAR_43(VAR_0, 1, /* COMMENT_36 */true);\n    if (!VAR_16\n             ->ThenMemcpy(VAR_43.mutable_data(),\n                          se::DeviceMemoryBase(\n                              VAR_41.data() + (VAR_13 - 1),\n                              sizeof(*VAR_43.data())),\n                          sizeof(*VAR_43.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy num_empty_rows to host\");\n    }\n\n    ScratchSpace<int> VAR_44(VAR_0, 1, /* COMMENT_36 */true);\n    if (!VAR_16\n             ->ThenMemcpy(VAR_44.mutable_data(),\n                          VAR_27,\n                          sizeof(*VAR_44.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy rows_are_not_ordered to host\");\n    }\n\n    ScratchSpace<int> VAR_45(VAR_0, 1, /* COMMENT_36 */true);\n    if (!VAR_16\n             ->ThenMemcpy(VAR_45.mutable_data(),\n                          VAR_31,\n                          sizeof(*VAR_45.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy first_invalid_index to host\");\n    }\n\n    /* COMMENT_37 */\n    /* COMMENT_38 */\n    /* COMMENT_39 */\n    auto VAR_46 =\n        [this, VAR_0, VAR_30, VAR_14, VAR_11, VAR_12, VAR_8, VAR_9,\n         VAR_7, VAR_13, VAR_43,\n         VAR_44, VAR_45,\n         VAR_40, VAR_41, VAR_33,\n         VAR_34, VAR_35, VAR_36,\n         VAR_5]() -> void {\n      DCHECK(VAR_5);  /* COMMENT_40 */\n\n      /* COMMENT_41 */\n      /* COMMENT_42 */\n      auto VAR_16 = VAR_0->op_device_context()->stream();\n      ScopedActivateExecutorContext VAR_47{VAR_16->parent()};\n\n      int VAR_48 = *VAR_45.data();\n      OP_REQUIRES_ASYNC(VAR_0, VAR_48 == VAR_30,\n                        errors::InvalidArgument(\"indices(\", VAR_48,\n                                                \", 0) is invalid.\"),\n                        VAR_5);\n\n      Tindex VAR_49 = *VAR_43.data();\n\n      Tindex* VAR_17;\n      T* VAR_18;\n      Tindex* VAR_19;\n      OP_REQUIRES_OK_ASYNC(\n          VAR_0,\n          AllocateOutputsExceptEmptyRowIndicator(\n              VAR_0, VAR_11, VAR_12, VAR_49, &VAR_17, &VAR_18,\n              &VAR_19),\n          VAR_5);\n\n      const GPUDevice& VAR_15 = VAR_0->eigen_device<GPUDevice>();\n\n      Tindex* VAR_50 = nullptr;\n      Tensor VAR_51;\n      int VAR_52 = *VAR_44.data();\n      if (VAR_52) {\n        OP_REQUIRES_OK_ASYNC(VAR_0,\n                             ArgSortByRows(VAR_0, VAR_15, VAR_11, VAR_12, VAR_13,\n                                           VAR_8, &VAR_51),\n                             VAR_5);\n        VAR_50 = VAR_51.vec<Tindex>().data();\n      }\n\n      if (VAR_11 > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            VAR_0,\n            wrap_kernel_call(VAR_53<T, Tindex>,\n                             /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_11, VAR_13, VAR_12,\n                             VAR_50, VAR_8, VAR_9,\n                             VAR_41, VAR_17,\n                             VAR_18, VAR_19),\n            VAR_5);\n      }\n\n      if (VAR_13 > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            VAR_0,\n            wrap_kernel_call(VAR_54<T, Tindex>,\n                             /* COMMENT_29 */VAR_15, /* COMMENT_30 */VAR_13, VAR_12,\n                             VAR_7, VAR_41,\n                             VAR_34, VAR_36,\n                             VAR_17, VAR_18),\n            VAR_5);\n      }\n\n      VAR_5();\n    };\n\n    VAR_0->device()\n        ->tensorflow_accelerator_device_info()\n        ->event_mgr->ThenExecute(VAR_16, VAR_46);\n    return OkStatus();\n  }",
    "func_graph_path": "tensorflow/af4a6a3c8b95022c351edae94560acc61253a1b8/sparse_fill_empty_rows_op_gpu.cu.cc/vul/after/1.json",
    "diff_func": "--- func_before\n+++ func_after\n@@ -128,9 +128,12 @@\n       empty_row_indicator = empty_row_indicator_t.vec<bool>().data();\n     }\n \n-    TF_RETURN_IF_ERROR(wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n-                                        /*device=*/device, /*size=*/dense_rows,\n-                                        elements_per_row, empty_row_indicator));\n+    if (dense_rows > 0) {\n+      TF_RETURN_IF_ERROR(\n+          wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n+                           /*device=*/device, /*size=*/dense_rows,\n+                           elements_per_row, empty_row_indicator));\n+    }\n \n     // For each row, the number of empty rows up to and including that row.\n     Tensor num_empty_rows_through_t;\n@@ -236,14 +239,16 @@\n             done);\n       }\n \n-      OP_REQUIRES_OK_ASYNC(\n-          context,\n-          wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n-                           /*device=*/device, /*size=*/dense_rows, rank,\n-                           default_value, num_empty_rows_through,\n-                           input_row_ends, empty_row_indicator, output_indices,\n-                           output_values),\n-          done);\n+      if (dense_rows > 0) {\n+        OP_REQUIRES_OK_ASYNC(\n+            context,\n+            wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n+                             /*device=*/device, /*size=*/dense_rows, rank,\n+                             default_value, num_empty_rows_through,\n+                             input_row_ends, empty_row_indicator,\n+                             output_indices, output_values),\n+            done);\n+      }\n \n       done();\n     };",
    "diff_line_info": {
        "deleted_lines": [
            "    TF_RETURN_IF_ERROR(wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,",
            "                                        /*device=*/device, /*size=*/dense_rows,",
            "                                        elements_per_row, empty_row_indicator));",
            "      OP_REQUIRES_OK_ASYNC(",
            "          context,",
            "          wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,",
            "                           /*device=*/device, /*size=*/dense_rows, rank,",
            "                           default_value, num_empty_rows_through,",
            "                           input_row_ends, empty_row_indicator, output_indices,",
            "                           output_values),",
            "          done);"
        ],
        "added_lines": [
            "    if (dense_rows > 0) {",
            "      TF_RETURN_IF_ERROR(",
            "          wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,",
            "                           /*device=*/device, /*size=*/dense_rows,",
            "                           elements_per_row, empty_row_indicator));",
            "    }",
            "      if (dense_rows > 0) {",
            "        OP_REQUIRES_OK_ASYNC(",
            "            context,",
            "            wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,",
            "                             /*device=*/device, /*size=*/dense_rows, rank,",
            "                             default_value, num_empty_rows_through,",
            "                             input_row_ends, empty_row_indicator,",
            "                             output_indices, output_values),",
            "            done);",
            "      }"
        ]
    },
    "is_vul": true,
    "pr_url": null,
    "description": "no more info"
}