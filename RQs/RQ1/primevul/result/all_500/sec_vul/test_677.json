{
  "id": 677,
  "language": "cc",
  "cwe": "",
  "commit_url": "https://github.com/prividentity/tensorflow/commit/bc9c546ce7015c57c2f15c168b3d9201de679a1d",
  "commit_sha": "bc9c546ce7015c57c2f15c168b3d9201de679a1d",
  "commit_msg": "Prevent heap oob access in `resource_variable_ops.cc`\n\nPiperOrigin-RevId: 387936433\nChange-Id: I9e71ddaa8dbd51ec6afbf163a6b3b591f193b4f6",
  "pr_url": null,
  "pr_info": null,
  "file_name": "tensorflow/core/kernels/resource_variable_ops.cc",
  "func_name": "",
  "raw_func_from_json": "  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    // NOTE: We hold the lock for the whole gather operation instead\n    // of increasing the reference count of v->tensor() to avoid a\n    // situation where a write to the same variable will see a\n    // reference count greater than one and make a copy of the\n    // (potentially very large) tensor buffer.\n    tf_shared_lock ml(*v->mu());\n    const Tensor& params = *v->tensor();\n    const Tensor& indices = c->input(1);\n    OP_REQUIRES(\n        c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n        errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n\n    // Check that we have enough index space\n    const int64_t N = indices.NumElements();\n    OP_REQUIRES(\n        c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[0] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", params.dim_size(0), \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    // The result shape is params.shape[:batch_dims] +\n    // indices.shape[batch_dims:] + params.shape[batch_dims+1:].\n    TensorShape result_shape;\n    for (int i = 0; i < batch_dims_; ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n    for (int i = batch_dims_; i < indices.dims(); ++i) {\n      result_shape.AddDim(indices.dim_size(i));\n    }\n    for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n\n    Tensor* out = nullptr;\n    Tensor tmp;\n    if (params.dtype() == DT_VARIANT) {\n      tmp = Tensor(DT_VARIANT, result_shape);\n      c->set_output(0, tmp);\n      out = &tmp;\n    } else {\n      OP_REQUIRES_OK(c, c->allocate_output(0, result_shape, &out));\n    }\n\n    if (N > 0) {\n      Tensor tmp_indices;\n\n      // Points to the original or updated (if batch_dims is set) indices.\n      const Tensor* op_indices = &indices;\n      if (batch_dims_ > 0) {\n        OP_REQUIRES_OK(c, c->allocate_temp(indices.dtype(), indices.shape(),\n                                           &tmp_indices));\n        functor::DenseUpdate<Device, Index, ASSIGN> copy_functor;\n        copy_functor(c->eigen_device<Device>(), tmp_indices.flat<Index>(),\n                     indices.flat<Index>());\n\n        AddBatchOffsets(&tmp_indices, params);\n        op_indices = &tmp_indices;\n      }\n\n      int64_t gather_dim_size = 1;\n      for (int idx = 0; idx <= batch_dims_; ++idx) {\n        gather_dim_size *= params.dim_size(idx);\n      }\n      int64_t inner_size = 1;\n      for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n        inner_size *= params.dim_size(i);\n      }\n      auto params_flat = params.shaped<T, 3>({1, gather_dim_size, inner_size});\n      const auto indices_flat = op_indices->flat<Index>();\n      auto out_flat = out->shaped<T, 3>({1, N, out->NumElements() / N});\n\n      functor::GatherFunctor<Device, T, Index> functor;\n      int64_t bad_i = functor(c, params_flat, indices_flat, out_flat);\n\n      OP_REQUIRES(\n          c, bad_i < 0,\n          errors::InvalidArgument(\n              \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n              indices_flat(bad_i), \" is not in [0, \", params.dim_size(0), \")\"));\n    }\n  }",
  "diff_func": "@@ -660,6 +660,11 @@ class ResourceGatherOp : public OpKernel {\n     OP_REQUIRES(\n         c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n         errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n+    OP_REQUIRES(\n+        c, params.shape().dims() >= batch_dims_,\n+        errors::InvalidArgument(\"params must have at least \", batch_dims_,\n+                                \" (batch_dims) dimensions but it has shape \",\n+                                params.shape().DebugString()));\n \n     // Check that we have enough index space\n     const int64_t N = indices.NumElements();",
  "func": "  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    // NOTE: We hold the lock for the whole gather operation instead\n    // of increasing the reference count of v->tensor() to avoid a\n    // situation where a write to the same variable will see a\n    // reference count greater than one and make a copy of the\n    // (potentially very large) tensor buffer.\n    tf_shared_lock ml(*v->mu());\n    const Tensor& params = *v->tensor();\n    const Tensor& indices = c->input(1);\n    OP_REQUIRES(\n        c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n        errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n\n    // Check that we have enough index space\n    const int64_t N = indices.NumElements();\n    OP_REQUIRES(\n        c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[0] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", params.dim_size(0), \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    // The result shape is params.shape[:batch_dims] +\n    // indices.shape[batch_dims:] + params.shape[batch_dims+1:].\n    TensorShape result_shape;\n    for (int i = 0; i < batch_dims_; ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n    for (int i = batch_dims_; i < indices.dims(); ++i) {\n      result_shape.AddDim(indices.dim_size(i));\n    }\n    for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n\n    Tensor* out = nullptr;\n    Tensor tmp;\n    if (params.dtype() == DT_VARIANT) {\n      tmp = Tensor(DT_VARIANT, result_shape);\n      c->set_output(0, tmp);\n      out = &tmp;\n    } else {\n      OP_REQUIRES_OK(c, c->allocate_output(0, result_shape, &out));\n    }\n\n    if (N > 0) {\n      Tensor tmp_indices;\n\n      // Points to the original or updated (if batch_dims is set) indices.\n      const Tensor* op_indices = &indices;\n      if (batch_dims_ > 0) {\n        OP_REQUIRES_OK(c, c->allocate_temp(indices.dtype(), indices.shape(),\n                                           &tmp_indices));\n        functor::DenseUpdate<Device, Index, ASSIGN> copy_functor;\n        copy_functor(c->eigen_device<Device>(), tmp_indices.flat<Index>(),\n                     indices.flat<Index>());\n\n        AddBatchOffsets(&tmp_indices, params);\n        op_indices = &tmp_indices;\n      }\n\n      int64_t gather_dim_size = 1;\n      for (int idx = 0; idx <= batch_dims_; ++idx) {\n        gather_dim_size *= params.dim_size(idx);\n      }\n      int64_t inner_size = 1;\n      for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n        inner_size *= params.dim_size(i);\n      }\n      auto params_flat = params.shaped<T, 3>({1, gather_dim_size, inner_size});\n      const auto indices_flat = op_indices->flat<Index>();\n      auto out_flat = out->shaped<T, 3>({1, N, out->NumElements() / N});\n\n      functor::GatherFunctor<Device, T, Index> functor;\n      int64_t bad_i = functor(c, params_flat, indices_flat, out_flat);\n\n      OP_REQUIRES(\n          c, bad_i < 0,\n          errors::InvalidArgument(\n              \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n              indices_flat(bad_i), \" is not in [0, \", params.dim_size(0), \")\"));\n    }\n  }",
  "project": "tensorflow",
  "hash": 294361236653337986849576392701719119932,
  "size": 86,
  "commit_id": "bc9c546ce7015c57c2f15c168b3d9201de679a1d",
  "message": "Prevent heap oob access in `resource_variable_ops.cc`\n\nPiperOrigin-RevId: 387936433\nChange-Id: I9e71ddaa8dbd51ec6afbf163a6b3b591f193b4f6",
  "target": 1,
  "dataset": "other",
  "idx": 197110,
  "Bug Filter": "** Security Vulnerability Fix  \n**Confidence Score:** 0.95",
  "Bug Filter Confidence": 0.95,
  "Bug Filter Response": "\n\nThe patch addresses a heap out-of-bounds access vulnerability in TensorFlow's resource variable operations. It adds checks to ensure tensor dimensions are valid, preventing potential memory corruption.\n\n**Final Classification:** Security Vulnerability Fix  \n**Confidence Score:** 0.95"
}