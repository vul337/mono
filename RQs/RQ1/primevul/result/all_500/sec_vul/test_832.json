{
  "id": 832,
  "language": "h",
  "cwe": "",
  "commit_url": "https://github.com/HKL-Kevin/msm8998-mainline/commit/2a8859f373b0a86f0ece8ec8312607eacf12485d",
  "commit_sha": "2a8859f373b0a86f0ece8ec8312607eacf12485d",
  "commit_msg": "KVM: x86/mmu: do compare-and-exchange of gPTE via the user address\n\nFNAME(cmpxchg_gpte) is an inefficient mess.  It is at least decent if it\ncan go through get_user_pages_fast(), but if it cannot then it tries to\nuse memremap(); that is not just terribly slow, it is also wrong because\nit assumes that the VM_PFNMAP VMA is contiguous.\n\nThe right way to do it would be to do the same thing as\nhva_to_pfn_remapped() does since commit add6a0cd1c5b (\"KVM: MMU: try to\nfix up page faults before giving up\", 2016-07-05), using follow_pte()\nand fixup_user_fault() to determine the correct address to use for\nmemremap().  To do this, one could for example extract hva_to_pfn()\nfor use outside virt/kvm/kvm_main.c.  But really there is no reason to\ndo that either, because there is already a perfectly valid address to\ndo the cmpxchg() on, only it is a userspace address.  That means doing\nuser_access_begin()/user_access_end() and writing the code in assembly\nto handle exceptions correctly.  Worse, the guest PTE can be 8-byte\neven on i686 so there is the extra complication of using cmpxchg8b to\naccount for.  But at least it is an efficient mess.\n\n(Thanks to Linus for suggesting improvement on the inline assembly).\n\nReported-by: Qiuhao Li <qiuhao@sysec.org>\nReported-by: Gaoning Pan <pgn@zju.edu.cn>\nReported-by: Yongkang Jia <kangel@zju.edu.cn>\nReported-by: syzbot+6cde2282daa792c49ab8@syzkaller.appspotmail.com\nDebugged-by: Tadeusz Struk <tadeusz.struk@linaro.org>\nTested-by: Maxim Levitsky <mlevitsk@redhat.com>\nCc: stable@vger.kernel.org\nFixes: bd53cb35a3e9 (\"X86/KVM: Handle PFNs outside of kernel reach when touching GPTEs\")\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>",
  "pr_url": null,
  "pr_info": null,
  "file_name": "arch/x86/kvm/mmu/paging_tmpl.h",
  "func_name": "",
  "raw_func_from_json": "static int FNAME(cmpxchg_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t       pt_element_t __user *ptep_user, unsigned index,\n\t\t\t       pt_element_t orig_pte, pt_element_t new_pte)\n{\n\tint npages;\n\tpt_element_t ret;\n\tpt_element_t *table;\n\tstruct page *page;\n\n\tnpages = get_user_pages_fast((unsigned long)ptep_user, 1, FOLL_WRITE, &page);\n\tif (likely(npages == 1)) {\n\t\ttable = kmap_atomic(page);\n\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\t\tkunmap_atomic(table);\n\n\t\tkvm_release_page_dirty(page);\n\t} else {\n\t\tstruct vm_area_struct *vma;\n\t\tunsigned long vaddr = (unsigned long)ptep_user & PAGE_MASK;\n\t\tunsigned long pfn;\n\t\tunsigned long paddr;\n\n\t\tmmap_read_lock(current->mm);\n\t\tvma = find_vma_intersection(current->mm, vaddr, vaddr + PAGE_SIZE);\n\t\tif (!vma || !(vma->vm_flags & VM_PFNMAP)) {\n\t\t\tmmap_read_unlock(current->mm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tpfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n\t\tpaddr = pfn << PAGE_SHIFT;\n\t\ttable = memremap(paddr, PAGE_SIZE, MEMREMAP_WB);\n\t\tif (!table) {\n\t\t\tmmap_read_unlock(current->mm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\t\tmemunmap(table);\n\t\tmmap_read_unlock(current->mm);\n\t}\n\n\treturn (ret != orig_pte);\n}",
  "diff_func": "@@ -34,9 +34,8 @@\n \t#define PT_HAVE_ACCESSED_DIRTY(mmu) true\n \t#ifdef CONFIG_X86_64\n \t#define PT_MAX_FULL_LEVELS PT64_ROOT_MAX_LEVEL\n-\t#define CMPXCHG cmpxchg\n+\t#define CMPXCHG \"cmpxchgq\"\n \t#else\n-\t#define CMPXCHG cmpxchg64\n \t#define PT_MAX_FULL_LEVELS 2\n \t#endif\n #elif PTTYPE == 32\n@@ -52,7 +51,7 @@\n \t#define PT_GUEST_DIRTY_SHIFT PT_DIRTY_SHIFT\n \t#define PT_GUEST_ACCESSED_SHIFT PT_ACCESSED_SHIFT\n \t#define PT_HAVE_ACCESSED_DIRTY(mmu) true\n-\t#define CMPXCHG cmpxchg\n+\t#define CMPXCHG \"cmpxchgl\"\n #elif PTTYPE == PTTYPE_EPT\n \t#define pt_element_t u64\n \t#define guest_walker guest_walkerEPT\n@@ -65,7 +64,9 @@\n \t#define PT_GUEST_DIRTY_SHIFT 9\n \t#define PT_GUEST_ACCESSED_SHIFT 8\n \t#define PT_HAVE_ACCESSED_DIRTY(mmu) ((mmu)->ept_ad)\n-\t#define CMPXCHG cmpxchg64\n+\t#ifdef CONFIG_X86_64\n+\t#define CMPXCHG \"cmpxchgq\"\n+\t#endif\n \t#define PT_MAX_FULL_LEVELS PT64_ROOT_MAX_LEVEL\n #else\n \t#error Invalid PTTYPE value\n@@ -147,43 +148,36 @@ static int FNAME(cmpxchg_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n \t\t\t       pt_element_t __user *ptep_user, unsigned index,\n \t\t\t       pt_element_t orig_pte, pt_element_t new_pte)\n {\n-\tint npages;\n-\tpt_element_t ret;\n-\tpt_element_t *table;\n-\tstruct page *page;\n-\n-\tnpages = get_user_pages_fast((unsigned long)ptep_user, 1, FOLL_WRITE, &page);\n-\tif (likely(npages == 1)) {\n-\t\ttable = kmap_atomic(page);\n-\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n-\t\tkunmap_atomic(table);\n-\n-\t\tkvm_release_page_dirty(page);\n-\t} else {\n-\t\tstruct vm_area_struct *vma;\n-\t\tunsigned long vaddr = (unsigned long)ptep_user & PAGE_MASK;\n-\t\tunsigned long pfn;\n-\t\tunsigned long paddr;\n-\n-\t\tmmap_read_lock(current->mm);\n-\t\tvma = find_vma_intersection(current->mm, vaddr, vaddr + PAGE_SIZE);\n-\t\tif (!vma || !(vma->vm_flags & VM_PFNMAP)) {\n-\t\t\tmmap_read_unlock(current->mm);\n-\t\t\treturn -EFAULT;\n-\t\t}\n-\t\tpfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n-\t\tpaddr = pfn << PAGE_SHIFT;\n-\t\ttable = memremap(paddr, PAGE_SIZE, MEMREMAP_WB);\n-\t\tif (!table) {\n-\t\t\tmmap_read_unlock(current->mm);\n-\t\t\treturn -EFAULT;\n-\t\t}\n-\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n-\t\tmemunmap(table);\n-\t\tmmap_read_unlock(current->mm);\n-\t}\n+\tsigned char r;\n \n-\treturn (ret != orig_pte);\n+\tif (!user_access_begin(ptep_user, sizeof(pt_element_t)))\n+\t\treturn -EFAULT;\n+\n+#ifdef CMPXCHG\n+\tasm volatile(\"1:\" LOCK_PREFIX CMPXCHG \" %[new], %[ptr]\\n\"\n+\t\t     \"setnz %b[r]\\n\"\n+\t\t     \"2:\"\n+\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])\n+\t\t     : [ptr] \"+m\" (*ptep_user),\n+\t\t       [old] \"+a\" (orig_pte),\n+\t\t       [r] \"=q\" (r)\n+\t\t     : [new] \"r\" (new_pte)\n+\t\t     : \"memory\");\n+#else\n+\tasm volatile(\"1:\" LOCK_PREFIX \"cmpxchg8b %[ptr]\\n\"\n+\t\t     \"setnz %b[r]\\n\"\n+\t\t     \"2:\"\n+\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])\n+\t\t     : [ptr] \"+m\" (*ptep_user),\n+\t\t       [old] \"+A\" (orig_pte),\n+\t\t       [r] \"=q\" (r)\n+\t\t     : [new_lo] \"b\" ((u32)new_pte),\n+\t\t       [new_hi] \"c\" ((u32)(new_pte >> 32))\n+\t\t     : \"memory\");\n+#endif\n+\n+\tuser_access_end();\n+\treturn r;\n }\n \n static bool FNAME(prefetch_invalid_gpte)(struct kvm_vcpu *vcpu,",
  "func": "static int FNAME(cmpxchg_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t       pt_element_t __user *ptep_user, unsigned index,\n\t\t\t       pt_element_t orig_pte, pt_element_t new_pte)\n{\n\tint npages;\n\tpt_element_t ret;\n\tpt_element_t *table;\n\tstruct page *page;\n\n\tnpages = get_user_pages_fast((unsigned long)ptep_user, 1, FOLL_WRITE, &page);\n\tif (likely(npages == 1)) {\n\t\ttable = kmap_atomic(page);\n\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\t\tkunmap_atomic(table);\n\n\t\tkvm_release_page_dirty(page);\n\t} else {\n\t\tstruct vm_area_struct *vma;\n\t\tunsigned long vaddr = (unsigned long)ptep_user & PAGE_MASK;\n\t\tunsigned long pfn;\n\t\tunsigned long paddr;\n\n\t\tmmap_read_lock(current->mm);\n\t\tvma = find_vma_intersection(current->mm, vaddr, vaddr + PAGE_SIZE);\n\t\tif (!vma || !(vma->vm_flags & VM_PFNMAP)) {\n\t\t\tmmap_read_unlock(current->mm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tpfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n\t\tpaddr = pfn << PAGE_SHIFT;\n\t\ttable = memremap(paddr, PAGE_SIZE, MEMREMAP_WB);\n\t\tif (!table) {\n\t\t\tmmap_read_unlock(current->mm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\t\tmemunmap(table);\n\t\tmmap_read_unlock(current->mm);\n\t}\n\n\treturn (ret != orig_pte);\n}",
  "project": "linux",
  "hash": 242802409117904906138634378200961998106,
  "size": 42,
  "commit_id": "2a8859f373b0a86f0ece8ec8312607eacf12485d",
  "message": "KVM: x86/mmu: do compare-and-exchange of gPTE via the user address\n\nFNAME(cmpxchg_gpte) is an inefficient mess.  It is at least decent if it\ncan go through get_user_pages_fast(), but if it cannot then it tries to\nuse memremap(); that is not just terribly slow, it is also wrong because\nit assumes that the VM_PFNMAP VMA is contiguous.\n\nThe right way to do it would be to do the same thing as\nhva_to_pfn_remapped() does since commit add6a0cd1c5b (\"KVM: MMU: try to\nfix up page faults before giving up\", 2016-07-05), using follow_pte()\nand fixup_user_fault() to determine the correct address to use for\nmemremap().  To do this, one could for example extract hva_to_pfn()\nfor use outside virt/kvm/kvm_main.c.  But really there is no reason to\ndo that either, because there is already a perfectly valid address to\ndo the cmpxchg() on, only it is a userspace address.  That means doing\nuser_access_begin()/user_access_end() and writing the code in assembly\nto handle exceptions correctly.  Worse, the guest PTE can be 8-byte\neven on i686 so there is the extra complication of using cmpxchg8b to\naccount for.  But at least it is an efficient mess.\n\n(Thanks to Linus for suggesting improvement on the inline assembly).\n\nReported-by: Qiuhao Li <qiuhao@sysec.org>\nReported-by: Gaoning Pan <pgn@zju.edu.cn>\nReported-by: Yongkang Jia <kangel@zju.edu.cn>\nReported-by: syzbot+6cde2282daa792c49ab8@syzkaller.appspotmail.com\nDebugged-by: Tadeusz Struk <tadeusz.struk@linaro.org>\nTested-by: Maxim Levitsky <mlevitsk@redhat.com>\nCc: stable@vger.kernel.org\nFixes: bd53cb35a3e9 (\"X86/KVM: Handle PFNs outside of kernel reach when touching GPTEs\")\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>",
  "target": 1,
  "dataset": "other",
  "idx": 201006,
  "Bug Filter": "Security Vulnerability Fix\n\nConfidence Score: 0.8",
  "Bug Filter Confidence": 0.8,
  "Bug Filter Response": "\n\n**Final Output:**\n\nAnalysis: The patch modifies the handling of guest PTEs in the KVM module, addressing inefficiencies and potential issues with memory access. The changes use user space addresses directly, which could prevent unauthorized memory access or information leakage, indicating a security concern.\n\nFinal Classification: Security Vulnerability Fix\n\nConfidence Score: 0.8"
}