{
  "id": 659,
  "language": "h",
  "cwe": "CWE-284",
  "commit_url": "https://github.com/ljalves/linux_media/commit/ac6e780070e30e4c35bd395acfe9191e6268bdd3",
  "commit_sha": "ac6e780070e30e4c35bd395acfe9191e6268bdd3",
  "commit_msg": "tcp: take care of truncations done by sk_filter()\n\nWith syzkaller help, Marco Grassi found a bug in TCP stack,\ncrashing in tcp_collapse()\n\nRoot cause is that sk_filter() can truncate the incoming skb,\nbut TCP stack was not really expecting this to happen.\nIt probably was expecting a simple DROP or ACCEPT behavior.\n\nWe first need to make sure no part of TCP header could be removed.\nThen we need to adjust TCP_SKB_CB(skb)->end_seq\n\nMany thanks to syzkaller team and Marco for giving us a reproducer.\n\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nReported-by: Marco Grassi <marco.gra@gmail.com>\nReported-by: Vladis Dronov <vdronov@redhat.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
  "pr_url": null,
  "pr_info": null,
  "file_name": "include/net/tcp.h",
  "func_name": "",
  "raw_func_from_json": "int tcp_v4_rcv(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph;\n\tconst struct tcphdr *th;\n\tbool refcounted;\n\tstruct sock *sk;\n\tint ret;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/* Count it even if it's bad */\n\t__TCP_INC_STATS(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = (const struct tcphdr *)skb->data;\n\n\tif (unlikely(th->doff < sizeof(struct tcphdr) / 4))\n\t\tgoto bad_packet;\n\tif (!pskb_may_pull(skb, th->doff * 4))\n\t\tgoto discard_it;\n\n\t/* An explanation is required here, I think.\n\t * Packet length and doff are validated by header prediction,\n\t * provided case of th->doff==0 is eliminated.\n\t * So, we defer the checks. */\n\n\tif (skb_checksum_init(skb, IPPROTO_TCP, inet_compute_pseudo))\n\t\tgoto csum_error;\n\n\tth = (const struct tcphdr *)skb->data;\n\tiph = ip_hdr(skb);\n\t/* This is tricky : We move IPCB at its correct location into TCP_SKB_CB()\n\t * barrier() makes sure compiler wont play fool^Waliasing games.\n\t */\n\tmemmove(&TCP_SKB_CB(skb)->header.h4, IPCB(skb),\n\t\tsizeof(struct inet_skb_parm));\n\tbarrier();\n\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff * 4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->tcp_flags = tcp_flag_byte(th);\n\tTCP_SKB_CB(skb)->tcp_tw_isn = 0;\n\tTCP_SKB_CB(skb)->ip_dsfield = ipv4_get_dsfield(iph);\n\tTCP_SKB_CB(skb)->sacked\t = 0;\n\nlookup:\n\tsk = __inet_lookup_skb(&tcp_hashinfo, skb, __tcp_hdrlen(th), th->source,\n\t\t\t       th->dest, &refcounted);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\t\tstruct sock *nsk;\n\n\t\tsk = req->rsk_listener;\n\t\tif (unlikely(tcp_v4_inbound_md5_hash(sk, skb))) {\n\t\t\tsk_drops_add(sk, skb);\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_it;\n\t\t}\n\t\tif (unlikely(sk->sk_state != TCP_LISTEN)) {\n\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\n\t\t\tgoto lookup;\n\t\t}\n\t\t/* We own a reference on the listener, increase it again\n\t\t * as we might lose it too soon.\n\t\t */\n\t\tsock_hold(sk);\n\t\trefcounted = true;\n\t\tnsk = tcp_check_req(sk, skb, req, false);\n\t\tif (!nsk) {\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_and_relse;\n\t\t}\n\t\tif (nsk == sk) {\n\t\t\treqsk_put(req);\n\t\t} else if (tcp_child_process(sk, nsk, skb)) {\n\t\t\ttcp_v4_send_reset(nsk, skb);\n\t\t\tgoto discard_and_relse;\n\t\t} else {\n\t\t\tsock_put(sk);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\tif (tcp_v4_inbound_md5_hash(sk, skb))\n\t\tgoto discard_and_relse;\n \n \tnf_reset(skb);\n \n\tif (sk_filter(sk, skb))\n \t\tgoto discard_and_relse;\n \n \tskb->dev = NULL;\n \n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tret = tcp_v4_do_rcv(sk, skb);\n\t\tgoto put_and_return;\n\t}\n\n\tsk_incoming_cpu_update(sk);\n\n\tbh_lock_sock_nested(sk);\n\ttcp_segs_in(tcp_sk(sk), skb);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n\t\tif (!tcp_prequeue(sk, skb))\n\t\t\tret = tcp_v4_do_rcv(sk, skb);\n\t} else if (tcp_add_backlog(sk, skb)) {\n\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\nput_and_return:\n\tif (refcounted)\n\t\tsock_put(sk);\n\n\treturn ret;\n\nno_tcp_socket:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\tif (tcp_checksum_complete(skb)) {\ncsum_error:\n\t\t__TCP_INC_STATS(net, TCP_MIB_CSUMERRORS);\nbad_packet:\n\t\t__TCP_INC_STATS(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v4_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\t/* Discard frame. */\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsk_drops_add(sk, skb);\n\tif (refcounted)\n\t\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tif (tcp_checksum_complete(skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto csum_error;\n\t}\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN: {\n\t\tstruct sock *sk2 = inet_lookup_listener(dev_net(skb->dev),\n\t\t\t\t\t\t\t&tcp_hashinfo, skb,\n\t\t\t\t\t\t\t__tcp_hdrlen(th),\n\t\t\t\t\t\t\tiph->saddr, th->source,\n\t\t\t\t\t\t\tiph->daddr, th->dest,\n\t\t\t\t\t\t\tinet_iif(skb));\n\t\tif (sk2) {\n\t\t\tinet_twsk_deschedule_put(inet_twsk(sk));\n\t\t\tsk = sk2;\n\t\t\trefcounted = false;\n\t\t\tgoto process;\n\t\t}\n\t\t/* Fall through to ACK */\n\t}\n\tcase TCP_TW_ACK:\n\t\ttcp_v4_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\ttcp_v4_send_reset(sk, skb);\n\t\tinet_twsk_deschedule_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\tcase TCP_TW_SUCCESS:;\n\t}\n\tgoto discard_it;\n}\n",
  "diff_func": "@@ -1220,6 +1220,7 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)\n \n bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);\n bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);\n+int tcp_filter(struct sock *sk, struct sk_buff *skb);\n \n #undef STATE_TRACE\n ",
  "project": "linux",
  "commit_id": "ac6e780070e30e4c35bd395acfe9191e6268bdd3",
  "target": 1,
  "func": "int tcp_v4_rcv(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph;\n\tconst struct tcphdr *th;\n\tbool refcounted;\n\tstruct sock *sk;\n\tint ret;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/* Count it even if it's bad */\n\t__TCP_INC_STATS(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = (const struct tcphdr *)skb->data;\n\n\tif (unlikely(th->doff < sizeof(struct tcphdr) / 4))\n\t\tgoto bad_packet;\n\tif (!pskb_may_pull(skb, th->doff * 4))\n\t\tgoto discard_it;\n\n\t/* An explanation is required here, I think.\n\t * Packet length and doff are validated by header prediction,\n\t * provided case of th->doff==0 is eliminated.\n\t * So, we defer the checks. */\n\n\tif (skb_checksum_init(skb, IPPROTO_TCP, inet_compute_pseudo))\n\t\tgoto csum_error;\n\n\tth = (const struct tcphdr *)skb->data;\n\tiph = ip_hdr(skb);\n\t/* This is tricky : We move IPCB at its correct location into TCP_SKB_CB()\n\t * barrier() makes sure compiler wont play fool^Waliasing games.\n\t */\n\tmemmove(&TCP_SKB_CB(skb)->header.h4, IPCB(skb),\n\t\tsizeof(struct inet_skb_parm));\n\tbarrier();\n\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff * 4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->tcp_flags = tcp_flag_byte(th);\n\tTCP_SKB_CB(skb)->tcp_tw_isn = 0;\n\tTCP_SKB_CB(skb)->ip_dsfield = ipv4_get_dsfield(iph);\n\tTCP_SKB_CB(skb)->sacked\t = 0;\n\nlookup:\n\tsk = __inet_lookup_skb(&tcp_hashinfo, skb, __tcp_hdrlen(th), th->source,\n\t\t\t       th->dest, &refcounted);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\t\tstruct sock *nsk;\n\n\t\tsk = req->rsk_listener;\n\t\tif (unlikely(tcp_v4_inbound_md5_hash(sk, skb))) {\n\t\t\tsk_drops_add(sk, skb);\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_it;\n\t\t}\n\t\tif (unlikely(sk->sk_state != TCP_LISTEN)) {\n\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\n\t\t\tgoto lookup;\n\t\t}\n\t\t/* We own a reference on the listener, increase it again\n\t\t * as we might lose it too soon.\n\t\t */\n\t\tsock_hold(sk);\n\t\trefcounted = true;\n\t\tnsk = tcp_check_req(sk, skb, req, false);\n\t\tif (!nsk) {\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_and_relse;\n\t\t}\n\t\tif (nsk == sk) {\n\t\t\treqsk_put(req);\n\t\t} else if (tcp_child_process(sk, nsk, skb)) {\n\t\t\ttcp_v4_send_reset(nsk, skb);\n\t\t\tgoto discard_and_relse;\n\t\t} else {\n\t\t\tsock_put(sk);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\tif (tcp_v4_inbound_md5_hash(sk, skb))\n\t\tgoto discard_and_relse;\n \n \tnf_reset(skb);\n \n\tif (sk_filter(sk, skb))\n \t\tgoto discard_and_relse;\n \n \tskb->dev = NULL;\n \n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tret = tcp_v4_do_rcv(sk, skb);\n\t\tgoto put_and_return;\n\t}\n\n\tsk_incoming_cpu_update(sk);\n\n\tbh_lock_sock_nested(sk);\n\ttcp_segs_in(tcp_sk(sk), skb);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n\t\tif (!tcp_prequeue(sk, skb))\n\t\t\tret = tcp_v4_do_rcv(sk, skb);\n\t} else if (tcp_add_backlog(sk, skb)) {\n\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\nput_and_return:\n\tif (refcounted)\n\t\tsock_put(sk);\n\n\treturn ret;\n\nno_tcp_socket:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\tif (tcp_checksum_complete(skb)) {\ncsum_error:\n\t\t__TCP_INC_STATS(net, TCP_MIB_CSUMERRORS);\nbad_packet:\n\t\t__TCP_INC_STATS(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v4_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\t/* Discard frame. */\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsk_drops_add(sk, skb);\n\tif (refcounted)\n\t\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tif (tcp_checksum_complete(skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto csum_error;\n\t}\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN: {\n\t\tstruct sock *sk2 = inet_lookup_listener(dev_net(skb->dev),\n\t\t\t\t\t\t\t&tcp_hashinfo, skb,\n\t\t\t\t\t\t\t__tcp_hdrlen(th),\n\t\t\t\t\t\t\tiph->saddr, th->source,\n\t\t\t\t\t\t\tiph->daddr, th->dest,\n\t\t\t\t\t\t\tinet_iif(skb));\n\t\tif (sk2) {\n\t\t\tinet_twsk_deschedule_put(inet_twsk(sk));\n\t\t\tsk = sk2;\n\t\t\trefcounted = false;\n\t\t\tgoto process;\n\t\t}\n\t\t/* Fall through to ACK */\n\t}\n\tcase TCP_TW_ACK:\n\t\ttcp_v4_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\ttcp_v4_send_reset(sk, skb);\n\t\tinet_twsk_deschedule_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\tcase TCP_TW_SUCCESS:;\n\t}\n\tgoto discard_it;\n}\n",
  "big_vul_idx": 180085,
  "idx": 1719,
  "hash": 284014189874385785790868783305397110089,
  "Bug Filter": "Defect Remediation & Feature Upgrades",
  "Bug Filter Confidence": 0.95,
  "Bug Filter Response": "\n\nThe patch addresses a bug causing a TCP stack crash by handling skb truncation properly. It doesn't involve security issues, so it's classified under defect remediation.\n\n**Final Output:**\nAnalysis: The commit fixes a TCP stack bug causing crashes, not a security vulnerability. The code adds a function to handle skb filtering correctly.\nFinal Classification: Defect Remediation & Feature Upgrades\nConfidence Score: 0.95"
}