{
  "id": 1091,
  "language": "c",
  "cwe": "CWE-362",
  "commit_url": "https://github.com/sjp38/linux.personal/commit/321027c1fe77f892f4ea07846aeae08cefbbb290",
  "commit_sha": "321027c1fe77f892f4ea07846aeae08cefbbb290",
  "commit_msg": "perf/core: Fix concurrent sys_perf_event_open() vs. 'move_group' race\n\nDi Shen reported a race between two concurrent sys_perf_event_open()\ncalls where both try and move the same pre-existing software group\ninto a hardware context.\n\nThe problem is exactly that described in commit:\n\n  f63a8daa5812 (\"perf: Fix event->ctx locking\")\n\n... where, while we wait for a ctx->mutex acquisition, the event->ctx\nrelation can have changed under us.\n\nThat very same commit failed to recognise sys_perf_event_context() as an\nexternal access vector to the events and thereby didn't apply the\nestablished locking rules correctly.\n\nSo while one sys_perf_event_open() call is stuck waiting on\nmutex_lock_double(), the other (which owns said locks) moves the group\nabout. So by the time the former sys_perf_event_open() acquires the\nlocks, the context we've acquired is stale (and possibly dead).\n\nApply the established locking rules as per perf_event_ctx_lock_nested()\nto the mutex_lock_double() for the 'move_group' case. This obviously means\nwe need to validate state after we acquire the locks.\n\nReported-by: Di Shen (Keen Lab)\nTested-by: John Dias <joaodias@google.com>\nSigned-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Alexander Shishkin <alexander.shishkin@linux.intel.com>\nCc: Arnaldo Carvalho de Melo <acme@kernel.org>\nCc: Arnaldo Carvalho de Melo <acme@redhat.com>\nCc: Jiri Olsa <jolsa@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Linus Torvalds <torvalds@linux-foundation.org>\nCc: Min Chong <mchong@google.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Stephane Eranian <eranian@google.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vince Weaver <vincent.weaver@maine.edu>\nFixes: f63a8daa5812 (\"perf: Fix event->ctx locking\")\nLink: http://lkml.kernel.org/r/20170106131444.GZ3174@twins.programming.kicks-ass.net\nSigned-off-by: Ingo Molnar <mingo@kernel.org>",
  "pr_url": null,
  "pr_info": null,
  "file_name": "kernel/events/core.c",
  "func_name": "",
  "raw_func_from_json": "SYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\tint cgroup_fd = -1;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!attr.sample_max_stack)\n\t\tattr.sample_max_stack = sysctl_perf_event_max_stack;\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tif (task) {\n\t\terr = mutex_lock_interruptible(&task->signal->cred_guard_mutex);\n\t\tif (err)\n\t\t\tgoto err_cpus;\n\n\t\t/*\n\t\t * Reuse ptrace permission checks for now.\n\t\t *\n\t\t * We must hold cred_guard_mutex across this and any potential\n\t\t * perf_install_in_context() call for this new event to\n\t\t * serialize against exec() altering our credentials (and the\n\t\t * perf_event_exit_task() that could imply).\n\t\t */\n\t\terr = -EACCES;\n\t\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))\n\t\t\tgoto err_cred;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP)\n\t\tcgroup_fd = pid;\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL, cgroup_fd);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cred;\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (attr.use_clockid) {\n\t\terr = perf_event_set_clock(event, attr.clockid);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {\n\t\terr = -EBUSY;\n\t\tgoto err_context;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\n\t\t/* All events in a group should have the same clock */\n\t\tif (group_leader->clock != event->clock)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tevent_file = NULL;\n\t\tgoto err_context;\n \t}\n \n \tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n \t\tif (gctx->task == TASK_TOMBSTONE) {\n \t\t\terr = -ESRCH;\n \t\t\tgoto err_locked;\n \t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n \t}\n\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_locked;\n\t}\n\n\tif (!perf_event_validate_size(event)) {\n\t\terr = -E2BIG;\n\t\tgoto err_locked;\n\t}\n\n\t/*\n\t * Must be under the same ctx::mutex as perf_install_in_context(),\n\t * because we need to serialize with concurrent event creation.\n\t */\n\tif (!exclusive_event_installable(event, ctx)) {\n\t\t/* exclusive and group stuff are assumed mutually exclusive */\n\t\tWARN_ON_ONCE(move_group);\n\n\t\terr = -EBUSY;\n\t\tgoto err_locked;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\t/*\n\t * This is the point on no return; we cannot fail hereafter. This is\n\t * where we start modifying current state.\n\t */\n\n\tif (move_group) {\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tperf_remove_from_context(group_leader, 0);\n\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_ctx(gctx);\n\t\t}\n\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\t/*\n\t\t * Install the group siblings before the group leader.\n\t\t *\n\t\t * Because a group leader will try and install the entire group\n\t\t * (through the sibling list, which is still in-tact), we can\n\t\t * end up with siblings installed in the wrong context.\n\t\t *\n\t\t * By installing siblings first we NO-OP because they're not\n\t\t * reachable through the group lists.\n\t\t */\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_event__state_init(sibling);\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\n\t\t/*\n\t\t * Now that all events are installed in @ctx, nothing\n\t\t * references @gctx anymore, so drop the last reference we have\n\t\t * on it.\n\t\t */\n\t\tput_ctx(gctx);\n\t}\n\n\t/*\n\t * Precalculate sample_data sizes; do while holding ctx::mutex such\n\t * that we're serialized against further additions and before\n\t * perf_install_in_context() which is the point the event is active and\n\t * can use these values.\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\tevent->owner = current;\n\n\tperf_install_in_context(ctx, event, event->cpu);\n \tperf_unpin_context(ctx);\n \n \tif (move_group)\n\t\tmutex_unlock(&gctx->mutex);\n \tmutex_unlock(&ctx->mutex);\n \n \tif (task) {\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\n\t\tput_task_struct(task);\n\t}\n\n\tput_online_cpus();\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n \n err_locked:\n \tif (move_group)\n\t\tmutex_unlock(&gctx->mutex);\n \tmutex_unlock(&ctx->mutex);\n /* err_file: */\n \tfput(event_file);\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\t/*\n\t * If event_file is set, the fput() above will have called ->release()\n\t * and that will take care of freeing the event.\n\t */\n\tif (!event_file)\n\t\tfree_event(event);\nerr_cred:\n\tif (task)\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n",
  "diff_func": "@@ -9529,6 +9529,37 @@ static int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)\n \treturn 0;\n }\n \n+/*\n+ * Variation on perf_event_ctx_lock_nested(), except we take two context\n+ * mutexes.\n+ */\n+static struct perf_event_context *\n+__perf_event_ctx_lock_double(struct perf_event *group_leader,\n+\t\t\t     struct perf_event_context *ctx)\n+{\n+\tstruct perf_event_context *gctx;\n+\n+again:\n+\trcu_read_lock();\n+\tgctx = READ_ONCE(group_leader->ctx);\n+\tif (!atomic_inc_not_zero(&gctx->refcount)) {\n+\t\trcu_read_unlock();\n+\t\tgoto again;\n+\t}\n+\trcu_read_unlock();\n+\n+\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\n+\tif (group_leader->ctx != gctx) {\n+\t\tmutex_unlock(&ctx->mutex);\n+\t\tmutex_unlock(&gctx->mutex);\n+\t\tput_ctx(gctx);\n+\t\tgoto again;\n+\t}\n+\n+\treturn gctx;\n+}\n+\n /**\n  * sys_perf_event_open - open a performance event, associate it to a task/cpu\n  *\n@@ -9772,12 +9803,31 @@ SYSCALL_DEFINE5(perf_event_open,\n \t}\n \n \tif (move_group) {\n-\t\tgctx = group_leader->ctx;\n-\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n+\n \t\tif (gctx->task == TASK_TOMBSTONE) {\n \t\t\terr = -ESRCH;\n \t\t\tgoto err_locked;\n \t\t}\n+\n+\t\t/*\n+\t\t * Check if we raced against another sys_perf_event_open() call\n+\t\t * moving the software group underneath us.\n+\t\t */\n+\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n+\t\t\t/*\n+\t\t\t * If someone moved the group out from under us, check\n+\t\t\t * if this new event wound up on the same ctx, if so\n+\t\t\t * its the regular !move_group case, otherwise fail.\n+\t\t\t */\n+\t\t\tif (gctx != ctx) {\n+\t\t\t\terr = -EINVAL;\n+\t\t\t\tgoto err_locked;\n+\t\t\t} else {\n+\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n+\t\t\t\tmove_group = 0;\n+\t\t\t}\n+\t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n \t}\n@@ -9879,7 +9929,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \tperf_unpin_context(ctx);\n \n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n \n \tif (task) {\n@@ -9905,7 +9955,7 @@ SYSCALL_DEFINE5(perf_event_open,\n \n err_locked:\n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n /* err_file: */\n \tfput(event_file);",
  "project": "linux",
  "commit_id": "321027c1fe77f892f4ea07846aeae08cefbbb290",
  "target": 1,
  "func": "SYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\tint cgroup_fd = -1;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!attr.sample_max_stack)\n\t\tattr.sample_max_stack = sysctl_perf_event_max_stack;\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tif (task) {\n\t\terr = mutex_lock_interruptible(&task->signal->cred_guard_mutex);\n\t\tif (err)\n\t\t\tgoto err_cpus;\n\n\t\t/*\n\t\t * Reuse ptrace permission checks for now.\n\t\t *\n\t\t * We must hold cred_guard_mutex across this and any potential\n\t\t * perf_install_in_context() call for this new event to\n\t\t * serialize against exec() altering our credentials (and the\n\t\t * perf_event_exit_task() that could imply).\n\t\t */\n\t\terr = -EACCES;\n\t\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))\n\t\t\tgoto err_cred;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP)\n\t\tcgroup_fd = pid;\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL, cgroup_fd);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cred;\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (attr.use_clockid) {\n\t\terr = perf_event_set_clock(event, attr.clockid);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {\n\t\terr = -EBUSY;\n\t\tgoto err_context;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\n\t\t/* All events in a group should have the same clock */\n\t\tif (group_leader->clock != event->clock)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tevent_file = NULL;\n\t\tgoto err_context;\n \t}\n \n \tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n \t\tif (gctx->task == TASK_TOMBSTONE) {\n \t\t\terr = -ESRCH;\n \t\t\tgoto err_locked;\n \t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n \t}\n\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_locked;\n\t}\n\n\tif (!perf_event_validate_size(event)) {\n\t\terr = -E2BIG;\n\t\tgoto err_locked;\n\t}\n\n\t/*\n\t * Must be under the same ctx::mutex as perf_install_in_context(),\n\t * because we need to serialize with concurrent event creation.\n\t */\n\tif (!exclusive_event_installable(event, ctx)) {\n\t\t/* exclusive and group stuff are assumed mutually exclusive */\n\t\tWARN_ON_ONCE(move_group);\n\n\t\terr = -EBUSY;\n\t\tgoto err_locked;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\t/*\n\t * This is the point on no return; we cannot fail hereafter. This is\n\t * where we start modifying current state.\n\t */\n\n\tif (move_group) {\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tperf_remove_from_context(group_leader, 0);\n\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_ctx(gctx);\n\t\t}\n\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\t/*\n\t\t * Install the group siblings before the group leader.\n\t\t *\n\t\t * Because a group leader will try and install the entire group\n\t\t * (through the sibling list, which is still in-tact), we can\n\t\t * end up with siblings installed in the wrong context.\n\t\t *\n\t\t * By installing siblings first we NO-OP because they're not\n\t\t * reachable through the group lists.\n\t\t */\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_event__state_init(sibling);\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\n\t\t/*\n\t\t * Now that all events are installed in @ctx, nothing\n\t\t * references @gctx anymore, so drop the last reference we have\n\t\t * on it.\n\t\t */\n\t\tput_ctx(gctx);\n\t}\n\n\t/*\n\t * Precalculate sample_data sizes; do while holding ctx::mutex such\n\t * that we're serialized against further additions and before\n\t * perf_install_in_context() which is the point the event is active and\n\t * can use these values.\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\tevent->owner = current;\n\n\tperf_install_in_context(ctx, event, event->cpu);\n \tperf_unpin_context(ctx);\n \n \tif (move_group)\n\t\tmutex_unlock(&gctx->mutex);\n \tmutex_unlock(&ctx->mutex);\n \n \tif (task) {\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\n\t\tput_task_struct(task);\n\t}\n\n\tput_online_cpus();\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n \n err_locked:\n \tif (move_group)\n\t\tmutex_unlock(&gctx->mutex);\n \tmutex_unlock(&ctx->mutex);\n /* err_file: */\n \tfput(event_file);\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\t/*\n\t * If event_file is set, the fput() above will have called ->release()\n\t * and that will take care of freeing the event.\n\t */\n\tif (!event_file)\n\t\tfree_event(event);\nerr_cred:\n\tif (task)\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n",
  "big_vul_idx": 181539,
  "idx": 2985,
  "hash": 208685279778457638967404001174081094433,
  "Bug Filter": "Defect Remediation & Feature Upgrades",
  "Bug Filter Confidence": 0.95,
  "Bug Filter Response": "\n\n**Final Output:**\n\nAnalysis: The patch addresses a race condition in the sys_perf_event_open() function, ensuring proper locking and preventing stale contexts. The issue is a functional bug fix, not a security vulnerability.\n\nFinal Classification: Defect Remediation & Feature Upgrades\n\nConfidence Score: 0.95"
}