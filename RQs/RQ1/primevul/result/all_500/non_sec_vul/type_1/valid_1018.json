{
  "id": 1018,
  "language": "js",
  "cwe": "",
  "commit_url": "https://github.com/VoerEirAB/mongo/commit/f3604b901d688c194de5e430c7fbab060c9dc8e0",
  "commit_sha": "f3604b901d688c194de5e430c7fbab060c9dc8e0",
  "commit_msg": "SERVER-59071 Treat '$sample' as unsharded when connecting directly to shards",
  "pr_url": null,
  "pr_info": null,
  "file_name": "jstests/sharding/sample_direct_connection.js",
  "func_name": "",
  "raw_func_from_json": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    // If the incoming operation is sharded, use the CSS to infer the filtering metadata for the\n    // collection, otherwise treat it as unsharded\n    auto collectionFilter =\n        CollectionShardingState::get(opCtx, coll->ns())\n            ->getOwnershipFilter(\n                opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (collectionFilter.isSharded() && !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // We can't take ARHASH optimization path for a direct $sample on the system.buckets\n        // collection because data is in compressed form. If we did have a direct $sample on the\n        // system.buckets collection, then the 'bucketUnpacker' would not be set up properly. We\n        // also should bail out early if a $sample is made against a time series collection that is\n        // empty. If we don't the 'minAdvancedToWorkRatio' can be nan/-nan depending on the\n        // architecture.\n        if (!(bucketUnpacker && numRecords)) {\n            return std::pair{nullptr, false};\n        }\n\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}",
  "diff_func": "@@ -0,0 +1,24 @@\n+// This test ensures that an aggregation pipeline with a $sample stage can be issued through a\n+// direct connection to a shard without failing, and $sample behaves as if we sampled an unsharded\n+// collection.\n+// @tags: [requires_fcv_51]\n+(function() {\n+let st = new ShardingTest({shards: 1});\n+\n+assert.commandWorked(st.s.adminCommand({enableSharding: 'test'}));\n+assert.commandWorked(st.s.adminCommand({shardCollection: 'test.sharded', key: {x: 1}}));\n+const testDB = st.s.getDB('test');\n+\n+// We must have >100 samples to attempt the storage optimized sample path.\n+for (let x = 0; x < 101; x++) {\n+    assert.commandWorked(testDB.foo.insert({x: x}));\n+}\n+\n+const shardDB = st.rs0.getPrimary().getDB('test');\n+\n+const res = assert.commandWorked(\n+    shardDB.runCommand({aggregate: 'foo', pipeline: [{$sample: {size: 3}}], cursor: {}}));\n+assert.eq(res.cursor.firstBatch.length, 3);\n+\n+st.stop();\n+})();",
  "func": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    // If the incoming operation is sharded, use the CSS to infer the filtering metadata for the\n    // collection, otherwise treat it as unsharded\n    auto collectionFilter =\n        CollectionShardingState::get(opCtx, coll->ns())\n            ->getOwnershipFilter(\n                opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (collectionFilter.isSharded() && !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // We can't take ARHASH optimization path for a direct $sample on the system.buckets\n        // collection because data is in compressed form. If we did have a direct $sample on the\n        // system.buckets collection, then the 'bucketUnpacker' would not be set up properly. We\n        // also should bail out early if a $sample is made against a time series collection that is\n        // empty. If we don't the 'minAdvancedToWorkRatio' can be nan/-nan depending on the\n        // architecture.\n        if (!(bucketUnpacker && numRecords)) {\n            return std::pair{nullptr, false};\n        }\n\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}",
  "project": "mongo",
  "hash": 215078869678300356623594887635131238477,
  "size": 171,
  "commit_id": "f3604b901d688c194de5e430c7fbab060c9dc8e0",
  "message": "SERVER-59071 Treat '$sample' as unsharded when connecting directly to shards",
  "target": 1,
  "dataset": "other",
  "idx": 210273,
  "Bug Filter": "Testing & Validation Updates",
  "Bug Filter Confidence": 0.95,
  "Bug Filter Response": "**Final Output:**\n\n- **Final Classification:** Testing & Validation Updates\n- **Confidence:** 0.95"
}