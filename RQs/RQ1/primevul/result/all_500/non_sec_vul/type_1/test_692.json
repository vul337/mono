{
  "id": 692,
  "language": "cc",
  "cwe": "",
  "commit_url": "https://github.com/prividentity/tensorflow/commit/203214568f5bc237603dbab6e1fd389f1572f5c9",
  "commit_sha": "203214568f5bc237603dbab6e1fd389f1572f5c9",
  "commit_msg": "Reorganize and add more validation to MKL requantization\n\nPiperOrigin-RevId: 387901341\nChange-Id: I2515b9034c64e113db0bcec8337d30643ab0a0f1",
  "pr_url": null,
  "pr_info": null,
  "file_name": "tensorflow/core/kernels/mkl/mkl_requantize_per_channel_op.cc",
  "func_name": "",
  "raw_func_from_json": "  void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
  "diff_func": "@@ -49,35 +49,45 @@ class MklRequantizePerChannelOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override {\n     try {\n       const Tensor& input = ctx->input(kInputTensorIndex);\n+      OP_REQUIRES(\n+          ctx, input.dims() == 4,\n+          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n+                                  \"supports 4D tensors only.\"));\n+\n       const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n+      size_t depth = input_min_vec.NumElements();\n       float* input_min_vec_data = (float*)const_cast<void*>(\n           static_cast<const void*>(input_min_vec.flat<float>().data()));\n+\n       const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n+      OP_REQUIRES(\n+          ctx, input_max_vec.NumElements() == depth,\n+          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n+                                  depth, \" was \", input_max_vec.NumElements()));\n       float* input_max_vec_data = (float*)const_cast<void*>(\n           static_cast<const void*>(input_max_vec.flat<float>().data()));\n \n       const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n+      OP_REQUIRES(\n+          ctx, input_requested_min.NumElements() == 1,\n+          errors::InvalidArgument(\"requested_output_min must be a scalar\"));\n       const float input_requested_min_float =\n           input_requested_min.flat<float>()(0);\n+\n       const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n+      OP_REQUIRES(\n+          ctx, input_requested_min.NumElements() == 1,\n+          errors::InvalidArgument(\"requested_output_max must be a scalar\"));\n       const float input_requested_max_float =\n           input_requested_max.flat<float>()(0);\n \n-      size_t depth = input_min_vec.NumElements();\n-      OP_REQUIRES(\n-          ctx, input.dims() == 4,\n-          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n-                                  \"supports 4D tensors only.\"));\n-      OP_REQUIRES(\n-          ctx, input_min_vec.dim_size(0) == depth,\n-          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n-                                  depth, \" was \", input_min_vec.dim_size(0)));\n-      OP_REQUIRES(\n-          ctx, input_max_vec.dim_size(0) == depth,\n-          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n-                                  depth, \" was \", input_max_vec.dim_size(0)));\n-\n-      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n+      if (out_type_ == DT_QINT8) {\n+        OP_REQUIRES(ctx, input_requested_min_float < 0.0f,\n+                    errors::InvalidArgument(\n+                        \"If out_type is QINT8, requested_output_max must be \"\n+                        \"non negative, got \",\n+                        input_requested_min_float));\n+      }\n \n       const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n       const float requested_min_max =",
  "func": "  void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
  "project": "tensorflow",
  "hash": 103677223950190323637234469418983500216,
  "size": 109,
  "commit_id": "203214568f5bc237603dbab6e1fd389f1572f5c9",
  "message": "Reorganize and add more validation to MKL requantization\n\nPiperOrigin-RevId: 387901341\nChange-Id: I2515b9034c64e113db0bcec8337d30643ab0a0f1",
  "target": 1,
  "dataset": "other",
  "idx": 197239,
  "Bug Filter": "Testing & Validation Updates",
  "Bug Filter Confidence": 0.9,
  "Bug Filter Response": "\n\n**Analysis:**\nThe patch introduces additional validation checks to the MklRequantizePerChannelOp class, ensuring that input tensors meet specific dimensional and scalar requirements. These changes enhance the robustness of the code by preventing invalid tensor shapes or sizes from causing issues during computation.\n\n**Final Classification: Testing & Validation Updates**\n\n**Confidence Score: 0.9**\n\nThe changes are focused on adding more rigorous input validation, which helps prevent incorrect tensor dimensions or values. While these validations improve the function's correctness, they do not address a security vulnerability, as there's no evidence of exploitable issues."
}