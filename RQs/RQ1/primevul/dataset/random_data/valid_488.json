{
  "id": 488,
  "language": "cc",
  "cwe": "",
  "commit_url": "https://github.com/prividentity/tensorflow/commit/12c727cee857fa19be717f336943d95fca4ffe4f",
  "commit_sha": "12c727cee857fa19be717f336943d95fca4ffe4f",
  "commit_msg": "Validate inputs of `FractionalAvgPoolGrad`.\n\nPiperOrigin-RevId: 372420640\nChange-Id: Icc583928e6cdc3062e12498e4d2337a8fe3da016",
  "pr_url": null,
  "pr_info": null,
  "file_name": "tensorflow/core/kernels/fractional_avg_pool_op.cc",
  "func_name": "",
  "raw_func_from_json": "  void Compute(OpKernelContext* context) override {\n    // Here's the basic idea:\n    // Batch and depth dimension are independent from row and col dimension. And\n    // because FractionalAvgPool currently only support pooling along row and\n    // col, we can basically think of this 4D tensor backpropagation as\n    // operation of a series of 2D planes.\n    //\n    // For each element of a 'slice' (2D plane) of output_backprop, we need to\n    // figure out its contributors when doing FractionalAvgPool operation. This\n    // can be done based on row_pooling_sequence, col_pooling_seq and\n    // overlapping.\n    // Once we figure out the original contributors, we just need to evenly\n    // divide the value of this element among these contributors.\n    //\n    // Internally, we divide the out_backprop tensor and store it in a temporary\n    // tensor of double type. And cast it to the corresponding type.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenDoubleMatrixMap;\n\n    // Grab the inputs.\n    const Tensor& orig_input_tensor_shape = context->input(0);\n    OP_REQUIRES(context,\n                orig_input_tensor_shape.dims() == 1 &&\n                    orig_input_tensor_shape.NumElements() == 4,\n                errors::InvalidArgument(\"original input tensor shape must be\"\n                                        \"1-dimensional and 4 elements\"));\n    const Tensor& out_backprop = context->input(1);\n    const Tensor& row_seq_tensor = context->input(2);\n    const Tensor& col_seq_tensor = context->input(3);\n\n    const int64 out_batch = out_backprop.dim_size(0);\n    const int64 out_rows = out_backprop.dim_size(1);\n    const int64 out_cols = out_backprop.dim_size(2);\n    const int64 out_depth = out_backprop.dim_size(3);\n\n    auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n    auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n    auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();\n\n    const int64 in_batch = orig_input_tensor_shape_flat(0);\n    const int64 in_rows = orig_input_tensor_shape_flat(1);\n    const int64 in_cols = orig_input_tensor_shape_flat(2);\n    const int64 in_depth = orig_input_tensor_shape_flat(3);\n\n    constexpr int tensor_in_and_out_dims = 4;\n    // Transform orig_input_tensor_shape into TensorShape\n    TensorShape in_shape;\n    for (auto i = 0; i < tensor_in_and_out_dims; ++i) {\n      in_shape.AddDim(orig_input_tensor_shape_flat(i));\n    }\n\n    // Create intermediate in_backprop.\n    Tensor in_backprop_tensor_temp;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {0}, DataTypeToEnum<double>::v(), in_shape,\n                                &in_backprop_tensor_temp));\n    in_backprop_tensor_temp.flat<double>().setZero();\n    // Transform 4D tensor to 2D matrix.\n    EigenDoubleMatrixMap in_backprop_tensor_temp_mat(\n        in_backprop_tensor_temp.flat<double>().data(), in_depth,\n        in_cols * in_rows * in_batch);\n    ConstEigenMatrixMap out_backprop_mat(out_backprop.flat<T>().data(),\n                                         out_depth,\n                                         out_cols * out_rows * out_batch);\n    // Loop through each element of out_backprop and evenly distribute the\n    // element to the corresponding pooling cell.\n    const int64 in_max_row_index = in_rows - 1;\n    const int64 in_max_col_index = in_cols - 1;\n    for (int64 b = 0; b < out_batch; ++b) {\n      for (int64 r = 0; r < out_rows; ++r) {\n        const int64 in_row_start = row_seq_tensor_flat(r);\n        int64 in_row_end = overlapping_ ? row_seq_tensor_flat(r + 1)\n                                        : row_seq_tensor_flat(r + 1) - 1;\n        in_row_end = std::min(in_row_end, in_max_row_index);\n        for (int64 c = 0; c < out_cols; ++c) {\n          const int64 in_col_start = col_seq_tensor_flat(c);\n          int64 in_col_end = overlapping_ ? col_seq_tensor_flat(c + 1)\n                                          : col_seq_tensor_flat(c + 1) - 1;\n          in_col_end = std::min(in_col_end, in_max_col_index);\n\n          const int64 num_elements_in_pooling_cell =\n              (in_row_end - in_row_start + 1) * (in_col_end - in_col_start + 1);\n          const int64 out_index = (b * out_rows + r) * out_cols + c;\n          // Now we can evenly distribute out_backprop(b, h, w, *) to\n          // in_backprop(b, hs:he, ws:we, *).\n          for (int64 in_r = in_row_start; in_r <= in_row_end; ++in_r) {\n            for (int64 in_c = in_col_start; in_c <= in_col_end; ++in_c) {\n              const int64 in_index = (b * in_rows + in_r) * in_cols + in_c;\n              // Walk through each channel (depth).\n              for (int64 d = 0; d < out_depth; ++d) {\n                const double out_backprop_element = static_cast<double>(\n                    out_backprop_mat.coeffRef(d, out_index));\n                double& in_backprop_ref =\n                    in_backprop_tensor_temp_mat.coeffRef(d, in_index);\n                in_backprop_ref +=\n                    out_backprop_element / num_elements_in_pooling_cell;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Depending on the type, cast double to type T.\n    Tensor* in_backprop_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, in_shape, &in_backprop_tensor));\n    auto in_backprop_tensor_flat = in_backprop_tensor->flat<T>();\n    auto in_backprop_tensor_temp_flat = in_backprop_tensor_temp.flat<double>();\n    for (int64 i = 0; i < in_backprop_tensor_flat.size(); ++i) {\n      in_backprop_tensor_flat(i) =\n          static_cast<T>(in_backprop_tensor_temp_flat(i));\n    }\n  }",
  "diff_func": "@@ -250,6 +250,19 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     const int64 out_cols = out_backprop.dim_size(2);\n     const int64 out_depth = out_backprop.dim_size(3);\n \n+    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", row_seq_tensor must have at least \",\n+                                        out_rows + 1, \" elements, but got \",\n+                                        row_seq_tensor.NumElements()));\n+    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", col_seq_tensor must have at least \",\n+                                        out_cols + 1, \" elements, but got \",\n+                                        col_seq_tensor.NumElements()));\n+\n     auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n     auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n     auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();",
  "func": "  void Compute(OpKernelContext* context) override {\n    // Here's the basic idea:\n    // Batch and depth dimension are independent from row and col dimension. And\n    // because FractionalAvgPool currently only support pooling along row and\n    // col, we can basically think of this 4D tensor backpropagation as\n    // operation of a series of 2D planes.\n    //\n    // For each element of a 'slice' (2D plane) of output_backprop, we need to\n    // figure out its contributors when doing FractionalAvgPool operation. This\n    // can be done based on row_pooling_sequence, col_pooling_seq and\n    // overlapping.\n    // Once we figure out the original contributors, we just need to evenly\n    // divide the value of this element among these contributors.\n    //\n    // Internally, we divide the out_backprop tensor and store it in a temporary\n    // tensor of double type. And cast it to the corresponding type.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenDoubleMatrixMap;\n\n    // Grab the inputs.\n    const Tensor& orig_input_tensor_shape = context->input(0);\n    OP_REQUIRES(context,\n                orig_input_tensor_shape.dims() == 1 &&\n                    orig_input_tensor_shape.NumElements() == 4,\n                errors::InvalidArgument(\"original input tensor shape must be\"\n                                        \"1-dimensional and 4 elements\"));\n    const Tensor& out_backprop = context->input(1);\n    const Tensor& row_seq_tensor = context->input(2);\n    const Tensor& col_seq_tensor = context->input(3);\n\n    const int64 out_batch = out_backprop.dim_size(0);\n    const int64 out_rows = out_backprop.dim_size(1);\n    const int64 out_cols = out_backprop.dim_size(2);\n    const int64 out_depth = out_backprop.dim_size(3);\n\n    auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n    auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n    auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();\n\n    const int64 in_batch = orig_input_tensor_shape_flat(0);\n    const int64 in_rows = orig_input_tensor_shape_flat(1);\n    const int64 in_cols = orig_input_tensor_shape_flat(2);\n    const int64 in_depth = orig_input_tensor_shape_flat(3);\n\n    constexpr int tensor_in_and_out_dims = 4;\n    // Transform orig_input_tensor_shape into TensorShape\n    TensorShape in_shape;\n    for (auto i = 0; i < tensor_in_and_out_dims; ++i) {\n      in_shape.AddDim(orig_input_tensor_shape_flat(i));\n    }\n\n    // Create intermediate in_backprop.\n    Tensor in_backprop_tensor_temp;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {0}, DataTypeToEnum<double>::v(), in_shape,\n                                &in_backprop_tensor_temp));\n    in_backprop_tensor_temp.flat<double>().setZero();\n    // Transform 4D tensor to 2D matrix.\n    EigenDoubleMatrixMap in_backprop_tensor_temp_mat(\n        in_backprop_tensor_temp.flat<double>().data(), in_depth,\n        in_cols * in_rows * in_batch);\n    ConstEigenMatrixMap out_backprop_mat(out_backprop.flat<T>().data(),\n                                         out_depth,\n                                         out_cols * out_rows * out_batch);\n    // Loop through each element of out_backprop and evenly distribute the\n    // element to the corresponding pooling cell.\n    const int64 in_max_row_index = in_rows - 1;\n    const int64 in_max_col_index = in_cols - 1;\n    for (int64 b = 0; b < out_batch; ++b) {\n      for (int64 r = 0; r < out_rows; ++r) {\n        const int64 in_row_start = row_seq_tensor_flat(r);\n        int64 in_row_end = overlapping_ ? row_seq_tensor_flat(r + 1)\n                                        : row_seq_tensor_flat(r + 1) - 1;\n        in_row_end = std::min(in_row_end, in_max_row_index);\n        for (int64 c = 0; c < out_cols; ++c) {\n          const int64 in_col_start = col_seq_tensor_flat(c);\n          int64 in_col_end = overlapping_ ? col_seq_tensor_flat(c + 1)\n                                          : col_seq_tensor_flat(c + 1) - 1;\n          in_col_end = std::min(in_col_end, in_max_col_index);\n\n          const int64 num_elements_in_pooling_cell =\n              (in_row_end - in_row_start + 1) * (in_col_end - in_col_start + 1);\n          const int64 out_index = (b * out_rows + r) * out_cols + c;\n          // Now we can evenly distribute out_backprop(b, h, w, *) to\n          // in_backprop(b, hs:he, ws:we, *).\n          for (int64 in_r = in_row_start; in_r <= in_row_end; ++in_r) {\n            for (int64 in_c = in_col_start; in_c <= in_col_end; ++in_c) {\n              const int64 in_index = (b * in_rows + in_r) * in_cols + in_c;\n              // Walk through each channel (depth).\n              for (int64 d = 0; d < out_depth; ++d) {\n                const double out_backprop_element = static_cast<double>(\n                    out_backprop_mat.coeffRef(d, out_index));\n                double& in_backprop_ref =\n                    in_backprop_tensor_temp_mat.coeffRef(d, in_index);\n                in_backprop_ref +=\n                    out_backprop_element / num_elements_in_pooling_cell;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Depending on the type, cast double to type T.\n    Tensor* in_backprop_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, in_shape, &in_backprop_tensor));\n    auto in_backprop_tensor_flat = in_backprop_tensor->flat<T>();\n    auto in_backprop_tensor_temp_flat = in_backprop_tensor_temp.flat<double>();\n    for (int64 i = 0; i < in_backprop_tensor_flat.size(); ++i) {\n      in_backprop_tensor_flat(i) =\n          static_cast<T>(in_backprop_tensor_temp_flat(i));\n    }\n  }",
  "project": "tensorflow",
  "hash": 220832190112288234713294919372485684625,
  "size": 116,
  "commit_id": "12c727cee857fa19be717f336943d95fca4ffe4f",
  "message": "Validate inputs of `FractionalAvgPoolGrad`.\n\nPiperOrigin-RevId: 372420640\nChange-Id: Icc583928e6cdc3062e12498e4d2337a8fe3da016",
  "target": 1,
  "dataset": "other",
  "idx": 197133
}