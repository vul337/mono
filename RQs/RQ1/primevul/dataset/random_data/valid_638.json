{
  "id": 638,
  "language": "c",
  "cwe": "",
  "commit_url": "https://github.com/tzcnt/tmc-blosc2/commit/c4c6470e88210afc95262c8b9fcc27e30ca043ee",
  "commit_sha": "c4c6470e88210afc95262c8b9fcc27e30ca043ee",
  "commit_msg": "Fixed asan heap buffer overflow when not enough space to write compressed block size.",
  "pr_url": null,
  "pr_info": null,
  "file_name": "blosc/blosc2.c",
  "func_name": "",
  "raw_func_from_json": "static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n                   int32_t leftoverblock, int32_t ntbytes, int32_t maxbytes,\n                   const uint8_t* src, const int32_t offset, uint8_t* dest,\n                   uint8_t* tmp, uint8_t* tmp2) {\n  blosc2_context* context = thread_context->parent_context;\n  int dont_split = (context->header_flags & 0x10) >> 4;\n  int dict_training = context->use_dict && context->dict_cdict == NULL;\n  int32_t j, neblock, nstreams;\n  int32_t cbytes;                   /* number of compressed bytes in split */\n  int32_t ctbytes = 0;              /* number of compressed bytes in block */\n  int64_t maxout;\n  int32_t typesize = context->typesize;\n  const char* compname;\n  int accel;\n  const uint8_t* _src;\n  uint8_t *_tmp = tmp, *_tmp2 = tmp2;\n  uint8_t *_tmp3 = thread_context->tmp4;\n  int last_filter_index = last_filter(context->filters, 'c');\n  bool memcpyed = context->header_flags & (uint8_t)BLOSC_MEMCPYED;\n\n  if (last_filter_index >= 0 || context->prefilter != NULL) {\n    /* Apply the filter pipeline just for the prefilter */\n    if (memcpyed && context->prefilter != NULL) {\n      // We only need the prefilter output\n      _src = pipeline_c(thread_context, bsize, src, offset, dest, _tmp2, _tmp3);\n\n      if (_src == NULL) {\n        return -9;  // signals a problem with the filter pipeline\n      }\n      return bsize;\n    }\n    /* Apply regular filter pipeline */\n    _src = pipeline_c(thread_context, bsize, src, offset, _tmp, _tmp2, _tmp3);\n\n    if (_src == NULL) {\n      return -9;  // signals a problem with the filter pipeline\n    }\n  } else {\n    _src = src + offset;\n  }\n\n  assert(context->clevel > 0);\n\n  /* Calculate acceleration for different compressors */\n  accel = get_accel(context);\n\n  /* The number of compressed data streams for this block */\n  if (!dont_split && !leftoverblock && !dict_training) {\n    nstreams = (int32_t)typesize;\n  }\n  else {\n    nstreams = 1;\n  }\n  neblock = bsize / nstreams;\n  for (j = 0; j < nstreams; j++) {\n    if (!dict_training) {\n      dest += sizeof(int32_t);\n      ntbytes += sizeof(int32_t);\n      ctbytes += sizeof(int32_t);\n    }\n\n    // See if we have a run here\n    const uint8_t* ip = (uint8_t*)_src + j * neblock;\n    const uint8_t* ipbound = (uint8_t*)_src + (j + 1) * neblock;\n    if (get_run(ip, ipbound)) {\n      // A run.  Encode the repeated byte as a negative length in the length of the split.\n      int32_t value = _src[j * neblock];\n      _sw32(dest - 4, -value);\n      continue;\n    }\n\n    maxout = neblock;\n  #if defined(HAVE_SNAPPY)\n    if (context->compcode == BLOSC_SNAPPY) {\n      maxout = (int32_t)snappy_max_compressed_length((size_t)neblock);\n    }\n  #endif /*  HAVE_SNAPPY */\n    if (ntbytes + maxout > maxbytes) {\n      /* avoid buffer * overrun */\n      maxout = (int64_t)maxbytes - (int64_t)ntbytes;\n      if (maxout <= 0) {\n        return 0;                  /* non-compressible block */\n      }\n    }\n    if (dict_training) {\n      // We are in the build dict state, so don't compress\n      // TODO: copy only a percentage for sampling\n      memcpy(dest, _src + j * neblock, (unsigned int)neblock);\n      cbytes = (int32_t)neblock;\n    }\n    else if (context->compcode == BLOSC_BLOSCLZ) {\n      cbytes = blosclz_compress(context->clevel, _src + j * neblock,\n                                (int)neblock, dest, (int)maxout);\n    }\n  #if defined(HAVE_LZ4)\n    else if (context->compcode == BLOSC_LZ4) {\n      void *hash_table = NULL;\n    #ifdef HAVE_IPP\n      hash_table = (void*)thread_context->lz4_hash_table;\n    #endif\n      cbytes = lz4_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                 (char*)dest, (size_t)maxout, accel, hash_table);\n    }\n    else if (context->compcode == BLOSC_LZ4HC) {\n      cbytes = lz4hc_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                   (char*)dest, (size_t)maxout, context->clevel);\n    }\n  #endif /* HAVE_LZ4 */\n  #if defined(HAVE_LIZARD)\n    else if (context->compcode == BLOSC_LIZARD) {\n      cbytes = lizard_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                    (char*)dest, (size_t)maxout, accel);\n    }\n  #endif /* HAVE_LIZARD */\n  #if defined(HAVE_SNAPPY)\n    else if (context->compcode == BLOSC_SNAPPY) {\n      cbytes = snappy_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                    (char*)dest, (size_t)maxout);\n    }\n  #endif /* HAVE_SNAPPY */\n  #if defined(HAVE_ZLIB)\n    else if (context->compcode == BLOSC_ZLIB) {\n      cbytes = zlib_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                  (char*)dest, (size_t)maxout, context->clevel);\n    }\n  #endif /* HAVE_ZLIB */\n  #if defined(HAVE_ZSTD)\n    else if (context->compcode == BLOSC_ZSTD) {\n      cbytes = zstd_wrap_compress(thread_context,\n                                  (char*)_src + j * neblock, (size_t)neblock,\n                                  (char*)dest, (size_t)maxout, context->clevel);\n    }\n  #endif /* HAVE_ZSTD */\n\n    else {\n      blosc_compcode_to_compname(context->compcode, &compname);\n      fprintf(stderr, \"Blosc has not been compiled with '%s' \", compname);\n      fprintf(stderr, \"compression support.  Please use one having it.\");\n      return -5;    /* signals no compression support */\n    }\n\n    if (cbytes > maxout) {\n      /* Buffer overrun caused by compression (should never happen) */\n      return -1;\n    }\n    if (cbytes < 0) {\n      /* cbytes should never be negative */\n      return -2;\n    }\n    if (!dict_training) {\n      if (cbytes == 0 || cbytes == neblock) {\n        /* The compressor has been unable to compress data at all. */\n        /* Before doing the copy, check that we are not running into a\n           buffer overflow. */\n        if ((ntbytes + neblock) > maxbytes) {\n          return 0;    /* Non-compressible data */\n        }\n        memcpy(dest, _src + j * neblock, (unsigned int)neblock);\n        cbytes = neblock;\n      }\n      _sw32(dest - 4, cbytes);\n    }\n    dest += cbytes;\n    ntbytes += cbytes;\n    ctbytes += cbytes;\n  }  /* Closes j < nstreams */\n\n  //printf(\"c%d\", ctbytes);\n  return ctbytes;\n}",
  "diff_func": "@@ -706,7 +706,7 @@ static bool get_run(const uint8_t* ip, const uint8_t* ip_bound) {\n \n /* Shuffle & compress a single block */\n static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n-                   int32_t leftoverblock, int32_t ntbytes, int32_t maxbytes,\n+                   int32_t leftoverblock, int32_t ntbytes, int32_t destsize,\n                    const uint8_t* src, const int32_t offset, uint8_t* dest,\n                    uint8_t* tmp, uint8_t* tmp2) {\n   blosc2_context* context = thread_context->parent_context;\n@@ -772,6 +772,10 @@ static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n     if (get_run(ip, ipbound)) {\n       // A run.  Encode the repeated byte as a negative length in the length of the split.\n       int32_t value = _src[j * neblock];\n+      if (ntbytes > destsize) {\n+        /* Not enough space to write out compressed block size */\n+        return -1;\n+      }\n       _sw32(dest - 4, -value);\n       continue;\n     }\n@@ -782,9 +786,9 @@ static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n       maxout = (int32_t)snappy_max_compressed_length((size_t)neblock);\n     }\n   #endif /*  HAVE_SNAPPY */\n-    if (ntbytes + maxout > maxbytes) {\n+    if (ntbytes + maxout > destsize) {\n       /* avoid buffer * overrun */\n-      maxout = (int64_t)maxbytes - (int64_t)ntbytes;\n+      maxout = (int64_t)destsize - (int64_t)ntbytes;\n       if (maxout <= 0) {\n         return 0;                  /* non-compressible block */\n       }\n@@ -859,7 +863,7 @@ static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n         /* The compressor has been unable to compress data at all. */\n         /* Before doing the copy, check that we are not running into a\n            buffer overflow. */\n-        if ((ntbytes + neblock) > maxbytes) {\n+        if ((ntbytes + neblock) > destsize) {\n           return 0;    /* Non-compressible data */\n         }\n         memcpy(dest, _src + j * neblock, (unsigned int)neblock);",
  "func": "static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n                   int32_t leftoverblock, int32_t ntbytes, int32_t maxbytes,\n                   const uint8_t* src, const int32_t offset, uint8_t* dest,\n                   uint8_t* tmp, uint8_t* tmp2) {\n  blosc2_context* context = thread_context->parent_context;\n  int dont_split = (context->header_flags & 0x10) >> 4;\n  int dict_training = context->use_dict && context->dict_cdict == NULL;\n  int32_t j, neblock, nstreams;\n  int32_t cbytes;                   /* number of compressed bytes in split */\n  int32_t ctbytes = 0;              /* number of compressed bytes in block */\n  int64_t maxout;\n  int32_t typesize = context->typesize;\n  const char* compname;\n  int accel;\n  const uint8_t* _src;\n  uint8_t *_tmp = tmp, *_tmp2 = tmp2;\n  uint8_t *_tmp3 = thread_context->tmp4;\n  int last_filter_index = last_filter(context->filters, 'c');\n  bool memcpyed = context->header_flags & (uint8_t)BLOSC_MEMCPYED;\n\n  if (last_filter_index >= 0 || context->prefilter != NULL) {\n    /* Apply the filter pipeline just for the prefilter */\n    if (memcpyed && context->prefilter != NULL) {\n      // We only need the prefilter output\n      _src = pipeline_c(thread_context, bsize, src, offset, dest, _tmp2, _tmp3);\n\n      if (_src == NULL) {\n        return -9;  // signals a problem with the filter pipeline\n      }\n      return bsize;\n    }\n    /* Apply regular filter pipeline */\n    _src = pipeline_c(thread_context, bsize, src, offset, _tmp, _tmp2, _tmp3);\n\n    if (_src == NULL) {\n      return -9;  // signals a problem with the filter pipeline\n    }\n  } else {\n    _src = src + offset;\n  }\n\n  assert(context->clevel > 0);\n\n  /* Calculate acceleration for different compressors */\n  accel = get_accel(context);\n\n  /* The number of compressed data streams for this block */\n  if (!dont_split && !leftoverblock && !dict_training) {\n    nstreams = (int32_t)typesize;\n  }\n  else {\n    nstreams = 1;\n  }\n  neblock = bsize / nstreams;\n  for (j = 0; j < nstreams; j++) {\n    if (!dict_training) {\n      dest += sizeof(int32_t);\n      ntbytes += sizeof(int32_t);\n      ctbytes += sizeof(int32_t);\n    }\n\n    // See if we have a run here\n    const uint8_t* ip = (uint8_t*)_src + j * neblock;\n    const uint8_t* ipbound = (uint8_t*)_src + (j + 1) * neblock;\n    if (get_run(ip, ipbound)) {\n      // A run.  Encode the repeated byte as a negative length in the length of the split.\n      int32_t value = _src[j * neblock];\n      _sw32(dest - 4, -value);\n      continue;\n    }\n\n    maxout = neblock;\n  #if defined(HAVE_SNAPPY)\n    if (context->compcode == BLOSC_SNAPPY) {\n      maxout = (int32_t)snappy_max_compressed_length((size_t)neblock);\n    }\n  #endif /*  HAVE_SNAPPY */\n    if (ntbytes + maxout > maxbytes) {\n      /* avoid buffer * overrun */\n      maxout = (int64_t)maxbytes - (int64_t)ntbytes;\n      if (maxout <= 0) {\n        return 0;                  /* non-compressible block */\n      }\n    }\n    if (dict_training) {\n      // We are in the build dict state, so don't compress\n      // TODO: copy only a percentage for sampling\n      memcpy(dest, _src + j * neblock, (unsigned int)neblock);\n      cbytes = (int32_t)neblock;\n    }\n    else if (context->compcode == BLOSC_BLOSCLZ) {\n      cbytes = blosclz_compress(context->clevel, _src + j * neblock,\n                                (int)neblock, dest, (int)maxout);\n    }\n  #if defined(HAVE_LZ4)\n    else if (context->compcode == BLOSC_LZ4) {\n      void *hash_table = NULL;\n    #ifdef HAVE_IPP\n      hash_table = (void*)thread_context->lz4_hash_table;\n    #endif\n      cbytes = lz4_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                 (char*)dest, (size_t)maxout, accel, hash_table);\n    }\n    else if (context->compcode == BLOSC_LZ4HC) {\n      cbytes = lz4hc_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                   (char*)dest, (size_t)maxout, context->clevel);\n    }\n  #endif /* HAVE_LZ4 */\n  #if defined(HAVE_LIZARD)\n    else if (context->compcode == BLOSC_LIZARD) {\n      cbytes = lizard_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                    (char*)dest, (size_t)maxout, accel);\n    }\n  #endif /* HAVE_LIZARD */\n  #if defined(HAVE_SNAPPY)\n    else if (context->compcode == BLOSC_SNAPPY) {\n      cbytes = snappy_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                    (char*)dest, (size_t)maxout);\n    }\n  #endif /* HAVE_SNAPPY */\n  #if defined(HAVE_ZLIB)\n    else if (context->compcode == BLOSC_ZLIB) {\n      cbytes = zlib_wrap_compress((char*)_src + j * neblock, (size_t)neblock,\n                                  (char*)dest, (size_t)maxout, context->clevel);\n    }\n  #endif /* HAVE_ZLIB */\n  #if defined(HAVE_ZSTD)\n    else if (context->compcode == BLOSC_ZSTD) {\n      cbytes = zstd_wrap_compress(thread_context,\n                                  (char*)_src + j * neblock, (size_t)neblock,\n                                  (char*)dest, (size_t)maxout, context->clevel);\n    }\n  #endif /* HAVE_ZSTD */\n\n    else {\n      blosc_compcode_to_compname(context->compcode, &compname);\n      fprintf(stderr, \"Blosc has not been compiled with '%s' \", compname);\n      fprintf(stderr, \"compression support.  Please use one having it.\");\n      return -5;    /* signals no compression support */\n    }\n\n    if (cbytes > maxout) {\n      /* Buffer overrun caused by compression (should never happen) */\n      return -1;\n    }\n    if (cbytes < 0) {\n      /* cbytes should never be negative */\n      return -2;\n    }\n    if (!dict_training) {\n      if (cbytes == 0 || cbytes == neblock) {\n        /* The compressor has been unable to compress data at all. */\n        /* Before doing the copy, check that we are not running into a\n           buffer overflow. */\n        if ((ntbytes + neblock) > maxbytes) {\n          return 0;    /* Non-compressible data */\n        }\n        memcpy(dest, _src + j * neblock, (unsigned int)neblock);\n        cbytes = neblock;\n      }\n      _sw32(dest - 4, cbytes);\n    }\n    dest += cbytes;\n    ntbytes += cbytes;\n    ctbytes += cbytes;\n  }  /* Closes j < nstreams */\n\n  //printf(\"c%d\", ctbytes);\n  return ctbytes;\n}",
  "project": "c-blosc2",
  "hash": 33213175756612861371893504542614898630,
  "size": 170,
  "commit_id": "c4c6470e88210afc95262c8b9fcc27e30ca043ee",
  "message": "Fixed asan heap buffer overflow when not enough space to write compressed block size.",
  "target": 1,
  "dataset": "other",
  "idx": 200450
}