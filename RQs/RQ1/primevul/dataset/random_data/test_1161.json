{
  "id": 1161,
  "language": "c",
  "cwe": [
    "CWE-20"
  ],
  "commit_url": "https://github.com/malvira/lpc31xx/commit/89f5b7da2a6bad2e84670422ab8192382a5aeb9f",
  "commit_sha": "89f5b7da2a6bad2e84670422ab8192382a5aeb9f",
  "commit_msg": "Reinstate ZERO_PAGE optimization in 'get_user_pages()' and fix XIP\n\nKAMEZAWA Hiroyuki and Oleg Nesterov point out that since the commit\n557ed1fa2620dc119adb86b34c614e152a629a80 (\"remove ZERO_PAGE\") removed\nthe ZERO_PAGE from the VM mappings, any users of get_user_pages() will\ngenerally now populate the VM with real empty pages needlessly.\n\nWe used to get the ZERO_PAGE when we did the \"handle_mm_fault()\", but\nsince fault handling no longer uses ZERO_PAGE for new anonymous pages,\nwe now need to handle that special case in follow_page() instead.\n\nIn particular, the removal of ZERO_PAGE effectively removed the core\nfile writing optimization where we would skip writing pages that had not\nbeen populated at all, and increased memory pressure a lot by allocating\nall those useless newly zeroed pages.\n\nThis reinstates the optimization by making the unmapped PTE case the\nsame as for a non-existent page table, which already did this correctly.\n\nWhile at it, this also fixes the XIP case for follow_page(), where the\ncaller could not differentiate between the case of a page that simply\ncould not be used (because it had no \"struct page\" associated with it)\nand a page that just wasn't mapped.\n\nWe do that by simply returning an error pointer for pages that could not\nbe turned into a \"struct page *\".  The error is arbitrarily picked to be\nEFAULT, since that was what get_user_pages() already used for the\nequivalent IO-mapped page case.\n\n[ Also removed an impossible test for pte_offset_map_lock() failing:\n  that's not how that function works ]\n\nAcked-by: Oleg Nesterov <oleg@tv-sign.ru>\nAcked-by: Nick Piggin <npiggin@suse.de>\nCc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>\nCc: Hugh Dickins <hugh@veritas.com>\nCc: Andrew Morton <akpm@linux-foundation.org>\nCc: Ingo Molnar <mingo@elte.hu>\nCc: Roland McGrath <roland@redhat.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "pr_url": null,
  "pr_info": null,
  "file_name": "arch/powerpc/kernel/vdso.c",
  "func_name": "",
  "raw_func_from_json": "int get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, int len, int write, int force,\n\t\tstruct page **pages, struct vm_area_struct **vmas)\n{\n\tint i;\n\tunsigned int vm_flags;\n\n\tif (len <= 0)\n\t\treturn 0;\n\t/* \n\t * Require read or write permissions.\n\t * If 'force' is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = write ? (VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= force ? (VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\ti = 0;\n\n\tdo {\n\t\tstruct vm_area_struct *vma;\n\t\tunsigned int foll_flags;\n\n\t\tvma = find_extend_vma(mm, start);\n\t\tif (!vma && in_gate_area(tsk, start)) {\n\t\t\tunsigned long pg = start & PAGE_MASK;\n\t\t\tstruct vm_area_struct *gate_vma = get_gate_vma(tsk);\n\t\t\tpgd_t *pgd;\n\t\t\tpud_t *pud;\n\t\t\tpmd_t *pmd;\n\t\t\tpte_t *pte;\n\t\t\tif (write) /* user gate pages are read-only */\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\tif (pg > TASK_SIZE)\n\t\t\t\tpgd = pgd_offset_k(pg);\n\t\t\telse\n\t\t\t\tpgd = pgd_offset_gate(mm, pg);\n\t\t\tBUG_ON(pgd_none(*pgd));\n\t\t\tpud = pud_offset(pgd, pg);\n\t\t\tBUG_ON(pud_none(*pud));\n\t\t\tpmd = pmd_offset(pud, pg);\n\t\t\tif (pmd_none(*pmd))\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\tpte = pte_offset_map(pmd, pg);\n\t\t\tif (pte_none(*pte)) {\n\t\t\t\tpte_unmap(pte);\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\t}\n\t\t\tif (pages) {\n\t\t\t\tstruct page *page = vm_normal_page(gate_vma, start, *pte);\n\t\t\t\tpages[i] = page;\n\t\t\t\tif (page)\n\t\t\t\t\tget_page(page);\n\t\t\t}\n\t\t\tpte_unmap(pte);\n\t\t\tif (vmas)\n\t\t\t\tvmas[i] = gate_vma;\n\t\t\ti++;\n\t\t\tstart += PAGE_SIZE;\n\t\t\tlen--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!vma || (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\t\t\t|| !(vm_flags & vma->vm_flags))\n\t\t\treturn i ? : -EFAULT;\n\n\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &len, i, write);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfoll_flags = FOLL_TOUCH;\n\t\tif (pages)\n\t\t\tfoll_flags |= FOLL_GET;\n\t\tif (!write && !(vma->vm_flags & VM_LOCKED) &&\n\t\t    (!vma->vm_ops || !vma->vm_ops->fault))\n\t\t\tfoll_flags |= FOLL_ANON;\n\n\t\tdo {\n\t\t\tstruct page *page;\n\n\t\t\t/*\n\t\t\t * If tsk is ooming, cut off its access to large memory\n\t\t\t * allocations. It has a pending SIGKILL, but it can't\n\t\t\t * be processed until returning to user space.\n\t\t\t */\n\t\t\tif (unlikely(test_tsk_thread_flag(tsk, TIF_MEMDIE)))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (write)\n\t\t\t\tfoll_flags |= FOLL_WRITE;\n\n\t\t\tcond_resched();\n\t\t\twhile (!(page = follow_page(vma, start, foll_flags))) {\n\t\t\t\tint ret;\n\t\t\t\tret = handle_mm_fault(mm, vma, start,\n\t\t\t\t\t\tfoll_flags & FOLL_WRITE);\n\t\t\t\tif (ret & VM_FAULT_ERROR) {\n\t\t\t\t\tif (ret & VM_FAULT_OOM)\n\t\t\t\t\t\treturn i ? i : -ENOMEM;\n\t\t\t\t\telse if (ret & VM_FAULT_SIGBUS)\n\t\t\t\t\t\treturn i ? i : -EFAULT;\n\t\t\t\t\tBUG();\n\t\t\t\t}\n\t\t\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\t\t\ttsk->maj_flt++;\n\t\t\t\telse\n\t\t\t\t\ttsk->min_flt++;\n\n\t\t\t\t/*\n\t\t\t\t * The VM_FAULT_WRITE bit tells us that\n\t\t\t\t * do_wp_page has broken COW when necessary,\n\t\t\t\t * even if maybe_mkwrite decided not to set\n\t\t\t\t * pte_write. We can thus safely do subsequent\n\t\t\t\t * page lookups as if they were reads.\n\t\t\t\t */\n\t\t\t\tif (ret & VM_FAULT_WRITE)\n\t\t\t\t\tfoll_flags &= ~FOLL_WRITE;\n\n\t\t\t\tcond_resched();\n\t\t\t}\n\t\t\tif (pages) {\n\t\t\t\tpages[i] = page;\n\n\t\t\t\tflush_anon_page(vma, page, start);\n\t\t\t\tflush_dcache_page(page);\n\t\t\t}\n\t\t\tif (vmas)\n\t\t\t\tvmas[i] = vma;\n\t\t\ti++;\n\t\t\tstart += PAGE_SIZE;\n\t\t\tlen--;\n\t\t} while (len && start < vma->vm_end);\n\t} while (len);\n\treturn i;\n}",
  "diff_func": "@@ -142,7 +142,7 @@ static void dump_one_vdso_page(struct page *pg, struct page *upg)\n \tprintk(\"kpg: %p (c:%d,f:%08lx)\", __va(page_to_pfn(pg) << PAGE_SHIFT),\n \t       page_count(pg),\n \t       pg->flags);\n-\tif (upg/* && pg != upg*/) {\n+\tif (upg && !IS_ERR(upg) /* && pg != upg*/) {\n \t\tprintk(\" upg: %p (c:%d,f:%08lx)\", __va(page_to_pfn(upg)\n \t\t\t\t\t\t       << PAGE_SHIFT),\n \t\t       page_count(upg),",
  "func": "int get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, int len, int write, int force,\n\t\tstruct page **pages, struct vm_area_struct **vmas)\n{\n\tint i;\n\tunsigned int vm_flags;\n\n\tif (len <= 0)\n\t\treturn 0;\n\t/* \n\t * Require read or write permissions.\n\t * If 'force' is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = write ? (VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= force ? (VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\ti = 0;\n\n\tdo {\n\t\tstruct vm_area_struct *vma;\n\t\tunsigned int foll_flags;\n\n\t\tvma = find_extend_vma(mm, start);\n\t\tif (!vma && in_gate_area(tsk, start)) {\n\t\t\tunsigned long pg = start & PAGE_MASK;\n\t\t\tstruct vm_area_struct *gate_vma = get_gate_vma(tsk);\n\t\t\tpgd_t *pgd;\n\t\t\tpud_t *pud;\n\t\t\tpmd_t *pmd;\n\t\t\tpte_t *pte;\n\t\t\tif (write) /* user gate pages are read-only */\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\tif (pg > TASK_SIZE)\n\t\t\t\tpgd = pgd_offset_k(pg);\n\t\t\telse\n\t\t\t\tpgd = pgd_offset_gate(mm, pg);\n\t\t\tBUG_ON(pgd_none(*pgd));\n\t\t\tpud = pud_offset(pgd, pg);\n\t\t\tBUG_ON(pud_none(*pud));\n\t\t\tpmd = pmd_offset(pud, pg);\n\t\t\tif (pmd_none(*pmd))\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\tpte = pte_offset_map(pmd, pg);\n\t\t\tif (pte_none(*pte)) {\n\t\t\t\tpte_unmap(pte);\n\t\t\t\treturn i ? : -EFAULT;\n\t\t\t}\n\t\t\tif (pages) {\n\t\t\t\tstruct page *page = vm_normal_page(gate_vma, start, *pte);\n\t\t\t\tpages[i] = page;\n\t\t\t\tif (page)\n\t\t\t\t\tget_page(page);\n\t\t\t}\n\t\t\tpte_unmap(pte);\n\t\t\tif (vmas)\n\t\t\t\tvmas[i] = gate_vma;\n\t\t\ti++;\n\t\t\tstart += PAGE_SIZE;\n\t\t\tlen--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!vma || (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\t\t\t|| !(vm_flags & vma->vm_flags))\n\t\t\treturn i ? : -EFAULT;\n\n\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &len, i, write);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfoll_flags = FOLL_TOUCH;\n\t\tif (pages)\n\t\t\tfoll_flags |= FOLL_GET;\n\t\tif (!write && !(vma->vm_flags & VM_LOCKED) &&\n\t\t    (!vma->vm_ops || !vma->vm_ops->fault))\n\t\t\tfoll_flags |= FOLL_ANON;\n\n\t\tdo {\n\t\t\tstruct page *page;\n\n\t\t\t/*\n\t\t\t * If tsk is ooming, cut off its access to large memory\n\t\t\t * allocations. It has a pending SIGKILL, but it can't\n\t\t\t * be processed until returning to user space.\n\t\t\t */\n\t\t\tif (unlikely(test_tsk_thread_flag(tsk, TIF_MEMDIE)))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (write)\n\t\t\t\tfoll_flags |= FOLL_WRITE;\n\n\t\t\tcond_resched();\n\t\t\twhile (!(page = follow_page(vma, start, foll_flags))) {\n\t\t\t\tint ret;\n\t\t\t\tret = handle_mm_fault(mm, vma, start,\n\t\t\t\t\t\tfoll_flags & FOLL_WRITE);\n\t\t\t\tif (ret & VM_FAULT_ERROR) {\n\t\t\t\t\tif (ret & VM_FAULT_OOM)\n\t\t\t\t\t\treturn i ? i : -ENOMEM;\n\t\t\t\t\telse if (ret & VM_FAULT_SIGBUS)\n\t\t\t\t\t\treturn i ? i : -EFAULT;\n\t\t\t\t\tBUG();\n\t\t\t\t}\n\t\t\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\t\t\ttsk->maj_flt++;\n\t\t\t\telse\n\t\t\t\t\ttsk->min_flt++;\n\n\t\t\t\t/*\n\t\t\t\t * The VM_FAULT_WRITE bit tells us that\n\t\t\t\t * do_wp_page has broken COW when necessary,\n\t\t\t\t * even if maybe_mkwrite decided not to set\n\t\t\t\t * pte_write. We can thus safely do subsequent\n\t\t\t\t * page lookups as if they were reads.\n\t\t\t\t */\n\t\t\t\tif (ret & VM_FAULT_WRITE)\n\t\t\t\t\tfoll_flags &= ~FOLL_WRITE;\n\n\t\t\t\tcond_resched();\n\t\t\t}\n\t\t\tif (pages) {\n\t\t\t\tpages[i] = page;\n\n\t\t\t\tflush_anon_page(vma, page, start);\n\t\t\t\tflush_dcache_page(page);\n\t\t\t}\n\t\t\tif (vmas)\n\t\t\t\tvmas[i] = vma;\n\t\t\ti++;\n\t\t\tstart += PAGE_SIZE;\n\t\t\tlen--;\n\t\t} while (len && start < vma->vm_end);\n\t} while (len);\n\treturn i;\n}",
  "target": 1,
  "project": "linux-2.6",
  "commit_id": "89f5b7da2a6bad2e84670422ab8192382a5aeb9f",
  "hash": 192373302205049669343839180384526250684,
  "size": 136,
  "message": "Reinstate ZERO_PAGE optimization in 'get_user_pages()' and fix XIP\n\nKAMEZAWA Hiroyuki and Oleg Nesterov point out that since the commit\n557ed1fa2620dc119adb86b34c614e152a629a80 (\"remove ZERO_PAGE\") removed\nthe ZERO_PAGE from the VM mappings, any users of get_user_pages() will\ngenerally now populate the VM with real empty pages needlessly.\n\nWe used to get the ZERO_PAGE when we did the \"handle_mm_fault()\", but\nsince fault handling no longer uses ZERO_PAGE for new anonymous pages,\nwe now need to handle that special case in follow_page() instead.\n\nIn particular, the removal of ZERO_PAGE effectively removed the core\nfile writing optimization where we would skip writing pages that had not\nbeen populated at all, and increased memory pressure a lot by allocating\nall those useless newly zeroed pages.\n\nThis reinstates the optimization by making the unmapped PTE case the\nsame as for a non-existent page table, which already did this correctly.\n\nWhile at it, this also fixes the XIP case for follow_page(), where the\ncaller could not differentiate between the case of a page that simply\ncould not be used (because it had no \"struct page\" associated with it)\nand a page that just wasn't mapped.\n\nWe do that by simply returning an error pointer for pages that could not\nbe turned into a \"struct page *\".  The error is arbitrarily picked to be\nEFAULT, since that was what get_user_pages() already used for the\nequivalent IO-mapped page case.\n\n[ Also removed an impossible test for pte_offset_map_lock() failing:\n  that's not how that function works ]\n\nAcked-by: Oleg Nesterov <oleg@tv-sign.ru>\nAcked-by: Nick Piggin <npiggin@suse.de>\nCc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>\nCc: Hugh Dickins <hugh@veritas.com>\nCc: Andrew Morton <akpm@linux-foundation.org>\nCc: Ingo Molnar <mingo@elte.hu>\nCc: Roland McGrath <roland@redhat.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "dataset": "other",
  "idx": 215342
}