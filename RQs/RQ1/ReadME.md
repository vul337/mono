# RQ1 : CleanVul, PrimeVul, ReposVul, and MONOLENS

This README outlines the methodologies and datasets used in our research, focusing on the preparation and analysis of vulnerabilities across different sources.

## CleanVul & PrimeVul

For our analysis, we utilized a merged dataset comprising **CleanVul (Level 4)** and **PrimeVul**. From this combined dataset, we randomly sampled **500 files from each** (CleanVul and PrimeVul), resulting in a total of 1000 files. For each of these selected files, we meticulously collected their associated **Pull Request (PR) discussions**.

Subsequent experiments were conducted using the consistent experimental settings applied across all runs.

## MONOLENS

Our approach to the MONOLENS dataset involved a targeted selection of samples for both vulnerable (Vul) and non-vulnerable (NonVul) categories.

### Vulnerable (Vul) Samples

* From **C patches**, we randomly selected **260 instances**.
* From **Java patches**, we randomly selected **90 instances**.

### Non-Vulnerable (NonVul) Samples

* We merged all available non-vulnerable samples and then randomly sampled **150 instances** from this consolidated set.

Crucially, for the MONOLENS dataset, the **labels generated by the Language Model (LLM) were concealed** from the annotators. The final labeling process was performed through **manual annotation** to ensure high-quality ground truth.


## ReposVul

We opted **not to use the ReposVul dataset** for our study. Upon processing its dataset into individual JSON files, we observed a significant imbalance: the proportion of **positive samples (labeled as vulnerable)** constituted only **10%** of the total dataset, which comprised over 6000 entries. This low percentage of positive samples would likely lead to skewed experimental results and challenges in robust model training.
