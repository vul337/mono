{
  "id": 3035,
  "language": "Java",
  "commit_url": "https://github.com/bazelbuild/bazel/commit/abb402e82fa680f5b22fdf54e978bc6a644209e4",
  "commit_sha": "abb402e82fa680f5b22fdf54e978bc6a644209e4",
  "commit_msg": "Handle crashes when `SkyFunctionEnvironment#batchPrefetch()` cannot find dependency for the given `SkyKey`\n\nhttps://github.com/bazelbuild/bazel/commit/658ba15d9f37969edfaae507d267ee3aaba8b44e introduced flag `--experimental_heuristically_drop_nodes` and started to drop `FILE_STATE` or `DIRECTORY_LISTING_STATE` node after the corresponding `FILE` or `DIRECTORY_LISTING` node is evaluated. However, this causes blaze crash due to `SkyFunctionEnvironment#batchPrefetch()` method currently has a hard `checkNotNull` check on node dependency.\n\nIn order to gracefully avoid crash, this CL implements the following features:\n\n* Enhance `GraphInconsistencyReceiver` to tolerate inconsistency caused by heuristically dropping state nodes:\n  * Implement `isExpectedInconsistency` static method which checks the input inconsistency is caused by heuristically dropping state nodes;\n  * Create a new `NodeDroppingInconsistencyReceiver` to replace `GraphInconsistencyReceiver.THROWING` when `--experimental_heuristically_drop_nodes` is enabled;\n  * Enhance `RewindableGraphInconsistencyReceiver` to reflect additional tolerance in case `--experimental_heuristically_drop_nodes` is enabled;\n* Update `SkyFunctionEnvironment#batchPrefetch()` method so as not to crash blaze by utilizing the updated `GraphInconsistencyReceiver`\n\nIntegration tests are added to both `SkyframeIntegrationTest` and `SkybuildV2NoBuildIntegrationTest` to verify that blaze will not crash when needed file state file is missing.\n\nPiperOrigin-RevId: 498178625\nChange-Id: I76ed00a1b03e90193c1484b79e6651a3980d64b6",
  "pr_url": null,
  "pr_info": "no more info",
  "file_name": "src/main/java/com/google/devtools/build/skyframe/AbstractParallelEvaluator.java",
  "func_name": "run",
  "func_before": "@Override\n    public void run() {\n      SkyFunctionEnvironment env = null;\n      try {\n        NodeEntry nodeEntry = checkNotNull(graph.get(null, Reason.EVALUATION, skyKey), skyKey);\n        checkState(nodeEntry.isReady(), \"%s %s\", skyKey, nodeEntry);\n        try {\n          evaluatorContext.getProgressReceiver().stateStarting(skyKey, NodeState.CHECK_DIRTY);\n          if (maybeHandleDirtyNode(nodeEntry) == DirtyOutcome.ALREADY_PROCESSED) {\n            return;\n          }\n        } finally {\n          evaluatorContext.getProgressReceiver().stateEnding(skyKey, NodeState.CHECK_DIRTY);\n        }\n\n        ImmutableSet<SkyKey> oldDeps = nodeEntry.getAllRemainingDirtyDirectDeps();\n        try {\n          evaluatorContext\n              .getProgressReceiver()\n              .stateStarting(skyKey, NodeState.INITIALIZING_ENVIRONMENT);\n          env =\n              SkyFunctionEnvironment.create(\n                  skyKey, nodeEntry.getTemporaryDirectDeps(), oldDeps, evaluatorContext);\n        } catch (UndonePreviouslyRequestedDep undonePreviouslyRequestedDep) {\n          // If a previously requested dep is no longer done, restart this node from scratch.\n          stateCache.invalidate(skyKey);\n          restart(skyKey, nodeEntry);\n          evaluatorContext.getVisitor().enqueueEvaluation(skyKey, determineRestartPriority());\n          return;\n        } finally {\n          evaluatorContext\n              .getProgressReceiver()\n              .stateEnding(skyKey, NodeState.INITIALIZING_ENVIRONMENT);\n        }\n        SkyFunctionName functionName = skyKey.functionName();\n        SkyFunction skyFunction =\n            checkNotNull(\n                evaluatorContext.getSkyFunctions().get(functionName),\n                \"Unable to find SkyFunction '%s' for node with key %s, %s\",\n                functionName,\n                skyKey,\n                nodeEntry);\n\n        SkyValue value = null;\n        long startTimeNanos = BlazeClock.instance().nanoTime();\n        try {\n          try {\n            evaluatorContext.getProgressReceiver().stateStarting(skyKey, NodeState.COMPUTE);\n            value = skyFunction.compute(skyKey, env);\n          } finally {\n            evaluatorContext.getProgressReceiver().stateEnding(skyKey, NodeState.COMPUTE);\n            long elapsedTimeNanos = BlazeClock.instance().nanoTime() - startTimeNanos;\n            if (elapsedTimeNanos > 0) {\n              Profiler.instance()\n                  .logSimpleTaskDuration(\n                      startTimeNanos,\n                      Duration.ofNanos(elapsedTimeNanos),\n                      ProfilerTask.SKYFUNCTION,\n                      skyKey.functionName().getName());\n            }\n          }\n        } catch (final SkyFunctionException builderException) {\n          stateCache.invalidate(skyKey);\n\n          ReifiedSkyFunctionException reifiedBuilderException =\n              new ReifiedSkyFunctionException(builderException);\n          // In keep-going mode, we do not let SkyFunctions complete with a thrown error if they\n          // have missing deps. Instead, we wait until their deps are done and restart the\n          // SkyFunction, so we can have a definitive error and definitive graph structure, thus\n          // avoiding non-determinism. It's completely reasonable for SkyFunctions to throw eagerly\n          // because they do not know if they are in keep-going mode.\n          if (!evaluatorContext.keepGoing() || !env.valuesMissing()) {\n            if (maybeHandleRegisteringNewlyDiscoveredDepsForDoneEntry(\n                skyKey, nodeEntry, oldDeps, env, evaluatorContext.keepGoing())) {\n              // A newly requested dep transitioned from done to dirty before this node finished.\n              // It is not safe to set the error because the now-dirty dep has not signaled this\n              // node. We return (without preventing new evaluations) so that the dep can complete\n              // and signal this node.\n              return;\n            }\n\n            boolean shouldFailFast =\n                !evaluatorContext.keepGoing() || builderException.isCatastrophic();\n            if (shouldFailFast) {\n              // After we commit this error to the graph but before the doMutatingEvaluation call\n              // completes with the error there is a race-like opportunity for the error to be used,\n              // either by an in-flight computation or by a future computation.\n              if (!evaluatorContext.getVisitor().preventNewEvaluations()) {\n                // This is not the first error encountered, so we ignore it so that we can terminate\n                // with the first error.\n                return;\n              } else {\n                logger.atWarning().withCause(builderException).log(\n                    \"Aborting evaluation while evaluating %s\", skyKey);\n              }\n            }\n            boolean isTransitivelyTransient =\n                reifiedBuilderException.isTransient()\n                    || env.isAnyDirectDepErrorTransitivelyTransient()\n                    || env.isAnyNewlyRequestedDepErrorTransitivelyTransient();\n            ErrorInfo errorInfo =\n                evaluatorContext\n                    .getErrorInfoManager()\n                    .fromException(skyKey, reifiedBuilderException, isTransitivelyTransient);\n            env.setError(nodeEntry, errorInfo);\n            Set<SkyKey> rdepsToBubbleUpTo = env.commitAndGetParents(nodeEntry);\n            if (shouldFailFast) {\n              evaluatorContext.signalParentsOnAbort(\n                  skyKey, rdepsToBubbleUpTo, nodeEntry.getVersion());\n              throw SchedulerException.ofError(errorInfo, skyKey, rdepsToBubbleUpTo);\n            }\n            evaluatorContext.signalParentsAndEnqueueIfReady(\n                skyKey, rdepsToBubbleUpTo, nodeEntry.getVersion(), determineRestartPriority());\n            return;\n          }\n        } catch (RuntimeException re) {\n          // Programmer error (most likely NPE or a failed precondition in a SkyFunction). Output\n          // some context together with the exception.\n          String msg = prepareCrashMessage(skyKey, nodeEntry.getInProgressReverseDeps());\n          RuntimeException ex = new RuntimeException(msg, re);\n          evaluatorContext.getVisitor().noteCrash(ex);\n          throw ex;\n        } finally {\n          env.doneBuilding();\n        }\n\n        if (maybeHandleRestart(skyKey, nodeEntry, value)) {\n          stateCache.invalidate(skyKey);\n          cancelExternalDeps(env);\n          evaluatorContext.getVisitor().enqueueEvaluation(skyKey, determineRestartPriority());\n          return;\n        }\n\n        Set<SkyKey> newDeps = env.getNewlyRequestedDeps();\n        if (value != null) {\n          stateCache.invalidate(skyKey);\n\n          checkState(\n              !env.valuesMissing(),\n              \"Evaluation of %s returned non-null value but requested dependencies that weren't \"\n                  + \"computed yet (one of %s), NodeEntry: %s\",\n              skyKey,\n              newDeps,\n              nodeEntry);\n\n          try {\n            evaluatorContext.getProgressReceiver().stateStarting(skyKey, NodeState.COMMIT);\n            if (maybeHandleRegisteringNewlyDiscoveredDepsForDoneEntry(\n                skyKey, nodeEntry, oldDeps, env, evaluatorContext.keepGoing())) {\n              // A newly requested dep transitioned from done to dirty before this node finished.\n              // This node will be signalled again, and so we should return.\n              return;\n            }\n            env.setValue(value);\n            Set<SkyKey> reverseDeps = env.commitAndGetParents(nodeEntry);\n            evaluatorContext.signalParentsAndEnqueueIfReady(\n                skyKey, reverseDeps, nodeEntry.getVersion(), determineRestartPriority());\n          } finally {\n            evaluatorContext.getProgressReceiver().stateEnding(skyKey, NodeState.COMMIT);\n          }\n          return;\n        }\n\n        SkyKey childErrorKey = env.getDepErrorKey();\n        if (childErrorKey != null) {\n          checkState(!evaluatorContext.keepGoing(), \"%s %s %s\", skyKey, nodeEntry, childErrorKey);\n          // We encountered a child error in noKeepGoing mode, so we want to fail fast. But we first\n          // need to add the edge between the current node and the child error it requested so that\n          // error bubbling can occur. Note that this edge will subsequently be removed during graph\n          // cleaning (since the current node will never be committed to the graph).\n          NodeEntry childErrorEntry =\n              checkNotNull(\n                  graph.get(skyKey, Reason.OTHER, childErrorKey),\n                  \"skyKey: %s, nodeEntry: %s childErrorKey: %s\",\n                  skyKey,\n                  nodeEntry,\n                  childErrorKey);\n          if (newDeps.contains(childErrorKey)) {\n            // Add this dep if it was just requested. In certain rare race conditions (see\n            // MemoizingEvaluatorTest.cachedErrorCausesRestart) this dep may have already been\n            // requested.\n            nodeEntry.addSingletonTemporaryDirectDep(childErrorKey);\n            DependencyState childErrorState;\n            if (oldDeps.contains(childErrorKey)) {\n              childErrorState = childErrorEntry.checkIfDoneForDirtyReverseDep(skyKey);\n            } else {\n              childErrorState = childErrorEntry.addReverseDepAndCheckIfDone(skyKey);\n            }\n            if (childErrorState != DependencyState.DONE) {\n              // The child in error may have transitioned from done to dirty between when this node\n              // discovered the error and now. Notify the graph inconsistency receiver so that we\n              // can crash if that's unexpected.\n              // We don't enqueue the child, even if it returns NEEDS_SCHEDULING, because we are\n              // about to shut down evaluation.\n              evaluatorContext\n                  .getGraphInconsistencyReceiver()\n                  .noteInconsistencyAndMaybeThrow(\n                      skyKey,\n                      ImmutableList.of(childErrorKey),\n                      Inconsistency.BUILDING_PARENT_FOUND_UNDONE_CHILD);\n            }\n          }\n          SkyValue childErrorInfoMaybe =\n              checkNotNull(\n                  env.maybeGetValueFromErrorOrDeps(childErrorKey),\n                  \"dep error found but then lost while building: %s %s\",\n                  skyKey,\n                  childErrorKey);\n          ErrorInfo childErrorInfo =\n              checkNotNull(\n                  ValueWithMetadata.getMaybeErrorInfo(childErrorInfoMaybe),\n                  \"dep error found but then wasn't an error while building: %s %s %s\",\n                  skyKey,\n                  childErrorKey,\n                  childErrorInfoMaybe);\n          evaluatorContext.getVisitor().preventNewEvaluations();\n          // TODO(b/166268889): Remove when fixed.\n          if (childErrorInfo.getException() instanceof IOException) {\n            logger.atInfo().withCause(childErrorInfo.getException()).log(\n                \"Child %s with IOException forced abort of %s\", childErrorKey, skyKey);\n          }\n          throw SchedulerException.ofError(childErrorInfo, childErrorKey, ImmutableSet.of(skyKey));\n        }\n\n        // TODO(bazel-team): This code is not safe to interrupt, because we would lose the state in\n        // newDirectDeps.\n\n        // TODO(bazel-team): An ill-behaved SkyFunction can throw us into an infinite loop where we\n        // add more dependencies on every run. [skyframe-core]\n\n        // Add all the newly requested dependencies to the temporary direct deps. Note that\n        // newDirectDeps does not contain any elements in common with the already existing temporary\n        // direct deps. uniqueNewDeps will be the set of unique keys contained in newDirectDeps.\n        env.addTemporaryDirectDepsTo(nodeEntry);\n\n        List<ListenableFuture<?>> externalDeps = env.externalDeps;\n        // If there were no newly requested dependencies, at least one of them was in error or there\n        // is a bug in the SkyFunction implementation. The environment has collected its errors, so\n        // we just order it to be built.\n        if (newDeps.isEmpty() && externalDeps == null) {\n          // TODO(bazel-team): This means a bug in the SkyFunction. What to do?\n          checkState(\n              !env.getChildErrorInfos().isEmpty(),\n              \"Evaluation of SkyKey failed and no dependencies were requested: %s %s\",\n              skyKey,\n              nodeEntry);\n          checkState(\n              evaluatorContext.keepGoing(),\n              \"nokeep_going evaluation should have failed on first child error: %s %s %s\",\n              skyKey,\n              nodeEntry,\n              env.getChildErrorInfos());\n          // If the child error was catastrophic, committing this parent to the graph is not\n          // necessary, but since we don't do error bubbling in catastrophes, it doesn't violate any\n          // invariants either.\n          Set<SkyKey> reverseDeps = env.commitAndGetParents(nodeEntry);\n          evaluatorContext.signalParentsAndEnqueueIfReady(\n              skyKey, reverseDeps, nodeEntry.getVersion(), determineRestartPriority());\n          return;\n        }\n\n        // If there are external deps, we register that fact on the NodeEntry before we enqueue\n        // child nodes in order to prevent the current node from being re-enqueued between here and\n        // the call to registerExternalDeps below.\n        if (externalDeps != null) {\n          nodeEntry.addExternalDep();\n        }\n\n        // We want to split apart the dependencies that existed for this node the last time we did\n        // an evaluation and those that were introduced in this evaluation. To be clear, the prefix\n        // \"newDeps\" refers to newly discovered this time around after a SkyFunction#compute call\n        // and not to be confused with the oldDeps variable which refers to the last evaluation,\n        // i.e. a prior call to ParallelEvaluator#eval.\n        Set<SkyKey> newDepsThatWerentInTheLastEvaluation;\n        Set<SkyKey> newDepsThatWereInTheLastEvaluation;\n        if (oldDeps.isEmpty()) {\n          // When there are no old deps (clean evaluations), avoid set views which have O(n) size.\n          newDepsThatWerentInTheLastEvaluation = newDeps;\n          newDepsThatWereInTheLastEvaluation = ImmutableSet.of();\n        } else {\n          newDepsThatWerentInTheLastEvaluation = Sets.difference(newDeps, oldDeps);\n          newDepsThatWereInTheLastEvaluation = Sets.intersection(newDeps, oldDeps);\n        }\n\n        int childEvaluationPriority = determineChildPriority();\n        InterruptibleSupplier<NodeBatch> newDepsThatWerentInTheLastEvaluationNodes =\n            graph.createIfAbsentBatchAsync(\n                skyKey, Reason.RDEP_ADDITION, newDepsThatWerentInTheLastEvaluation);\n        handleKnownChildrenForDirtyNode(\n            newDepsThatWereInTheLastEvaluation,\n            graph.getBatch(skyKey, Reason.ENQUEUING_CHILD, newDepsThatWereInTheLastEvaluation),\n            nodeEntry,\n            childEvaluationPriority,\n            /*enqueueParentIfReady=*/ true);\n\n        // Due to multi-threading, this can potentially cause the current node to be re-enqueued if\n        // all 'new' children of this node are already done. Therefore, there should not be any code\n        // after this loop, as it would potentially race with the re-evaluation in another thread.\n        NodeBatch newNodes = newDepsThatWerentInTheLastEvaluationNodes.get();\n        for (SkyKey newDirectDep : newDepsThatWerentInTheLastEvaluation) {\n          enqueueChild(\n              skyKey,\n              nodeEntry,\n              newDirectDep,\n              newNodes.get(newDirectDep),\n              /*depAlreadyExists=*/ false,\n              childEvaluationPriority,\n              /*enqueueParentIfReady=*/ true);\n        }\n        if (externalDeps != null) {\n          // This can cause the current node to be re-enqueued if all futures are already done.\n          // This is an exception to the rule above that there must not be code below the for\n          // loop. It is safe because we call nodeEntry.addExternalDep above, which prevents\n          // re-enqueueing of the current node in the above loop if externalDeps != null.\n          evaluatorContext\n              .getVisitor()\n              .registerExternalDeps(skyKey, nodeEntry, externalDeps, determineRestartPriority());\n        }\n        // Do not put any code here! Any code here can race with a re-evaluation of this same node\n        // in another thread.\n      } catch (InterruptedException ie) {\n        // The current thread can be interrupted at various places during evaluation or while\n        // committing the result in this method. Since we only register the future(s) with the\n        // underlying AbstractQueueVisitor in the registerExternalDeps call above, we have to make\n        // sure that any known futures are correctly canceled if we do not reach that call. Note\n        // that it is safe to cancel a future multiple times.\n        cancelExternalDeps(env);\n        // InterruptedException cannot be thrown by Runnable.run, so we must wrap it.\n        // Interrupts can be caught by both the Evaluator and the AbstractQueueVisitor.\n        // The former will unwrap the IE and propagate it as is; the latter will throw a new IE.\n        throw SchedulerException.ofInterruption(ie, skyKey);\n      }\n    }",
  "func_after": "@Override\n    public void run() {\n      SkyFunctionEnvironment env = null;\n      try {\n        NodeEntry nodeEntry = checkNotNull(graph.get(null, Reason.EVALUATION, skyKey), skyKey);\n        checkState(nodeEntry.isReady(), \"%s %s\", skyKey, nodeEntry);\n        try {\n          evaluatorContext.getProgressReceiver().stateStarting(skyKey, NodeState.CHECK_DIRTY);\n          if (maybeHandleDirtyNode(nodeEntry) == DirtyOutcome.ALREADY_PROCESSED) {\n            return;\n          }\n        } finally {\n          evaluatorContext.getProgressReceiver().stateEnding(skyKey, NodeState.CHECK_DIRTY);\n        }\n\n        ImmutableSet<SkyKey> oldDeps = nodeEntry.getAllRemainingDirtyDirectDeps();\n        try {\n          evaluatorContext\n              .getProgressReceiver()\n              .stateStarting(skyKey, NodeState.INITIALIZING_ENVIRONMENT);\n          env =\n              SkyFunctionEnvironment.create(\n                  skyKey, nodeEntry.getTemporaryDirectDeps(), oldDeps, evaluatorContext);\n        } catch (UndonePreviouslyRequestedDeps undonePreviouslyRequestedDeps) {\n          // If a previously requested dep is no longer done, restart this node from scratch.\n          stateCache.invalidate(skyKey);\n          restart(skyKey, nodeEntry);\n          evaluatorContext.getVisitor().enqueueEvaluation(skyKey, determineRestartPriority());\n          return;\n        } finally {\n          evaluatorContext\n              .getProgressReceiver()\n              .stateEnding(skyKey, NodeState.INITIALIZING_ENVIRONMENT);\n        }\n        SkyFunctionName functionName = skyKey.functionName();\n        SkyFunction skyFunction =\n            checkNotNull(\n                evaluatorContext.getSkyFunctions().get(functionName),\n                \"Unable to find SkyFunction '%s' for node with key %s, %s\",\n                functionName,\n                skyKey,\n                nodeEntry);\n\n        SkyValue value = null;\n        long startTimeNanos = BlazeClock.instance().nanoTime();\n        try {\n          try {\n            evaluatorContext.getProgressReceiver().stateStarting(skyKey, NodeState.COMPUTE);\n            value = skyFunction.compute(skyKey, env);\n          } finally {\n            evaluatorContext.getProgressReceiver().stateEnding(skyKey, NodeState.COMPUTE);\n            long elapsedTimeNanos = BlazeClock.instance().nanoTime() - startTimeNanos;\n            if (elapsedTimeNanos > 0) {\n              Profiler.instance()\n                  .logSimpleTaskDuration(\n                      startTimeNanos,\n                      Duration.ofNanos(elapsedTimeNanos),\n                      ProfilerTask.SKYFUNCTION,\n                      skyKey.functionName().getName());\n            }\n          }\n        } catch (final SkyFunctionException builderException) {\n          stateCache.invalidate(skyKey);\n\n          ReifiedSkyFunctionException reifiedBuilderException =\n              new ReifiedSkyFunctionException(builderException);\n          // In keep-going mode, we do not let SkyFunctions complete with a thrown error if they\n          // have missing deps. Instead, we wait until their deps are done and restart the\n          // SkyFunction, so we can have a definitive error and definitive graph structure, thus\n          // avoiding non-determinism. It's completely reasonable for SkyFunctions to throw eagerly\n          // because they do not know if they are in keep-going mode.\n          if (!evaluatorContext.keepGoing() || !env.valuesMissing()) {\n            if (maybeHandleRegisteringNewlyDiscoveredDepsForDoneEntry(\n                skyKey, nodeEntry, oldDeps, env, evaluatorContext.keepGoing())) {\n              // A newly requested dep transitioned from done to dirty before this node finished.\n              // It is not safe to set the error because the now-dirty dep has not signaled this\n              // node. We return (without preventing new evaluations) so that the dep can complete\n              // and signal this node.\n              return;\n            }\n\n            boolean shouldFailFast =\n                !evaluatorContext.keepGoing() || builderException.isCatastrophic();\n            if (shouldFailFast) {\n              // After we commit this error to the graph but before the doMutatingEvaluation call\n              // completes with the error there is a race-like opportunity for the error to be used,\n              // either by an in-flight computation or by a future computation.\n              if (!evaluatorContext.getVisitor().preventNewEvaluations()) {\n                // This is not the first error encountered, so we ignore it so that we can terminate\n                // with the first error.\n                return;\n              } else {\n                logger.atWarning().withCause(builderException).log(\n                    \"Aborting evaluation while evaluating %s\", skyKey);\n              }\n            }\n            boolean isTransitivelyTransient =\n                reifiedBuilderException.isTransient()\n                    || env.isAnyDirectDepErrorTransitivelyTransient()\n                    || env.isAnyNewlyRequestedDepErrorTransitivelyTransient();\n            ErrorInfo errorInfo =\n                evaluatorContext\n                    .getErrorInfoManager()\n                    .fromException(skyKey, reifiedBuilderException, isTransitivelyTransient);\n            env.setError(nodeEntry, errorInfo);\n            Set<SkyKey> rdepsToBubbleUpTo = env.commitAndGetParents(nodeEntry);\n            if (shouldFailFast) {\n              evaluatorContext.signalParentsOnAbort(\n                  skyKey, rdepsToBubbleUpTo, nodeEntry.getVersion());\n              throw SchedulerException.ofError(errorInfo, skyKey, rdepsToBubbleUpTo);\n            }\n            evaluatorContext.signalParentsAndEnqueueIfReady(\n                skyKey, rdepsToBubbleUpTo, nodeEntry.getVersion(), determineRestartPriority());\n            return;\n          }\n        } catch (RuntimeException re) {\n          // Programmer error (most likely NPE or a failed precondition in a SkyFunction). Output\n          // some context together with the exception.\n          String msg = prepareCrashMessage(skyKey, nodeEntry.getInProgressReverseDeps());\n          RuntimeException ex = new RuntimeException(msg, re);\n          evaluatorContext.getVisitor().noteCrash(ex);\n          throw ex;\n        } finally {\n          env.doneBuilding();\n        }\n\n        if (maybeHandleRestart(skyKey, nodeEntry, value)) {\n          stateCache.invalidate(skyKey);\n          cancelExternalDeps(env);\n          evaluatorContext.getVisitor().enqueueEvaluation(skyKey, determineRestartPriority());\n          return;\n        }\n\n        Set<SkyKey> newDeps = env.getNewlyRequestedDeps();\n        if (value != null) {\n          stateCache.invalidate(skyKey);\n\n          checkState(\n              !env.valuesMissing(),\n              \"Evaluation of %s returned non-null value but requested dependencies that weren't \"\n                  + \"computed yet (one of %s), NodeEntry: %s\",\n              skyKey,\n              newDeps,\n              nodeEntry);\n\n          try {\n            evaluatorContext.getProgressReceiver().stateStarting(skyKey, NodeState.COMMIT);\n            if (maybeHandleRegisteringNewlyDiscoveredDepsForDoneEntry(\n                skyKey, nodeEntry, oldDeps, env, evaluatorContext.keepGoing())) {\n              // A newly requested dep transitioned from done to dirty before this node finished.\n              // This node will be signalled again, and so we should return.\n              return;\n            }\n            env.setValue(value);\n            Set<SkyKey> reverseDeps = env.commitAndGetParents(nodeEntry);\n            evaluatorContext.signalParentsAndEnqueueIfReady(\n                skyKey, reverseDeps, nodeEntry.getVersion(), determineRestartPriority());\n          } finally {\n            evaluatorContext.getProgressReceiver().stateEnding(skyKey, NodeState.COMMIT);\n          }\n          return;\n        }\n\n        SkyKey childErrorKey = env.getDepErrorKey();\n        if (childErrorKey != null) {\n          checkState(!evaluatorContext.keepGoing(), \"%s %s %s\", skyKey, nodeEntry, childErrorKey);\n          // We encountered a child error in noKeepGoing mode, so we want to fail fast. But we first\n          // need to add the edge between the current node and the child error it requested so that\n          // error bubbling can occur. Note that this edge will subsequently be removed during graph\n          // cleaning (since the current node will never be committed to the graph).\n          NodeEntry childErrorEntry =\n              checkNotNull(\n                  graph.get(skyKey, Reason.OTHER, childErrorKey),\n                  \"skyKey: %s, nodeEntry: %s childErrorKey: %s\",\n                  skyKey,\n                  nodeEntry,\n                  childErrorKey);\n          if (newDeps.contains(childErrorKey)) {\n            // Add this dep if it was just requested. In certain rare race conditions (see\n            // MemoizingEvaluatorTest.cachedErrorCausesRestart) this dep may have already been\n            // requested.\n            nodeEntry.addSingletonTemporaryDirectDep(childErrorKey);\n            DependencyState childErrorState;\n            if (oldDeps.contains(childErrorKey)) {\n              childErrorState = childErrorEntry.checkIfDoneForDirtyReverseDep(skyKey);\n            } else {\n              childErrorState = childErrorEntry.addReverseDepAndCheckIfDone(skyKey);\n            }\n            if (childErrorState != DependencyState.DONE) {\n              // The child in error may have transitioned from done to dirty between when this node\n              // discovered the error and now. Notify the graph inconsistency receiver so that we\n              // can crash if that's unexpected.\n              // We don't enqueue the child, even if it returns NEEDS_SCHEDULING, because we are\n              // about to shut down evaluation.\n              evaluatorContext\n                  .getGraphInconsistencyReceiver()\n                  .noteInconsistencyAndMaybeThrow(\n                      skyKey,\n                      ImmutableList.of(childErrorKey),\n                      Inconsistency.BUILDING_PARENT_FOUND_UNDONE_CHILD);\n            }\n          }\n          SkyValue childErrorInfoMaybe =\n              checkNotNull(\n                  env.maybeGetValueFromErrorOrDeps(childErrorKey),\n                  \"dep error found but then lost while building: %s %s\",\n                  skyKey,\n                  childErrorKey);\n          ErrorInfo childErrorInfo =\n              checkNotNull(\n                  ValueWithMetadata.getMaybeErrorInfo(childErrorInfoMaybe),\n                  \"dep error found but then wasn't an error while building: %s %s %s\",\n                  skyKey,\n                  childErrorKey,\n                  childErrorInfoMaybe);\n          evaluatorContext.getVisitor().preventNewEvaluations();\n          // TODO(b/166268889): Remove when fixed.\n          if (childErrorInfo.getException() instanceof IOException) {\n            logger.atInfo().withCause(childErrorInfo.getException()).log(\n                \"Child %s with IOException forced abort of %s\", childErrorKey, skyKey);\n          }\n          throw SchedulerException.ofError(childErrorInfo, childErrorKey, ImmutableSet.of(skyKey));\n        }\n\n        // TODO(bazel-team): This code is not safe to interrupt, because we would lose the state in\n        // newDirectDeps.\n\n        // TODO(bazel-team): An ill-behaved SkyFunction can throw us into an infinite loop where we\n        // add more dependencies on every run. [skyframe-core]\n\n        // Add all the newly requested dependencies to the temporary direct deps. Note that\n        // newDirectDeps does not contain any elements in common with the already existing temporary\n        // direct deps. uniqueNewDeps will be the set of unique keys contained in newDirectDeps.\n        env.addTemporaryDirectDepsTo(nodeEntry);\n\n        List<ListenableFuture<?>> externalDeps = env.externalDeps;\n        // If there were no newly requested dependencies, at least one of them was in error or there\n        // is a bug in the SkyFunction implementation. The environment has collected its errors, so\n        // we just order it to be built.\n        if (newDeps.isEmpty() && externalDeps == null) {\n          // TODO(bazel-team): This means a bug in the SkyFunction. What to do?\n          checkState(\n              !env.getChildErrorInfos().isEmpty(),\n              \"Evaluation of SkyKey failed and no dependencies were requested: %s %s\",\n              skyKey,\n              nodeEntry);\n          checkState(\n              evaluatorContext.keepGoing(),\n              \"nokeep_going evaluation should have failed on first child error: %s %s %s\",\n              skyKey,\n              nodeEntry,\n              env.getChildErrorInfos());\n          // If the child error was catastrophic, committing this parent to the graph is not\n          // necessary, but since we don't do error bubbling in catastrophes, it doesn't violate any\n          // invariants either.\n          Set<SkyKey> reverseDeps = env.commitAndGetParents(nodeEntry);\n          evaluatorContext.signalParentsAndEnqueueIfReady(\n              skyKey, reverseDeps, nodeEntry.getVersion(), determineRestartPriority());\n          return;\n        }\n\n        // If there are external deps, we register that fact on the NodeEntry before we enqueue\n        // child nodes in order to prevent the current node from being re-enqueued between here and\n        // the call to registerExternalDeps below.\n        if (externalDeps != null) {\n          nodeEntry.addExternalDep();\n        }\n\n        // We want to split apart the dependencies that existed for this node the last time we did\n        // an evaluation and those that were introduced in this evaluation. To be clear, the prefix\n        // \"newDeps\" refers to newly discovered this time around after a SkyFunction#compute call\n        // and not to be confused with the oldDeps variable which refers to the last evaluation,\n        // i.e. a prior call to ParallelEvaluator#eval.\n        Set<SkyKey> newDepsThatWerentInTheLastEvaluation;\n        Set<SkyKey> newDepsThatWereInTheLastEvaluation;\n        if (oldDeps.isEmpty()) {\n          // When there are no old deps (clean evaluations), avoid set views which have O(n) size.\n          newDepsThatWerentInTheLastEvaluation = newDeps;\n          newDepsThatWereInTheLastEvaluation = ImmutableSet.of();\n        } else {\n          newDepsThatWerentInTheLastEvaluation = Sets.difference(newDeps, oldDeps);\n          newDepsThatWereInTheLastEvaluation = Sets.intersection(newDeps, oldDeps);\n        }\n\n        int childEvaluationPriority = determineChildPriority();\n        InterruptibleSupplier<NodeBatch> newDepsThatWerentInTheLastEvaluationNodes =\n            graph.createIfAbsentBatchAsync(\n                skyKey, Reason.RDEP_ADDITION, newDepsThatWerentInTheLastEvaluation);\n        handleKnownChildrenForDirtyNode(\n            newDepsThatWereInTheLastEvaluation,\n            graph.getBatch(skyKey, Reason.ENQUEUING_CHILD, newDepsThatWereInTheLastEvaluation),\n            nodeEntry,\n            childEvaluationPriority,\n            /*enqueueParentIfReady=*/ true);\n\n        // Due to multi-threading, this can potentially cause the current node to be re-enqueued if\n        // all 'new' children of this node are already done. Therefore, there should not be any code\n        // after this loop, as it would potentially race with the re-evaluation in another thread.\n        NodeBatch newNodes = newDepsThatWerentInTheLastEvaluationNodes.get();\n        for (SkyKey newDirectDep : newDepsThatWerentInTheLastEvaluation) {\n          enqueueChild(\n              skyKey,\n              nodeEntry,\n              newDirectDep,\n              newNodes.get(newDirectDep),\n              /*depAlreadyExists=*/ false,\n              childEvaluationPriority,\n              /*enqueueParentIfReady=*/ true);\n        }\n        if (externalDeps != null) {\n          // This can cause the current node to be re-enqueued if all futures are already done.\n          // This is an exception to the rule above that there must not be code below the for\n          // loop. It is safe because we call nodeEntry.addExternalDep above, which prevents\n          // re-enqueueing of the current node in the above loop if externalDeps != null.\n          evaluatorContext\n              .getVisitor()\n              .registerExternalDeps(skyKey, nodeEntry, externalDeps, determineRestartPriority());\n        }\n        // Do not put any code here! Any code here can race with a re-evaluation of this same node\n        // in another thread.\n      } catch (InterruptedException ie) {\n        // The current thread can be interrupted at various places during evaluation or while\n        // committing the result in this method. Since we only register the future(s) with the\n        // underlying AbstractQueueVisitor in the registerExternalDeps call above, we have to make\n        // sure that any known futures are correctly canceled if we do not reach that call. Note\n        // that it is safe to cancel a future multiple times.\n        cancelExternalDeps(env);\n        // InterruptedException cannot be thrown by Runnable.run, so we must wrap it.\n        // Interrupts can be caught by both the Evaluator and the AbstractQueueVisitor.\n        // The former will unwrap the IE and propagate it as is; the latter will throw a new IE.\n        throw SchedulerException.ofInterruption(ie, skyKey);\n      }\n    }",
  "diff_func": "--- func_before\n+++ func_after\n @Override\n     public void run() {\n       SkyFunctionEnvironment env = null;\n       try {\n         NodeEntry nodeEntry = checkNotNull(graph.get(null, Reason.EVALUATION, skyKey), skyKey);\n         checkState(nodeEntry.isReady(), \"%s %s\", skyKey, nodeEntry);\n         try {\n           evaluatorContext.getProgressReceiver().stateStarting(skyKey, NodeState.CHECK_DIRTY);\n           if (maybeHandleDirtyNode(nodeEntry) == DirtyOutcome.ALREADY_PROCESSED) {\n             return;\n           }\n         } finally {\n           evaluatorContext.getProgressReceiver().stateEnding(skyKey, NodeState.CHECK_DIRTY);\n         }\n \n         ImmutableSet<SkyKey> oldDeps = nodeEntry.getAllRemainingDirtyDirectDeps();\n         try {\n           evaluatorContext\n               .getProgressReceiver()\n               .stateStarting(skyKey, NodeState.INITIALIZING_ENVIRONMENT);\n           env =\n               SkyFunctionEnvironment.create(\n                   skyKey, nodeEntry.getTemporaryDirectDeps(), oldDeps, evaluatorContext);\n-        } catch (UndonePreviouslyRequestedDep undonePreviouslyRequestedDep) {\n+        } catch (UndonePreviouslyRequestedDeps undonePreviouslyRequestedDeps) {\n           // If a previously requested dep is no longer done, restart this node from scratch.\n           stateCache.invalidate(skyKey);\n           restart(skyKey, nodeEntry);\n           evaluatorContext.getVisitor().enqueueEvaluation(skyKey, determineRestartPriority());\n           return;\n         } finally {\n           evaluatorContext\n               .getProgressReceiver()\n               .stateEnding(skyKey, NodeState.INITIALIZING_ENVIRONMENT);\n         }\n         SkyFunctionName functionName = skyKey.functionName();\n         SkyFunction skyFunction =\n             checkNotNull(\n                 evaluatorContext.getSkyFunctions().get(functionName),\n                 \"Unable to find SkyFunction '%s' for node with key %s, %s\",\n                 functionName,\n                 skyKey,\n         ",
  "diff_source": "custom",
  "Bug Filter": "Defect Remediation & Feature Upgrades",
  "Bug Filter Confidence": 0.95,
  "Bug Filter Response": "\n\nThe patch addresses a crash caused by internal node handling issues, not a security exploit. It modifies exception handling and introduces new receivers to manage node drops gracefully. The changes prevent system crashes but don't resolve any security vulnerabilities.\n\n**Final Output:**\n\nAnalysis: The patch modifies exception handling in `SkyFunctionEnvironment` to prevent crashes when dependencies are missing, due to an experimental flag. It introduces new receivers to tolerate node inconsistencies, avoiding system crashes. The issue doesn't involve attacker-controlled inputs or security boundaries, so it's a stability fix.\n\nFinal Classification: Defect Remediation & Feature Upgrades\n\nConfidence Score: 0.95"
}