{
  "id": 5513,
  "language": "C/C++",
  "commit_url": "https://github.com/torvalds/linux/commit/3ca8138f014a913f98e6ef40e939868e1e9ea876",
  "commit_sha": "3ca8138f014a913f98e6ef40e939868e1e9ea876",
  "commit_msg": "fuse: break infinite loop in fuse_fill_write_pages()\n\nI got a report about unkillable task eating CPU. Further\ninvestigation shows, that the problem is in the fuse_fill_write_pages()\nfunction. If iov's first segment has zero length, we get an infinite\nloop, because we never reach iov_iter_advance() call.\n\nFix this by calling iov_iter_advance() before repeating an attempt to\ncopy data from userspace.\n\nA similar problem is described in 124d3b7041f (\"fix writev regression:\npan hanging unkillable and un-straceable\"). If zero-length segmend\nis followed by segment with invalid address,\niov_iter_fault_in_readable() checks only first segment (zero-length),\niov_iter_copy_from_user_atomic() skips it, fails at second and\nreturns zero -> goto again without skipping zero-length segment.\n\nPatch calls iov_iter_advance() before goto again: we'll skip zero-length\nsegment at second iteraction and iov_iter_fault_in_readable() will detect\ninvalid address.\n\nSpecial thanks to Konstantin Khlebnikov, who helped a lot with the commit\ndescription.\n\nCc: Andrew Morton <akpm@linux-foundation.org>\nCc: Maxim Patlasov <mpatlasov@parallels.com>\nCc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>\nSigned-off-by: Roman Gushchin <klamm@yandex-team.ru>\nSigned-off-by: Miklos Szeredi <miklos@szeredi.hu>\nFixes: ea9b9907b82a (\"fuse: implement perform_write\")\nCc: <stable@vger.kernel.org>",
  "pr_url": null,
  "pr_info": "no more info",
  "file_name": "fs/fuse/file.c",
  "func_name": "fuse_fill_write_pages",
  "func_before": "static ssize_t fuse_fill_write_pages(struct fuse_req *req,\n\t\t\t       struct address_space *mapping,\n\t\t\t       struct iov_iter *ii, loff_t pos)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(mapping->host);\n\tunsigned offset = pos & (PAGE_CACHE_SIZE - 1);\n\tsize_t count = 0;\n\tint err;\n\n\treq->in.argpages = 1;\n\treq->page_descs[0].offset = offset;\n\n\tdo {\n\t\tsize_t tmp;\n\t\tstruct page *page;\n\t\tpgoff_t index = pos >> PAGE_CACHE_SHIFT;\n\t\tsize_t bytes = min_t(size_t, PAGE_CACHE_SIZE - offset,\n\t\t\t\t     iov_iter_count(ii));\n\n\t\tbytes = min_t(size_t, bytes, fc->max_write - count);\n\n again:\n\t\terr = -EFAULT;\n\t\tif (iov_iter_fault_in_readable(ii, bytes))\n\t\t\tbreak;\n\n\t\terr = -ENOMEM;\n\t\tpage = grab_cache_page_write_begin(mapping, index, 0);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tif (mapping_writably_mapped(mapping))\n\t\t\tflush_dcache_page(page);\n\n\t\ttmp = iov_iter_copy_from_user_atomic(page, ii, offset, bytes);\n\t\tflush_dcache_page(page);\n\n\t\tif (!tmp) {\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t\tbytes = min(bytes, iov_iter_single_seg_count(ii));\n\t\t\tgoto again;\n\t\t}\n\n\t\terr = 0;\n\t\treq->pages[req->num_pages] = page;\n\t\treq->page_descs[req->num_pages].length = tmp;\n\t\treq->num_pages++;\n\n\t\tiov_iter_advance(ii, tmp);\n\t\tcount += tmp;\n\t\tpos += tmp;\n\t\toffset += tmp;\n\t\tif (offset == PAGE_CACHE_SIZE)\n\t\t\toffset = 0;\n\n\t\tif (!fc->big_writes)\n\t\t\tbreak;\n\t} while (iov_iter_count(ii) && count < fc->max_write &&\n\t\t req->num_pages < req->max_pages && offset == 0);\n\n\treturn count > 0 ? count : err;\n}",
  "func_after": "static ssize_t fuse_fill_write_pages(struct fuse_req *req,\n\t\t\t       struct address_space *mapping,\n\t\t\t       struct iov_iter *ii, loff_t pos)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(mapping->host);\n\tunsigned offset = pos & (PAGE_CACHE_SIZE - 1);\n\tsize_t count = 0;\n\tint err;\n\n\treq->in.argpages = 1;\n\treq->page_descs[0].offset = offset;\n\n\tdo {\n\t\tsize_t tmp;\n\t\tstruct page *page;\n\t\tpgoff_t index = pos >> PAGE_CACHE_SHIFT;\n\t\tsize_t bytes = min_t(size_t, PAGE_CACHE_SIZE - offset,\n\t\t\t\t     iov_iter_count(ii));\n\n\t\tbytes = min_t(size_t, bytes, fc->max_write - count);\n\n again:\n\t\terr = -EFAULT;\n\t\tif (iov_iter_fault_in_readable(ii, bytes))\n\t\t\tbreak;\n\n\t\terr = -ENOMEM;\n\t\tpage = grab_cache_page_write_begin(mapping, index, 0);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tif (mapping_writably_mapped(mapping))\n\t\t\tflush_dcache_page(page);\n\n\t\ttmp = iov_iter_copy_from_user_atomic(page, ii, offset, bytes);\n\t\tflush_dcache_page(page);\n\n\t\tiov_iter_advance(ii, tmp);\n\t\tif (!tmp) {\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t\tbytes = min(bytes, iov_iter_single_seg_count(ii));\n\t\t\tgoto again;\n\t\t}\n\n\t\terr = 0;\n\t\treq->pages[req->num_pages] = page;\n\t\treq->page_descs[req->num_pages].length = tmp;\n\t\treq->num_pages++;\n\n\t\tcount += tmp;\n\t\tpos += tmp;\n\t\toffset += tmp;\n\t\tif (offset == PAGE_CACHE_SIZE)\n\t\t\toffset = 0;\n\n\t\tif (!fc->big_writes)\n\t\t\tbreak;\n\t} while (iov_iter_count(ii) && count < fc->max_write &&\n\t\t req->num_pages < req->max_pages && offset == 0);\n\n\treturn count > 0 ? count : err;\n}",
  "diff_func": "--- func_before\n+++ func_after\n static ssize_t fuse_fill_write_pages(struct fuse_req *req,\n \t\t\t       struct address_space *mapping,\n \t\t\t       struct iov_iter *ii, loff_t pos)\n {\n \tstruct fuse_conn *fc = get_fuse_conn(mapping->host);\n \tunsigned offset = pos & (PAGE_CACHE_SIZE - 1);\n \tsize_t count = 0;\n \tint err;\n \n \treq->in.argpages = 1;\n \treq->page_descs[0].offset = offset;\n \n \tdo {\n \t\tsize_t tmp;\n \t\tstruct page *page;\n \t\tpgoff_t index = pos >> PAGE_CACHE_SHIFT;\n \t\tsize_t bytes = min_t(size_t, PAGE_CACHE_SIZE - offset,\n \t\t\t\t     iov_iter_count(ii));\n \n \t\tbytes = min_t(size_t, bytes, fc->max_write - count);\n \n  again:\n \t\terr = -EFAULT;\n \t\tif (iov_iter_fault_in_readable(ii, bytes))\n \t\t\tbreak;\n \n \t\terr = -ENOMEM;\n \t\tpage = grab_cache_page_write_begin(mapping, index, 0);\n \t\tif (!page)\n \t\t\tbreak;\n \n \t\tif (mapping_writably_mapped(mapping))\n \t\t\tflush_dcache_page(page);\n \n \t\ttmp = iov_iter_copy_from_user_atomic(page, ii, offset, bytes);\n \t\tflush_dcache_page(page);\n \n+\t\tiov_iter_advance(ii, tmp);\n \t\tif (!tmp) {\n \t\t\tunlock_page(page);\n \t\t\tpage_cache_release(page);\n \t\t\tbytes = min(bytes, iov_iter_single_seg_count(ii));\n \t\t\tgoto again;\n \t\t}\n \n \t\terr = 0;\n \t\treq->pages[req->num_pages] = page;\n \t\treq->page_descs[req->num_pages].length = tmp;\n \t\treq->num_pages++;\n \n-\t\tiov_iter_advance(ii, tmp);\n \t\tcount += tmp;\n \t\tpos += tmp;\n \t\toffset += tmp;\n \t\tif (offset == PAGE_CACHE_SIZE)\n \t\t\toffset = 0;\n \n \t\tif (!fc->big_writes)\n \t\t\tbreak;\n \t} while (iov_iter_count(ii) && count < fc->max_write &&\n \t\t req->num_pages < req->max_pages && offset == 0);\n \n \treturn count > 0 ? count : err;\n }",
  "diff_source": "custom",
  "Bug Filter": "Defect Remediation & Feature Upgrades",
  "Bug Filter Confidence": 0.9,
  "Bug Filter Response": "\n\n**Final Output:**\n\nAnalysis:\nThe patch addresses an infinite loop in the fuse_fill_write_pages() function caused by handling zero-length segments. The fix ensures the iov_iter_advance() is called before retrying, preventing the loop. While it affects system availability, there's no evidence of an attacker-exploitable issue.\n\nFinal Classification: Defect Remediation & Feature Upgrades  \nConfidence Score: 0.9"
}