{
  "id": 6005,
  "language": "C/C++",
  "commit_url": "https://github.com/torvalds/linux/commit/70feee0e1ef331b22cc51f383d532a0d043fbdcc",
  "commit_sha": "70feee0e1ef331b22cc51f383d532a0d043fbdcc",
  "commit_msg": "mlock: fix mlock count can not decrease in race condition\n\nKefeng reported that when running the follow test, the mlock count in\nmeminfo will increase permanently:\n\n [1] testcase\n linux:~ # cat test_mlockal\n grep Mlocked /proc/meminfo\n  for j in `seq 0 10`\n  do\n \tfor i in `seq 4 15`\n \tdo\n \t\t./p_mlockall >> log &\n \tdone\n \tsleep 0.2\n done\n # wait some time to let mlock counter decrease and 5s may not enough\n sleep 5\n grep Mlocked /proc/meminfo\n\n linux:~ # cat p_mlockall.c\n #include <sys/mman.h>\n #include <stdlib.h>\n #include <stdio.h>\n\n #define SPACE_LEN\t4096\n\n int main(int argc, char ** argv)\n {\n\t \tint ret;\n\t \tvoid *adr = malloc(SPACE_LEN);\n\t \tif (!adr)\n\t \t\treturn -1;\n\n\t \tret = mlockall(MCL_CURRENT | MCL_FUTURE);\n\t \tprintf(\"mlcokall ret = %d\\n\", ret);\n\n\t \tret = munlockall();\n\t \tprintf(\"munlcokall ret = %d\\n\", ret);\n\n\t \tfree(adr);\n\t \treturn 0;\n\t }\n\nIn __munlock_pagevec() we should decrement NR_MLOCK for each page where\nwe clear the PageMlocked flag.  Commit 1ebb7cc6a583 (\"mm: munlock: batch\nNR_MLOCK zone state updates\") has introduced a bug where we don't\ndecrement NR_MLOCK for pages where we clear the flag, but fail to\nisolate them from the lru list (e.g.  when the pages are on some other\ncpu's percpu pagevec).  Since PageMlocked stays cleared, the NR_MLOCK\naccounting gets permanently disrupted by this.\n\nFix it by counting the number of page whose PageMlock flag is cleared.\n\nFixes: 1ebb7cc6a583 (\" mm: munlock: batch NR_MLOCK zone state updates\")\nLink: http://lkml.kernel.org/r/1495678405-54569-1-git-send-email-xieyisheng1@huawei.com\nSigned-off-by: Yisheng Xie <xieyisheng1@huawei.com>\nReported-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nTested-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Joern Engel <joern@logfs.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Michel Lespinasse <walken@google.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Rik van Riel <riel@redhat.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Michal Hocko <mhocko@suse.cz>\nCc: Xishi Qiu <qiuxishi@huawei.com>\nCc: zhongjiang <zhongjiang@huawei.com>\nCc: Hanjun Guo <guohanjun@huawei.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "pr_url": null,
  "pr_info": "no more info",
  "file_name": "mm/mlock.c",
  "func_name": "__munlock_pagevec",
  "func_before": "static void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(zone_lru_lock(zone));\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(zone_lru_lock(zone));\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}",
  "func_after": "static void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked = -nr;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(zone_lru_lock(zone));\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t} else {\n\t\t\tdelta_munlocked++;\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(zone_lru_lock(zone));\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}",
  "diff_func": "--- func_before\n+++ func_after\n static void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n {\n \tint i;\n \tint nr = pagevec_count(pvec);\n-\tint delta_munlocked;\n+\tint delta_munlocked = -nr;\n \tstruct pagevec pvec_putback;\n \tint pgrescued = 0;\n \n \tpagevec_init(&pvec_putback, 0);\n \n \t/* Phase 1: page isolation */\n \tspin_lock_irq(zone_lru_lock(zone));\n \tfor (i = 0; i < nr; i++) {\n \t\tstruct page *page = pvec->pages[i];\n \n \t\tif (TestClearPageMlocked(page)) {\n \t\t\t/*\n \t\t\t * We already have pin from follow_page_mask()\n \t\t\t * so we can spare the get_page() here.\n \t\t\t */\n \t\t\tif (__munlock_isolate_lru_page(page, false))\n \t\t\t\tcontinue;\n \t\t\telse\n \t\t\t\t__munlock_isolation_failed(page);\n+\t\t} else {\n+\t\t\tdelta_munlocked++;\n \t\t}\n \n \t\t/*\n \t\t * We won't be munlocking this page in the next phase\n \t\t * but we still need to release the follow_page_mask()\n \t\t * pin. We cannot do it under lru_lock however. If it's\n \t\t * the last pin, __page_cache_release() would deadlock.\n \t\t */\n \t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n \t\tpvec->pages[i] = NULL;\n \t}\n-\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);\n \t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n \tspin_unlock_irq(zone_lru_lock(zone));\n \n \t/* Now we can release pins of pages that we are not munlocking */\n \tpagevec_release(&pvec_putback);\n \n \t/* Phase 2: page munlock */\n \tfor (i = 0; i < nr; i++) {\n \t\tstruct page *page = pvec->pages[i];\n \n \t\tif (page) {\n \t\t\tlock_page(page);\n \t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n \t\t\t\t\t&pgrescued)) {\n \t\t\t\t/*\n \t\t\t\t * Slow path. We don't want to lose the last\n \t\t\t\t * pin before unlock_page()\n \t\t\t\t */\n \t\t\t\tget_page(page); /* for putback_lru_page() */\n \t\t\t\t__munlock_isolated_page(page);\n \t\t\t\tunlock_page(page);\n \t\t\t\tput_page(page); /* from follow_page_mask() */\n \t\t\t}\n \t\t}\n \t}\n \n \t/*\n \t * Phase 3: page putback for pages that qualified for the fast path\n \t * This will also call put_page() to return pin from follow_page_mask()\n \t */\n \tif (pagevec_count(&pvec_putback))\n \t\t_",
  "diff_source": "custom"
}