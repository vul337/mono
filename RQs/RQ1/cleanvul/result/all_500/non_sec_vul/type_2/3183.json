{
  "id": 3183,
  "language": "C/C++",
  "commit_url": "https://github.com/torvalds/linux/commit/95d78c28b5a85bacbc29b8dba7c04babb9b0d467",
  "commit_sha": "95d78c28b5a85bacbc29b8dba7c04babb9b0d467",
  "commit_msg": "fix unbalanced page refcounting in bio_map_user_iov\n\nbio_map_user_iov and bio_unmap_user do unbalanced pages refcounting if\nIO vector has small consecutive buffers belonging to the same page.\nbio_add_pc_page merges them into one, but the page reference is never\ndropped.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Vitaly Mayatskikh <v.mayatskih@gmail.com>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
  "pr_url": null,
  "pr_info": "no more info",
  "file_name": "block/bio.c",
  "func_name": "bio_find_or_create_slab",
  "func_before": "static struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)\n{\n\tunsigned int sz = sizeof(struct bio) + extra_size;\n\tstruct kmem_cache *slab = NULL;\n\tstruct bio_slab *bslab, *new_bio_slabs;\n\tunsigned int new_bio_slab_max;\n\tunsigned int i, entry = -1;\n\n\tmutex_lock(&bio_slab_lock);\n\n\ti = 0;\n\twhile (i < bio_slab_nr) {\n\t\tbslab = &bio_slabs[i];\n\n\t\tif (!bslab->slab && entry == -1)\n\t\t\tentry = i;\n\t\telse if (bslab->slab_size == sz) {\n\t\t\tslab = bslab->slab;\n\t\t\tbslab->slab_ref++;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\n\tif (slab)\n\t\tgoto out_unlock;\n\n\tif (bio_slab_nr == bio_slab_max && entry == -1) {\n\t\tnew_bio_slab_max = bio_slab_max << 1;\n\t\tnew_bio_slabs = krealloc(bio_slabs,\n\t\t\t\t\t new_bio_slab_max * sizeof(struct bio_slab),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!new_bio_slabs)\n\t\t\tgoto out_unlock;\n\t\tbio_slab_max = new_bio_slab_max;\n\t\tbio_slabs = new_bio_slabs;\n\t}\n\tif (entry == -1)\n\t\tentry = bio_slab_nr++;\n\n\tbslab = &bio_slabs[entry];\n\n\tsnprintf(bslab->name, sizeof(bslab->name), \"bio-%d\", entry);\n\tslab = kmem_cache_create(bslab->name, sz, ARCH_KMALLOC_MINALIGN,\n\t\t\t\t SLAB_HWCACHE_ALIGN, NULL);\n\tif (!slab)\n\t\tgoto out_unlock;\n\n\tbslab->slab = slab;\n\tbslab->slab_ref = 1;\n\tbslab->slab_size = sz;\nout_unlock:\n\tmutex_unlock(&bio_slab_lock);\n\treturn slab;\n}",
  "func_after": "struct bio *bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask)\n{\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (ret < local_nr_pages) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * check if vector was merged with previous\n\t\t\t * drop page reference if needed\n\t\t\t */\n\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n\t\t\t\tput_page(pages[j]);\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tfor (j = 0; j < nr_pages; j++) {\n\t\tif (!pages[j])\n\t\t\tbreak;\n\t\tput_page(pages[j]);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}",
  "diff_func": "--- func_before\n+++ func_after\n-static struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)\n+struct bio *bio_map_user_iov(struct request_queue *q,\n+\t\t\t     const struct iov_iter *iter,\n+\t\t\t     gfp_t gfp_mask)\n {\n-\tunsigned int sz = sizeof(struct bio) + extra_size;\n-\tstruct kmem_cache *slab = NULL;\n-\tstruct bio_slab *bslab, *new_bio_slabs;\n-\tunsigned int new_bio_slab_max;\n-\tunsigned int i, entry = -1;\n+\tint j;\n+\tint nr_pages = 0;\n+\tstruct page **pages;\n+\tstruct bio *bio;\n+\tint cur_page = 0;\n+\tint ret, offset;\n+\tstruct iov_iter i;\n+\tstruct iovec iov;\n \n-\tmutex_lock(&bio_slab_lock);\n+\tiov_for_each(iov, i, *iter) {\n+\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n+\t\tunsigned long len = iov.iov_len;\n+\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n+\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n \n-\ti = 0;\n-\twhile (i < bio_slab_nr) {\n-\t\tbslab = &bio_slabs[i];\n+\t\t/*\n+\t\t * Overflow, abort\n+\t\t */\n+\t\tif (end < start)\n+\t\t\treturn ERR_PTR(-EINVAL);\n \n+\t\tnr_pages += end - start;\n+\t\t/*\n+\t\t * buffer must be aligned to at least logical block size for now\n+\t\t */\n+\t\tif (uaddr & queue_dma_alignment(q))\n+\t\t\treturn ERR_PTR(-EINVAL);\n-\t\tif (!bslab->slab && entry == -1)\n-\t\t\tentry = i;\n-\t\telse if (bslab->slab_size == sz) {\n-\t\t\tslab = bslab->slab;\n-\t\t\tbslab->slab_ref++;\n-\t\t\tbreak;\n-\t\t}\n-\t\ti++;\n \t}\n \n-\tif (slab)\n-\t\tgoto out_unlock;\n+\tif (!nr_pages)\n+\t\treturn ERR_PTR(-EINVAL);\n \n-\tif (bio_slab_nr == bio_slab_max && entry == -1) {\n-\t\tnew_bio_slab_max = bio_slab_max << 1;\n-\t\tnew_bio_slabs = krealloc(bio_slabs,\n-\t\t\t\t\t new_bio_slab_max * sizeof(struct bio_slab),\n-\t\t\t\t\t GFP_KERNEL);\n-\t\tif (!new_bio_slabs)\n+\tbio = bio_kmalloc(gfp_mask, nr_pages);\n+\tif (!bio)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tret = -ENOMEM;\n+\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n+\tif (!pages)\n+\t\tgoto out;\n+\n+\tiov_for_each(iov, i, *iter) {\n+\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n+\t\tunsigned long len = iov.iov_len;\n+\t\tunsigned long end = (uaddr + len + PAGE_SIZE",
  "diff_source": "custom",
  "Bug Filter": "Defect Remediation & Feature Upgrades",
  "Bug Filter Confidence": 0.95,
  "Bug Filter Response": "\n\n**Analysis:**\nThe patch addresses an issue with unbalanced page reference counting in the `bio_map_user_iov` function. It modifies how pages are counted and referenced to prevent leaks or potential crashes, which is a functional bug fix.\n\n**Final Classification:** Defect Remediation & Feature Upgrades  \n**Confidence Score:** 0.95"
}